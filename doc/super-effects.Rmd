---
title: "super-fx"
author: "Knott, Z., Nydam, A., Bowman, H., Dux, PE., & Garner, KG."
date: '`r format(Sys.time())`'
output:
  bookdown::pdf_document2:
    includes:
      before_body: ../template/doc_prefix.tex
      in_header: ../template/preamble.tex
    keep_tex: yes
    latex_engine: xelatex
    number_sections: no
    toc: no
  bookdown::html_document2:
    number_sections: no
    theme: readable
    toc: yes
  bookdown::tufte_html2:
    number_sections: no
    toc: yes
  bookdown::word_document2: null
fontsize: 12pt
linestretch: 1.5
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-annotated-bibliography.csl
bibliography: ../template/ref.bib
always_allow_html: yes
links-as-notes: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# download template files if not available
tpl_1 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/preamble.tex'
tpl_2 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/doc_prefix.tex'
bib_1 = 'doc/refs.bib'
# change directory accordingly
# if(!file.exists(tpl_1f <- '../template/preamble.tex')) download.file(tpl_1, tpl_1f)
# if(!file.exists(tpl_2f <- '../template/doc_prefix.tex')) download.file(tpl_2, tpl_2f)
# if(knitr::is_latex_output() | knitr::is_html_output()){
#   library(kableExtra)
# } else {
#   options(kableExtra.auto_format = FALSE) # for docx
# }
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
```

**Running headline**: Environment and species richness

**Abstract**: Your awesome abstract here.

\clearpage

# Introduction

Here is your introduction. It should describe clearly the rationale for the study being done and the previous work related with the study. It should also tell readers about your specific hypothese/questions being addressed. Citations will be like this [@adair_single-pool_2010], or [e.g., @clark_loss_2008], or [@eriksson_seed_1993; @williamson_dissolved_1999]

Here is the second paragraph of the introduction. 


# Methods

### Participants

The current study utilises a dataset previously collected in the lab for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and individual differences. This dataset consists of performance measures from 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being of female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by the University of Queensland Human Reseach Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).


### Apparatus

Experimental procedures were run on an Apple Mac Minicomputer (OS X [which version]) with custom code using the Psychophysics toolbox [@brainardPsychophysicsToolbox1997a; @pelliVideoToolboxSoftwareVisual1997a] in Matlab v2015b. Participants completed 5 tasks; Attentional Blink (AB), Dual Task (DT), Contextual Cueing (CC), Serial Response Task (SRT), and Visual Statistical Learning (VSL). Task order was randomised for each participant, apart from the VSL task, which was presented last. This was because the recognition component of the task may have allowed participants to infer that other tasks were also assessing implicit learning. 

### Task Procedures

**How far were participants sat from the monitor?**

#### Attentional Blink (AB)

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli were subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of each target at the end of the trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

#### Dual Task (DT)

The DT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two possible coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two possible sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both a circle and a sound were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, task-hand mapping was counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the measures of interest.

#### Contextual Cueing (CC)

The CC protocol was the same as reported by [@nydamCathodalElectricalStimulation2018]. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'Repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'Novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1-12) and target orientation (left or right) was randomised for each block. After each block, the programme paused and participants pressed a key when they were ready to continue. 

#### Serial Response Task (SRT)

The SRT was adapted from [nissenAttentionalRequirementsLearning1987]. The task began with a []. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: ] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus was randomly selected for each trial. **what kind of randomisation?** These blocks are referred to as 'Novel'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'Repeats'. Of interest is the RT for Novel and Repeat blocks in the latter half of the experiment (block 4 vs 3).

#### Visual Statistical Learning (VSL)

### Simulation protocol

#### Sampling procedure
Insert info about sampling procedure


#### Effect sizes
Effect sizes: _FFX_: where required, Cohen's $d$ was attained by converting partial eta square using the formula defined in [https://www.journalofcognition.org/articles/10.5334/joc.10/].
_RFX_: as defined in [add ref above], an approximation of Cohen's $d$ was attained by computing the ratio of the proportion of variance explained by the regressor of interest, relative to the remaining regressors in the model (including the error variance).

Extra simulations were each run with an extra error term for the T1 or T2 stimulus (AB and SD), for experimentor ID, or for task-order, but the results suggested that the addition of an extra random effect resulted in a model structure that was too complex for the data.


#### Model definitions 
RFX: e.g. AB $$y_{ij} \sim lag + e_{i} + e_{ij}$$
Tested for significance by performing the likelihood ratio test between the model containing the effect of interest and the null model (see each task for definitions).


[Will prob move the below to the relevant section in the results]
##### Attentional Blink 

_FFX_: A repeated-measures ANOVA with one factor (lag: 2, 3, 5 & 7) was performed. Cohen's $d$ is reported for the main effect of lag. _RFX_: For the RFX analysis we included an extra intercept term for each subject. Again, Cohen's $d$ is reported for the main effect of lag. To test for the statistical significance of the main effect of lag, the RFX model containing the lag term was compared to the model containing the subject and error intercepts. 

##### Dual-task

_FFX_: Data were modelled using a 2 (modality: visual vs auditory) x 2 (task: single vs dual) repeated-measures ANOVA. Cohen's $d$ is reported for the main effect of task. _RFX_: The RFX model of interest contained an additional intercept term for each subject. To test for the statistical significance of the main effect of task, the model of interest was compared to a null model that contained only the regressor for modality, and the subject and error intercepts. 

##### Contextual Cueing

_FFX_: A block (1:12) x condition (repeat vs novel) repeated-measures ANOVA was applied to the data. Cohen's $d$ is reported for the block x condition interaction. _RFX_: An additional intercept term for each subject was added to the main model of interest. To test for statistical significance of the block x condition interaction, the model of interest was compared to a null model that contained only regressors for the main effect of block, the main effect of task, and the subject intercept terms. 

##### Serial Response Task 

_FFX_: For the SRT data, mean RTs from repeat and novel sequences were compared using a within-subjects t-test. _RFX_: A linear mixed effects model with a regressor for condition (repeat vs novel) and a subject intercept term was compared to a null model that contained only the subject intercept term.  

## Results

### Attentional Blink

```{r, ABsimresults, fig.width=7, fig.asp=1, fig.cap="AB simulation results. A) showing Cohen's {d} attained for each sample size for the FFX modelling for the main effect of lag, B) showing Cohen's $d$ attained from the RFX modelling of lag + subject intercept, C) showing the log p-values attained across simulations at each sample size N for the FFX model, the vertical dashed line shows log(p=.05). Note: all p-values to the left of this line would result in a rejection of the null hypothesis.  D) same as C, except for the RFX model."}
task = "AB"
med = 24
d_scale_ffx = 2
d_scale_rfx = d_scale_ffx
p_scale_ffx = d_scale_ffx
p_scale_rfx = d_scale_ffx
px_rng_d = c(0,5)
px_rng_p_ffx = c(-800,0)
px_rng_p_rfx = c(-800,0)
plot.in.doc(task, med, d_scale_ffx, d_scale_rfx, p_scale_ffx, p_scale_rfx, px_rng_d, px_rng_p_ffx, px_rng_p_rfx)


```

Notes: large Cohen's $d$, it looks like RFX modelling suggests a larger range of effect sizes for smaller sample sizes, but that this issue goes away as the precision capacity of the estimate increases. For FFX, sample sizes > 18 ensure the alternate will not be rejected, whereas for RFX, all exp sizes = reject null.  

Idea: ratio of range of d-values for each model/N?

```{r, doABstats}

AB.res <- do.stats("AB")
AB.res
```


### Dual-task

```{r, SDsimresults, warning=F, echo=F, fig.width=7, fig.asp=1, fig.cap="Dual-task simulation results. A) showing Cohen's {d} attained for each sample size for the FFX modelling for the main effect of task (single vs dual), B) showing Cohen's $d$ attained from the RFX modelling with an additional subject intercept, C) showing the log p-values attained across simulations at each sample size N for the FFX model, the vertical dashed line shows log(p=.05). D) same as C, except for the RFX model."}

task = "SD"
med = 24
d_scale_ffx = 2
d_scale_rfx = d_scale_ffx
p_scale_ffx = d_scale_ffx
p_scale_rfx = d_scale_ffx
px_rng_d = c(0,3)
px_rng_p_ffx = c(-1000,0)
px_rng_p_rfx = c(-1000,0)
plot.in.doc(task, med, d_scale_ffx, d_scale_rfx, p_scale_ffx, p_scale_rfx, px_rng_d, px_rng_p_ffx, px_rng_p_rfx)


```



```{r, doSDstats}

SD.res <- do.stats("SD")
SD.res
```

### Contextual Cueing

```{r, CCsimresults, warning=F, echo=F, fig.width=7, fig.asp=1, fig.cap="Contextual-cueing simulation results. A) showing Cohen's {d} attained for each sample size for the FFX modelling for the interaction of block x condition (repeat vs novel), B) showing Cohen's $d$ attained from the RFX modelling with an additional subject intercept, C) showing the log p-values attained across simulations at each sample size N for the FFX model, the vertical dashed line shows log(p=.05). D) same as C, except for the RFX model."}

task = "CC"
med = 23
d_scale_ffx = 2
d_scale_rfx = d_scale_ffx
p_scale_ffx = d_scale_ffx
p_scale_rfx = d_scale_ffx
px_rng_d = c(0,1)
px_rng_p_ffx = c(-50,0)
px_rng_p_rfx = c(-50,0)
plot.in.doc(task, med, d_scale_ffx, d_scale_rfx, p_scale_ffx, p_scale_rfx, px_rng_d, px_rng_p_ffx, px_rng_p_rfx)


```

Putative reason for differences in effect size observations between FFX and RFX: FFX models deviations from the grand mean - therefore a subject that shows a large deviation from the grand mean can inflate the variance soaked up by the condition regressor. As the RFX model includes a subject intercept, the regressor of interest can only soak up the variance of interest.

Q: are large effect size observations driven by individuals who are far from the grand mean?

- note from Chris: plot effect size vs p
- also, can you get sig with consistent d=.05 across subs?

One clue towards these thoughts would be if the subject intercept estimates showed, on some simulations, larger values than on other simulations - i.e. the distribution should be skewed or bimodal.

```{r, doCCstats}

CC.res <- do.stats("CC")
CC.res

```

### Serial Response Task

```{r, SRTsimresults, warning=F, echo=F, fig.width=7, fig.asp=1, fig.cap="Serial Response Task simulation results. A) showing Cohen's {d} attained for each sample size for the FFX modelling, using a within-subjects t-test to compare condition (repeat vs novel), B) showing Cohen's $d$ attained from the RFX modelling with an additional subject intercept, C) showing the log p-values attained across simulations at each sample size N for the FFX model, the vertical dashed line shows log(p=.05). D) same as C, except for the RFX model."}

task = "SRT"
med = 39
d_scale_ffx = 2
d_scale_rfx = d_scale_ffx
p_scale_ffx = d_scale_ffx
p_scale_rfx = d_scale_ffx
px_rng_d = c(0,3)
px_rng_p_ffx = c(-400,0)
px_rng_p_rfx = c(-400,0)
plot.in.doc(task, med, d_scale_ffx, d_scale_rfx, p_scale_ffx, p_scale_rfx, px_rng_d, px_rng_p_ffx, px_rng_p_rfx)
# med = 39, d_scale_ffx = 2, d_scale_rfx = 2, p_scale_ffx = 2, p_scale_rfx = 2, p_rng_d = c(0,3), px_rng_p_ffx = c(-400,0), px_rng_p_rfx = c(-400,0)


```

Note: FFX leads you awry far more often than RFX
Modelling subject means is important for implicit learning tasks.

```{r, doSRTstats}
# clean up AB data
SRT.res <- do.stats("SRT")
SRT.res
```


### Visual Statistical Learning
