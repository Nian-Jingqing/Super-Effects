---
title: "super-fx"
author: "Garner, KG., Knott, Z., Nydam, A., Nolan, CR., Bowman, H., & Dux, PE.,"
date: '`r format(Sys.time())`'
output:
  bookdown::html_document2:
    number_sections: no
    theme: readable
    toc: yes
  bookdown::tufte_html2:
    number_sections: no
    toc: yes
  bookdown::pdf_document2:
    includes:
      before_body: ../template/doc_prefix.tex
      in_header: ../template/preamble.tex
    keep_tex: yes
    latex_engine: xelatex
    number_sections: no
    toc: no
  bookdown::word_document2: null
fontsize: 12pt
linestretch: 1.5
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-annotated-bibliography.csl
bibliography: /Users/kels/Dropbox/documents/MC-Docs/Super-Effects/doc/refs.bib
always_allow_html: yes
links-as-notes: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# download template files if not available
tpl_1 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/preamble.tex'
tpl_2 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/doc_prefix.tex'
bib_1 = '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/doc/refs.bib'
# change directory accordingly
if(!file.exists(tpl_1f <- '../template/preamble.tex')) download.file(tpl_1, tpl_1f)
if(!file.exists(tpl_2f <- '../template/doc_prefix.tex')) download.file(tpl_2, tpl_2f)
if(knitr::is_latex_output() | knitr::is_html_output()){
  library(kableExtra)
} else {
  options(kableExtra.auto_format = FALSE) # for docx
}
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
source('R/efilids_functions.R')
source('R/R_rainclouds.R')
#source('R/doc_functions.R') # some specific plotting functions for the document
```

**Running headline**: Quantifying imprecision in effect size estimates for tasks tapping executive function and implicit learning.  

**Abstract**: 

\clearpage

# Introduction

<br>

~100 words per paragraph

The brain is the most complex organism known to humans, yet decision making regarding theory for its function tends to be made on binary (i.e. pass or fail) terms, at least in the experimental psychological sciences. Specifically, theories often propose experimental tests for the presence or absence of given effects, rather than quantifying the extent to which an effect should be observed, i.e. the anticipated effect size. The latter prediction is more risky, and therefore constitutes a more desirable prediction for theory testing [insert Popper reference]. In fact, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. 

For example, in the study of EF: [AB: theory of p/f vs Ragnaroc?], MT costs - .  

Likewise, in the study of IL: VSL, CC, SRT

To promote quantised theories in experimental psychology, one extra piece of pertinent information that is informative to theory development is - we normally do x (insert Cummings refs), but it is missing y.

This is also useful for experimental development. i.e. The other important thing is that to ensure that we provide sufficiently precise information, so that this can be used. To do that we need to perform power calculations, There are at least 3 ways to do this: have a sufficiently precise theory that quantifies the effect size of interest, arbitrarily assume an effect size of theoretical interest, but unmotivated, or create an estimate by sampling the field. The problem with the latter is that given the current sample sizes typically employed, we have no idea if that estimate is precise and should be used.

A method to determine the precision of effect  

For example, theories in the experimental psychological sciences have tended only to predict the presence or absence, rather than the extent of a given phenomena. For example [example from implicit learning]. However, clear to see that if we are to understand how such a complex system processes sensory information, coordinates tasks and acquires new behavioural repertoires, a precise mapping between theory and outcome is going to be necessary. [get some Vehicles notions in that previous sentence]. Therefore important to start thinking about the size of effects - however, current state of field, very little knowledge about anticipated effect sizes due to x, y, & z.


Using a large dataset we will address this gap. Data on x-tasks. We will apply a simulation analysis to determine x, y and z.

Moreover, we will address two pertinrent analytical gaps: i) A further development is the recent use of linear mixed effects models, and the recommendation that we use them instead of
ii) it is common to use t-test on accuracies against chance but Allefeld (VSL).

# Methods

<br>

### Participants

<br>

The current study utilises a dataset collected in the lab for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This dataset consists of performance measures from 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being of female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by the University of Queensland Human Reseach Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

<br>

### Apparatus

<br>

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 5 tasks; Attentional Blink (AB), Dual Task (DT), Contextual Cueing (CC), Serial Response Task (SRT), and Visual Statistical Learning (VSL). Task order was randomised for each participant, apart from the VSL task, which was presented last. This was because the recognition component of the task may have allowed participants to infer that other tasks were also assessing implicit learning. 

<br>

### Procedures

<br>

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure xxxx. Further details regarding the task protocols are presented within each section below. In the interest of reducing working memory load, we provide an overview of the simulation procedures, before detailing the specific procedural and statistical methods for each task.

<br>

```{r paradigm, fig.align='center', out.width="600pix", fig.cap="Figure xxxx: Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters, B) Multitasking Paradigm (MT). Participants make a discriminate the colour of a disc, a complex tone, or both C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the blocks the stimulus follows a repeating sequence. E) Visual Statistical Learning Paradigm (VSL): i) 12 shapes are grouped into 6 base pairs. ii) Learning: three of the six pairs are presented as an array, this is repeated as participants passively view the displays. iii) Test: participants are presented with a base pair, and a novel pair formed from a recombination of the 12 shapes, and is asked which of the two pairs they have seen previously."}
paradigm.fig.pth <- '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/images/FigXXXX_alltasks.png'
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```

<br>


All the [data]() and [code](https://github.com/kel-github/Super-Effects) used for the current analysis are available online. To ascertain that we attained the typically observed findings for each of the paradigms under study, we first applied the conventional statistical model for that paradigm on the full dataset (N=313), the details of which are presented below in the context of each task. Next we implemented a simulation procedure to determine the effect size and p-values that would be attained over $k$ experiments across multiple sample sizes. 


<br>

#### Sampling procedure

<br>

To simulate $k$ experiments at our chosen sample sizes (N), we developed a sampling procedure that sought to balance the potentially increasing homegeneity of samples as N approached our largest possible sample size (N=313) against the heterogeneity of information offered by the whole dataset. Specifically, it could be that as N approaches 313, the overlap of participants between subsamples may be greater than when N equals a much lower number; e.g. 13. It could then be that decreasing variability in effect size estimates across N is due to the decrease in variability of the subsamples, rather than an increase in precision that would be expected with a larger N. Therefore at each level of N, we first selected a subsample from the total dataset _without_ replacement. From this subsample, we further subsampled $k$ = 1000 samples of the given N _with_ replacement. The relevant analysis was then applied to each of these subsamples. This latter sampling with replacement ensured that the data upon which the analysis was performed carried the Markov property.  However, given at lower Ns this could result in a significant loss of critical information (for example, conclusions regarding the distributions of effect sizes for lower N samples would be based solely on a dataset comprising that lower N (e.g. N=13), we repeated this process over $j$ = 1000 iterations. Therefore each density at each N reflects 1000^1000 simulated experiments. We refer to this now as the two-step sampling procedure: $j$ = 1000, $k$ = 1000. It is worth noting that we compared this procedure to a simpler one where we performed the two-step sampling procedure with $j$ = 1 (i.e. we only sampled N without replacement once), and to a one-step sampling procedure where we sampled N with replacement from the entire dataset (therefore we did not correct against potentially increasing homogeneity of the sample, although note that the Markov property is retained in this sampling procedure), therefore one-step $k$=1000 and $j$ = 0. Outcomes were highly comparable between the sampling procedures, with the two-step procedure ($j$ = 1000) offering better resolution of the resulting densities (see Supplemental Figure XXXX for a representative example). For each task, we sampled across 20 different Ns, defined on a logarithmic interval between N=13 and N=313. We opted for logarithmic interval given the decreasing information gain from increasing N's at higher N values.

```{r supp_sampling, fig.align='center', out.width="600pix", fig.cap="Supplemental Figure xxxx: Comparison between sampling procedures for the AB task data fit with a repeated measures ANOVA for 4 levels of N; densities of observed effect sizes for the A) two-step sampling procedure j=1000^k=1000, the B) two-step sampling procedure, j=1^k=1000, and C) the one-step procedure, $k$ = 1000"}

paradigm.fig.pth <- '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/images/SuppFigXXXX_AB_sampling.png'
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```

<br>

Specific details regarding which analyes were applied to each $k$ sample are detailed below. Given a) the documented advantages of linear mixed effects models (LME) over repeated-measures ANOVA [@muthAlternativeModelsSmall2016; @bagiellaMixedeffectsModelsPsychophysiology2000; mccullochRepeatedMeasuresANOVA2005], b) that only a conceptual proxy of $\eta_{p}^{2}$ is computable from these models [@brysbaertPowerAnalysisEffect2018], and c) there exists no data that we know of that quantifies to what extent we can expect comparable outcomes between both methods, we (where relevant) opted to apply both the standard statistical model, and a LME model to each $k$ sample. Specific model specifications are detailed below. Note that given the iterative fitting procedure, and therefore the increased temporal duration of fitting that is inherent to the LME approach, the models specified were the simplest possible. The computational requirements for fitting more complex models would have resulted in simulations running for over 11 months. Therefore, the following results should be interpreted with the caveat in mind that a more complex model may well yield different results in the LME case, and these findings merely serve as a proxy, and as an example of the simplest case. In the case of the VSL paradigm, we instead apply a prevalence statistic test, which provides an appropriate inferential statistical test when the dependent variable is an information based measure, such as accuracy [@allefeldValidPopulationInference2016].

From each set of simulations (per task) we report the following: 1) to assess the best estimate of the effect size of interest and its variability, we report the central tendency and variability of the distribution of effect sizes observed for our highest N (N=313) (apart from in once case of bimodality, where we report **suggestions for this would be great!**. 2) to determine imprecision in current effect size estimates present in the field, we report the ratio of the 95% quantiles of the effect size densities observed for the most precise estimate, relative to that oberved for the N that reflects the median sample size in the field. 3) To determine the N required to achieve 90% power to reject the null hypothesis, we report the N for which over 90% of p-values pass the threshold for significance ($\alpha$=.05). 4) To compare outcomes between the standard and LME approaches, we compare the ratio of 95% quantiles of the effect sizes observed for each level of N, and 5) whether there is agreement in the N required to achieve greater than 90% statistical power.

<br>

#### Attentional Blink (AB)

<br>

##### Protocol

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

<br>

##### Statistical Analysis

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\eta_{p}^{2}$ and the resulting $p$ value were taken for the main effect of lag. 

We also applied a LME to estimate the effect of lag, with a random intercept for each participant ($j$) and an intercept for random error ($i$): 

$$y_{ij} \sim lag + e_{i} + e_{ij}$$

The $\eta_{p}^{2}$ proxy was taken as the ratio of the estimated fixed effect of interest (lag) to the squareroot of the sum of the variance accounted for by the random effects in the model (see [@brysbaertPowerAnalysisEffect2018]):

$$\hat{\eta_{p}^{2}} = \frac{lag}{\sqrt{\sum{\sigma_{e_{i}}, \sigma_{e_{ij}}}}}$$ 

For this and for all subsequent applications of LME models, the $p$ value for the regressor of interest (i.e. lag) was attained by applying Wald's chi-square test as implemented using the Anova function from the car package [@foxCompanionAppliedRegression2018] for R (note: the results were comparable regardless of whether we used Wald's test or whether we performed a log-likelihood test between the model of interest and a null model, which for the current AB case contained only the two random intercept terms $e_{ij}$ and $e_{i}$). 

<br>

#### Multitasking (MT)

<br>

##### Protocol

The MT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two possible coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two possible sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, task-hand mapping was counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

<br>

##### Statistical Approach

To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: single vs dual) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\eta_{p}^{2}$ and $p$ are reported for the main effect of task. We also applied an LME model which included fixed effects regressors for task-modality, task and their interaction, and random intercepts for participants $e_{ij}$, and measurement error $e_{i}$. As above, $\hat{\eta_{p}^{2}}$ was computed as the ratio of the estimated fixed effect for the task regressor to the square root of the summed variance of the estimated random effects. Again, Wald's chi-square test was applied to attain a $p$ value for the main effect of task.

<br>

#### Contextual Cueing (CC)

<br>

##### Protocol

The CC protocol was the same as reported by [@nydamCathodalElectricalStimulation2018]. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'Repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'Novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1-12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable of interest.

<br>

##### Statistical Approach

To ascertain whether participants speeded responses to repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated arrays), the data were subject to a block (1:12) x condition (repeat vs novel array) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\eta_{p}^{2}$ and $p$ for the block x condition interaction. We also applied LME models, containing fixed effects regressors for block, condition and their interaction, and random intercepts for subjects and general measurement error. We report $\hat{\eta_{p}^{2}}$ for the block x condition interaction and the resulting $p$ value from Wald's chi-square test.

<br>

#### Serial Response Task (SRT)

<br>

##### Protocol

The SRT was adapted from [nissenAttentionalRequirementsLearning1987]. The task began with a []. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: ] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'Random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'Repeats'. Of interest is the RT for Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).

<br>

##### Statistical Approach

<br>

To ascertain whether participants learned the repeating sequences, RTs in the final block of repeats (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d$ and $p$ value from each test. We also applied an LME approach where we fit models with a fixed effects regressor for block (3 vs 4), and included the same random intercepts as described above. We report the resulting $\hat{\eta_{p}^{2}}$ and $p$ values for the fixed effect of block.

<br>

#### Visual Statistical Learning (VSL)

<br>

##### Protocol

The VSL task was adapted from Fiser and Aslin [-fiserUnsupervisedStatisticalLearning2001]. For each participant, twelve abstract shapes were grouped into 6 base pairs. On each trial of the learning phase, three base pairs were randomly positioned onto a 3 x 3 grid, which subtended ?$^\circ$ of visual angle. Each shape subtended ?$^\circ$. A total of 144 of these displays were each presented for 2000 ms, interleaved with a blank screen presented for 1000 ms. Participants were instructed to pay attention to the displays. The test phase consisted of trials where two shape pairs were each presented for 2000 ms, with an intervening blank screen lasting 1000 ms. One pair was selected from the 6 base pairs for that participant, and one pair was formed by a novel recombination formed by randomly selecting two of the 12 shapes, with the constraint that a base pair could not be selected. Participants were instructed to report which of the two pairs was more familiar, using the 'z' and 'm' keys to denote the first or second pair respectively, with the exact mapping counterbalanced across participants. Paticipants completed a total of 24 trials, with each base pair presented 4 times. The order of base and novel pairs was counterbalanced across trials, and trial order was randomised for each participant. Response accuracy in the testing phase was taken as the dependent variable of interest.

<br>

##### Statistical Approach

To assess whether participants recognised familiar base pairs more than would be expected by chance, it is typical to apply a one sample t-test to recognition test accuracies, against theoretical chance (p=.5) (e.g. [@fiserUnsupervisedStatisticalLearning2001]). Therefore we applied this test to our data to determine if our results corroborated that of the field. We also applied this test to each $k$ sample, and we report the resulting Cohen's $d$ and $p$ values.

As mentioned, the prevalence statistic is a valid second-level inferential test when the dependent variable is an infomration based measure such as classification accuract. Implementing the prevalence statistic typically involves a two step permutation procedure. First, a null distribution is generated for each participant's accuracies (first level permutations), given their responses and the trials to which they were exposed. This is typically attained by shuffling the trial labels over multiple iterations (e.g. $m$ = 1000), and taking the participant's accuracy under each permutation (with the constaint that $m$ = 1 is the neutral permuation) [@allefeldValidPopulationInference2016]. This yields $m$ accuracies per participant that would be attained given the null hypothesis was true. Secondly, to attain the second-level null distribution, each participant's first level permutations are sampled $i$ times, resulting in $i$ sets of N accuracies as would be expected given the null hypothesis. The proportion of which the minimum of the neutral accuracies is greater than the minimum from each of the $i$ sets of accuracies is taken as a measure of the probability that the neutral accuracies are drawn from the null distribution (i.e. serves as the $p$ value of the second-level inferential test). However, given that our sampling procedure already entailed 20x(1000^1000) permutations, and that such a procedure would therefore require 20x(1000^1000^1000^1000) permutations, we opted to instead apply an analytical definition of the first-level null distribution.

To summarise this analytical distribution, remember we have a binary forced choice task with possible answers we'll call $a$ and $b$, consisting of $N$ trials with exactly half of the trials having each possible answer as the correct response (i.e. $A_a = A_b = \frac{1}{2}N$). For any given set of responses $R_a + R_b = N$, we assign the identity of $a$ and $b$ to the possible choices such that $R_a \le A_a$.

Now we can assemble a distribution for this configuration of responses by computing the number possible combinations of $x$ responses in $A_a$ 'slots', multiplied by the number of $R_a - x$ responses in $A_b$ 'slots', for all x between zero and $R_a$. Or if $F(x)$ is the frequency for a given (not necessarily unique) accuracy, and $G(x)$ is the corresponding accuracy:

$F(x) = {{A_a}\choose {x}} \times {{A_b}\choose {R_a - x}}, G(x) = \frac{x + R_b - (R_a - x)}{R_a+R_b} , \forall x \in \Z \mid 0 \le x \le R_a$

To generate the second level null distribution, we then sampled each participant's first level distribution $i$ = 1000 times. 

Having calculated the $p$ value for a given set of neutral accuracies, we can compute the proportion of the population expected to show the effect $\gamma$, given the observed $p$ statistic (i.e. an effect size), using the formula defined by Allefeld et al [-@allefeldValidPopulationInference2016]:

$$\gamma = \frac{\alpha^\frac{1}{N} - p^\frac{1}{N}}{1-p^\frac{1}{N}}$$


## References

