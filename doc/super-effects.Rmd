---
title: "super-fx"
author: "Garner, KG., Knott, Z., Nydam, A., Nolan, CR., Bowman, H., & Dux, PE.,"
date: '`r format(Sys.time())`'
output:
  bookdown::html_document2:
    number_sections: no
    theme: readable
    toc: yes
  bookdown::tufte_html2:
    number_sections: no
    toc: yes
  bookdown::pdf_document2:
    includes:
      before_body: ../template/doc_prefix.tex
      in_header: ../template/preamble.tex
    keep_tex: yes
    latex_engine: xelatex
    number_sections: no
    toc: no
  bookdown::word_document2: null
fontsize: 12pt
linestretch: 1.5
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-annotated-bibliography.csl
bibliography: /Users/kels/Dropbox/documents/MC-Docs/Super-Effects/doc/refs.bib
always_allow_html: yes
links-as-notes: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# download template files if not available
tpl_1 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/preamble.tex'
tpl_2 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/doc_prefix.tex'
bib_1 = '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/doc/refs.bib'
# change directory accordingly
if(!file.exists(tpl_1f <- '../template/preamble.tex')) download.file(tpl_1, tpl_1f)
if(!file.exists(tpl_2f <- '../template/doc_prefix.tex')) download.file(tpl_2, tpl_2f)
if(knitr::is_latex_output() | knitr::is_html_output()){
  library(kableExtra)
} else {
  options(kableExtra.auto_format = FALSE) # for docx
}
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
#source('R/doc_functions.R') # some specific plotting functions for the document
```

**Running headline**: Quantifying imprecision in effect size estimates for tasks tapping executive function and implicit learning.  

**Abstract**: 

\clearpage

# Introduction

<br>

~100 words per paragraph

The brain is the most complex organism known to humans, yet decision making regarding theory for its function tends to be made on binary (i.e. pass or fail) terms, at least in the experimental psychological sciences. Specifically, theories often propose experimental tests for the presence or absence of given effects, rather than quantifying the extent to which an effect should be observed, i.e. the anticipated effect size. The latter prediction is more risky, and therefore constitutes a more desirable prediction for theory testing [insert Popper reference]. In fact, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. 

For example, in the study of EF: [AB: theory of p/f vs Ragnaroc?], MT costs - .  

Likewise, in the study of IL: VSL, CC, SRT

To promote quantised theories in experimental psychology, one extra piece of pertinent information that is informative to theory development is - we normally do x (insert Cummings refs), but it is missing y.

This is also useful for experimental development. i.e. The other important thing is that to ensure that we provide sufficiently precise information, so that this can be used. To do that we need to perform power calculations, There are at least 3 ways to do this: have a sufficiently precise theory that quantifies the effect size of interest, arbitrarily assume an effect size of theoretical interest, but unmotivated, or create an estimate by sampling the field. The problem with the latter is that given the current sample sizes typically employed, we have no idea if that estimate is precise and should be used.

A method to determine the precision of effect  

For example, theories in the experimental psychological sciences have tended only to predict the presence or absence, rather than the extent of a given phenomena. For example [example from implicit learning]. However, clear to see that if we are to understand how such a complex system processes sensory information, coordinates tasks and acquires new behavioural repertoires, a precise mapping between theory and outcome is going to be necessary. [get some Vehicles notions in that previous sentence]. Therefore important to start thinking about the size of effects - however, current state of field, very little knowledge about anticipated effect sizes due to x, y, & z.


Using a large dataset we will address this gap. Data on x-tasks. We will apply a simulation analysis to determine x, y and z.

Moreover, we will address two pertinrent analytical gaps: i) A further development is the recent use of linear mixed effects models, and the recommendation that we use them instead of
ii) it is common to use t-test on accuracies against chance but Allefeld (VSL).

PUT IN INTRO
Additionally, given the documented advantages of linear mixed effects models (LME) over repeated-measures ANOVA [@muthAlternativeModelsSmall2016; @bagiellaMixedeffectsModelsPsychophysiology2000; mccullochRepeatedMeasuresANOVA2005], and that only a conceptual proxy of $\eta_{p}^{2}$ is computable from these models [@brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014], and c) there exists no data that we know of that quantifies to what extent we can expect comparable outcomes between both methods, we (where relevant) opted to apply both the commonly used statistical model, and a LME model to each $k$ sample

# Methods

<br>

### Participants

<br>

The current study utilises a dataset collected in the lab for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This dataset contains performance measures from 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being of female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by the University of Queensland Human Reseach Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

<br>

### Apparatus

<br>

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 5 tasks; Attentional Blink (AB), Dual Task (DT), Contextual Cueing (CC), Serial Response Task (SRT), and Visual Statistical Learning (VSL). Task order was randomised for each participant, apart from the VSL task, which was presented last. This was because the recognition component of the task may have allowed participants to infer that other tasks were also assessing implicit learning. 

<br>

### Procedures

<br>

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure xxxx. Further details regarding the task protocols are presented within each section below. In the interest of reducing working memory load, we provide an overview of the simulation procedures, before detailing the specific procedural and statistical methods for each task.

<br>

```{r paradigm, fig.align='center', out.width="600pix", fig.cap="Figure xxxx: Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters, B) Multitasking Paradigm (MT). Participants make a discriminate the colour of a disc, a complex tone, or both C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the blocks the stimulus follows a repeating sequence. E) Visual Statistical Learning Paradigm (VSL): i) 12 shapes are grouped into 6 base pairs. ii) Learning: three of the six pairs are presented as an array, this is repeated as participants passively view the displays. iii) Test: participants are presented with a base pair, and a novel pair formed from a recombination of the 12 shapes, and is asked which of the two pairs they have seen previously."}
paradigm.fig.pth <- '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/images/FigXXXX_alltasks.png'
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```

<br>


All the [data]() and [code](https://github.com/kel-github/Super-Effects) used for the current analysis are available online. To ascertain that we observed the typical findings for each of the paradigms, we first applied the conventional statistical model for that paradigm on the full dataset (N=313), the details of which are presented below in the context of each task. Next, we implemented a simulation procedure to determine the effect size and p-values that would be attained over $k$ experiments across multiple sample sizes. 


<br>

#### Sampling procedure

<br>

For each task, we sampled across 20 different sample sizes (Ns), defined on a logarithmic interval between N=13 and N=313. We opted for a logarithmic interval given the decreasing information gained at higher N values. To simulate $k$ experiments at each of our chosen N, we developed a sampling procedure that sought to leverage information from across the whole dataset while also protecting against any reductions in effect size variablility that may be attributable to saturation as N approaches the maximum ($N_{max}$=313). Specifically, it could be that as N approaches 313, the overlap of participants between subsamples may be greater than when N equals a lower number such as N=13. It follows then that any decreasing variability in effect size estimates at higher Ns could be due to the decrease in variability of the subsamples, rather than the improved estimate of the population variance should come with a larger N. 

To protect against this possibility we applied the following procedure; for each level of N ($N_{1}, N_{2}, ...N_{20}$), we first selected a subsample from the total dataset _without_ replacement, e.g. N of 13 unique samples from the total N=313. We refer to this from now on as the _parent subsample_. From this parent subsample, we sampled $k$ = 1000 times _with_ replacement - e.g. N=13 sampled from N=13. These shall now be referred to as the _child subsamples_. The relevant analysis was then applied to each of the child subsamples. Sampling with replacement ensured that the child subsamples carried the Markov property. Given that this procedure reduces the heterogeneity of the parent sample (for example, in the case of N=13, all the child subsamples are derived from only 13 unique observations), we then repeated the entire process over $j$ = 1000 iterations. Therefore each density at each N reflects $j * k$ simulated experiments. We refer to this now as the two-step sampling procedure: $j$ = 1000, $k$ = 1000. It is worth noting that we compared this procedure to one where we performed the two-step sampling procedure with $j$ = 1, and to a one-step sampling procedure where we sampled each N with replacement from the entire dataset, i.e. $k$=1000 and $j$ = 0. Outcomes were comparable between the sampling procedures, with the two-step procedure ($j$ = 1000) offering better resolution of the resulting densities (see Supplemental Figure XXXX for a representative example). 

```{r supp_sampling, fig.align='center', out.width="600pix", fig.cap="Supplemental Figure xxxx: Comparison between sampling procedures for the AB task data fit with a repeated measures ANOVA for 4 levels of N; densities of observed effect sizes for the A) two-step sampling procedure j=1000^k=1000, the B) two-step sampling procedure, j=1^k=1000, and C) the one-step procedure, $k$ = 1000"}

paradigm.fig.pth <- '/Users/kels/Dropbox/documents/MC-Docs/Super-Effects/images/SuppFigXXXX_AB_sampling.png'
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```

<br>

Specific details regarding which analyses were applied to each $k$ sample are detailed below for each paradigm. Note that given the iterative fitting procedure that is inherent to the LME approach, the models specified were the simplest possible, as the computational requirements for fitting more complex models would have resulted in simulations running for over 11 months. Therefore, the following results should be interpreted with the caveat that a more complex model may well yield different results in the LME case. In these findings we present examples of the simplest case, to provide a basis for comparison, or a set of minimum assumptions for researchers interested in applying LME models to their data on these or comparable paradigms. 


_Effect Sizes_
To facilitate comparisons between ANOVA and LME approaches, $\eta_{p}^{2}$ values attained from the ANOVA models were converted to Cohen's $d$ using the following formula:  

For each task, we report the following information from the observed effect size densities: to assess the best estimate of the effect size of interest and its variability, we report the central tendency and standard deviation observed for our highest N (apart from in one case of bimodality, where we report the two points of highest probability). To determine imprecision in current effect size estimates in the field, we report the ratio of the .975-.025 quantiles observed with the highest N, relative to that observed for the N that reflects the median sample size found for the field. To compare outcomes between the standard and LME approaches, we compare the ratio of the effect sizes observed for each level of N, using the quantiles defined above. Lastly, to provide an estimate of the effect size that would be yielded through a meta-analysis of the published literature, we compute the mean effect size for all analyses that yielded a statistically significant result (p<.05). To determine the impact that increasing sample size would have on the meta-analytically attained effect size. we compare the difference between the meta-analytic and our mean observed effect sizes for each N.

_p Values_
To determine the N required to achieve 90% power to reject the null hypothesis, we report the N for which over 90% of p-values pass the threshold for significance ($\alpha$=.05). To assess the range of p-values that one can expect to observe given $k=1000^2$ experiments across N's, i.e. the confidence for the most likely observed p-value, we report the .025 and .975 quantiles for the observed p-values for each N. To compare the standard and the LME approaches, we then determine whether there is agreement in the N required to achieve greater than 90% statistical power. 

<br>

#### Attentional Blink (AB)

<br>

##### Protocol

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

<br>

##### Statistical Analysis

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $d$ and the resulting $p$ value were taken for the main effect of lag. 

We also applied a LME to estimate the effect of lag, with a random intercept for each participant ($j$) and an intercept for random error ($i$): 

$$y_{ij} \sim \beta_{0} + \beta_{1}lag + e_{j} + e_{ij}$$

The $d$ proxy was taken as the ratio of the estimated fixed effect of interest (lag) to the squareroot of the sum of the variance accounted for by the random effects in the model (see [@brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014]):

$$d = \frac{lag}{\sqrt{{\sigma_{e_{i}} + \sigma_{e_{ij}}}}}$$ 

For this and for all subsequent applications of LME models, the $p$ value for the regressor of interest (in this case 'lag') was attained by applying Wald's chi-square test as implemented using the Anova function from the car package [@foxCompanionAppliedRegression2018] for R (note: the results were comparable regardless of whether we used Wald's test or whether we performed a log-likelihood test between the model of interest and a null model, which for the current AB case contained only the two random intercept terms $e_{ij}$ and $e_{i}$). 

<br>

#### Multitasking (MT)

<br>

##### Protocol

The MT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two possible coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two possible sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

<br>

##### Statistical Analysis

To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: single vs dual) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $d$ and $p$ are reported for the main effect of task. We also applied an LME model which included fixed effects regressors for task-modality, task and their interaction, and random intercepts for participants $e_{ij}$, and measurement error $e_{i}$. As above, $d}$ was computed as the ratio of the estimated fixed effect for the task regressor to the square root of the summed variance of the estimated random effects. Again, Wald's chi-square test was applied to attain a $p$ value for the main effect of task.

<br>

#### Contextual Cueing (CC)

<br>

##### Protocol

The CC protocol was the same as that reported by [@nydamCathodalElectricalStimulation2018]. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1-12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable of interest.

<br>

##### Statistical Approach

To ascertain whether participants speeded responses to repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated arrays), the data were subject to a block (1:12) x condition (repeat vs novel array) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $d$ and $p$ for the block x condition interaction. We also applied LME models, containing fixed effects regressors for block, condition and their interaction, and random intercepts for subjects and general measurement error. We report $d$ for the block x condition interaction and the resulting $p$ value from Wald's chi-square test.

<br>

#### Serial Response Task (SRT)

<br>

##### Protocol

The SRT was adapted from [@nissenAttentionalRequirementsLearning1987]. The task began with a []. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: ] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'repeats'. Of interest is the RT for Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).

<br>

##### Statistical Approach

<br>

To ascertain whether participants learned the repeating sequences, RTs in the final block of repeats (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d$ and $p$ value from each test. We also applied an LME approach where we fit models with a fixed effects regressor for block (3 vs 4), and included the same random intercepts as described above. We report the resulting $d$ and $p$ values for the fixed effect of block.

<br>

#### Visual Statistical Learning (VSL)

<br>

##### Protocol

The VSL task was adapted from Fiser and Aslin [-fiserUnsupervisedStatisticalLearning2001]. For each participant, twelve abstract shapes were grouped into 6 base pairs. On each trial of the learning phase, three base pairs were randomly positioned onto a 3 x 3 grid, which subtended ?$^\circ$ of visual angle. Each shape subtended ?$^\circ$. A total of 144 of these displays were each presented for 2000 ms, interleaved with a blank screen presented for 1000 ms. Participants were instructed to pay attention to the displays. The test phase consisted of trials where two shape pairs were each presented for 2000 ms, with an intervening blank screen lasting 1000 ms. One pair was selected from the 6 base pairs for that participant, and one pair was formed by a novel recombination formed by randomly selecting two of the 12 shapes, with the constraint that a base pair could not be selected. Participants were instructed to report which of the two pairs was more familiar, using the 'z' and 'm' keys to denote the first or second pair respectively, with the exact mapping counterbalanced across participants. Paticipants completed a total of 24 trials, with each base pair presented 4 times. The order of base and novel pairs was counterbalanced across trials, and trial order was randomised for each participant. Response accuracy in the testing phase was taken as the dependent variable of interest.

<br>

##### Statistical Approach

To assess whether participants recognised familiar base pairs more than would be expected by chance, it is typical to apply a one-sample t-test to recognition test accuracies, against theoretical chance (p=.5) (e.g. [@fiserUnsupervisedStatisticalLearning2001]). Therefore we applied this test to our data to determine if our results corroborated that of the field. We applied this test to each $k$ sample, and we report the resulting Cohen's $d$ and $p$ values.

It has recently been shown that the assumptions of the one-sample t-test are not met when data are from an information based measure such as classification accuracy [@allefeldValidPopulationInference2016]. Specifically, because the true value of such a measure can never be below chance (50 % in the case of a 2 alternative-forced-choice), application of the t-test, which carries the assumption that the null distribution can include values above and below 50 %, becomes a fixed effects analysis (see Allefeld et al [-@allefeldValidPopulationInference2016] for a full treatment). Instead, application of the prevalence statistic [@allefeldValidPopulationInference2016] allows for valid second-level inference with information-based measures such as classification accuracy. 

Implementing the prevalence statistic typically involves a two step permutation procedure. First, a null distribution is generated for each participant's accuracies (first level permutations), given their responses and the trials to which they were exposed. This is typically attained by shuffling the trial labels over multiple iterations (e.g. $m$ = 1000), and taking the participant's accuracy under each permutation (with the constraint that $m$ = 1 is the observed data) [@allefeldValidPopulationInference2016]. This yields $m$ accuracies per participant that would be attained given the null hypothesis ($H_0$) was true, where $H_0$ is that the participant is guessing. Secondly, to attain the second-level null distribution, each participant's first level permutations are sampled $z$ times (e.g. $z$ = 1000), with the constraint that $z_{1}$ contains the observed accuracies across participants. This results in $z$ sets of N accuracies as would be expected given the second-level $H_0$. The minimum accuracy is then taken from each of the $z$ sets of permutated accuracies ($PA_{min}$). The proportion for which the observed minimum accuracy ($OA_{min}$) is greater than PA ($p = \frac{1}{i} \sum_{i=1}^z [OA_{min} > PA_{min}{z}]$) is taken as the probability that the observed accuracies are drawn from the distribution defined by $H_0$ (i.e. serves as the $p$ value of the second-level inferential test). 

Given that our sampling procedure already entailed $N*j*k$ permutations, and that such a permutation procedure as defined above would require $N*j*k*m*z$ permutations (which in our case is 2$e$+13), we opted to instead apply an analytical definition of the first- and second level distributions for $H_0$. 

To summarise this analytical distribution; each set of participant responses comes from a binary forced choice task with possible answers $a$ and $b$, consisting of $T$ trials with exactly half of the trials having each possible answer as the correct response (i.e. $A_a = A_b = \frac{1}{2}N$). For any given set of responses $R_a + R_b = N$, we assign the identity of $a$ and $b$ to the possible choices such that $R_a \le A_a$.

Now we can assemble a distribution for this configuration of responses by computing the number of possible combinations of $x$ responses in $A_a$ 'slots', multiplied by the number of $R_a - x$ responses in $A_b$ 'slots', for all x between zero and $R_a$. Or if $F(x)$ is the frequency for a given (not necessarily unique) accuracy, and $G(x)$ is the corresponding accuracy:

$F(x) = {{A_a}\choose {x}} \times {{A_b}\choose {R_a - x}}, G(x) = \frac{x + R_b - (R_a - x)}{R_a+R_b} , \forall x \in \mathbb{Z} \mid 0 \le x \le R_a$

To generate the second level null distribution, we then sampled each participant's first level distribution $z$ = 1000 times, to provide $z$ iterations of N accuracies as would be expected under $H_0$. 

Having calculated the $p$ value for a given set of neutral accuracies, we can compute the proportion of the population expected to show the effect $\gamma$, given the observed $p$ statistic (i.e. an effect size), using the formula defined by Allefeld et al [-@allefeldValidPopulationInference2016]:

$$\gamma = \frac{\alpha^\frac{1}{N} - p^\frac{1}{N}}{1-p^\frac{1}{N}}$$

To facilitate comparison between the outcomes from the t-tests and the prevalence tests, the $d$ values were converted

<br>

# Results

<br>

### Attentional Blink

```{r, plot_AB, fig.align='center', out.width="600pix", fig.cap="Figure xxxx: Results from the Attentional Blink Paradigm. A)"}

fname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
acc.p <- plot.AB.results(fname)


```

## References

