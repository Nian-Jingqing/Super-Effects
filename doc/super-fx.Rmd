---
title: Quantifying effect sizes in implicit learning tasks; the role of some stuff
authors:
  - name: Kelly G. Garner
    thanks: Corresponding author
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, UK, B13 2TT
    email: getkellygarner@gmail.com
  - name: Christopher R. Nolan
    department: School of Psychology
    affiliation: University of New South Wales
    location: Sydney, Australia
    email: cnolan@cn.id.au
  - name: Abbey Nydham
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Zoie Nott
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Howard Bowman
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, Birmingham, B13 2TT
    email: h.bowman@bham.ac.uk
  - name: Paul E. Dux
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
    email: paul.e.dux@gmail.com
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: refs.bib
biblio-style: unsrt
output: 
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
    keep_tex: true
  bookdown::word_document2:
    toc: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
#opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
# options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
#         knitr.kable.NA = '')
# knit_hooks$set(inline = function(x) {
#   x <- sprintf("%1.2f", x)
#   paste(x, collapse = ", ")
# })
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
library(rstatix)
library(emmeans)
library(ggridges)
library(car)
library(parallel)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
source('../R/plotting.R') # this has some stats functions we want to use in the document
source('../R/do_infer_stats.R') # extra functions for inferential stats

```

# Introduction

~100 words per paragraph

The brain is the most complex organism known to humans, yet decision making regarding theory for its function tends to be made on binary (i.e. pass or fail) terms, at least in the experimental psychological sciences. Specifically, theories often propose experimental tests for the presence or absence of given effects, rather than quantifying the extent to which an effect should be observed, i.e. the anticipated effect size. The latter prediction is more risky, and therefore constitutes a more desirable prediction for theory testing [insert Popper reference]. In fact, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. 

For example, in the study of EF: [AB: theory of p/f vs Ragnaroc?], MT costs - .  

Likewise, in the study of IL: VSL, CC, SRT

To promote quantised theories in experimental psychology, one extra piece of pertinent information that is informative to theory development is - we normally do x (insert Cummings refs), but it is missing y.

This is also useful for experimental development. i.e. The other important thing is that to ensure that we provide sufficiently precise information, so that this can be used. To do that we need to perform power calculations, There are at least 3 ways to do this: have a sufficiently precise theory that quantifies the effect size of interest, arbitrarily assume an effect size of theoretical interest, but unmotivated, or create an estimate by sampling the field. The problem with the latter is that given the current sample sizes typically employed, we have no idea if that estimate is precise and should be used.

A method to determine the precision of effect  

For example, theories in the experimental psychological sciences have tended only to predict the presence or absence, rather than the extent of a given phenomena. For example [example from implicit learning]. However, clear to see that if we are to understand how such a complex system processes sensory information, coordinates tasks and acquires new behavioural repertoires, a precise mapping between theory and outcome is going to be necessary. [get some Vehicles notions in that previous sentence]. Therefore important to start thinking about the size of effects - however, current state of field, very little knowledge about anticipated effect sizes due to x, y, & z.


Using a large dataset we will address this gap. Data on x-tasks. We will apply a simulation analysis to determine x, y and z.

Moreover, we will address two pertinent analytical gaps: i) A further development is the recent use of linear mixed effects models, and the recommendation that we use them instead of
ii) it is common to use t-test on accuracies against chance but Allefeld (VSL).

PUT IN INTRO
Additionally, given the documented advantages of linear mixed effects models (LME) over repeated-measures ANOVA [@muthAlternativeModelsSmall2016; @bagiellaMixedeffectsModelsPsychophysiology2000; @mccullochRepeatedMeasuresANOVA2005], and that only a conceptual proxy of $\eta_{p}^{2}$ is computable from these models [@brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014], and c) there exists no data that we know of that quantifies to what extent we can expect comparable outcomes between both methods, we (where relevant) opted to apply both the commonly used statistical model, and a LME model to each $k$ sample

INCLUDE Zoie's survey of the literature

# Methods

\label{sec:Method}


## Participants
\label{sec:Participants}

The current study uses a data set collected for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This data set contains performance measures from $N$ = 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by The University of Queensland Human Research Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

## Apparatus
\label{sec:Apparatus}

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 7 tasks; Attentional Blink (AB), Dual Task (DT), Contextual Cueing (CC), Serial Response Task (SRT), Visual Statistical Learning (VSL), Operation Span task and a Stop Signal Inhibition task. Only the data from the AB, DT, CC and SRT are reported here. 

## Procedures
\label{sec:Procedures}

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure \@ref(fig:FigureParadigm). Further details regarding the task protocols are presented within each section below. We first provide an overview of the simulation procedures, before detailing the specific procedural and statistical methods for each task.


```{r, FigureParadigm, out.width='70%', fig.cap='Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters, B) Multitasking Paradigm (MT). Participants make a discriminate the colour of a disc, a complex tone, or both C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task where they search for a rotated T among L distractors. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the blocks the stimulus follows a repeating sequence.'}

paradigm.fig.pth <- '../images/FigXXXX_alltasks.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

All the [data]() and [code](https://github.com/kel-github/Super-Effects) used for the current analysis are available online. All data were analysed using R -@rcoreteamLanguageEnvironmentStatistical2015 and RStudio -@rstudiocitation. The analysis of the data from each task followed two steps; first, to ascertain that we observed the typical findings for each of the paradigms, we applied the relevant conventional statistical model to the full dataset (N=313). The details of each analysis are presented below. Next, we implemented a simulation procedure to determine the effect size and p-values that would be attained over many experiments conducted at multiple levels of sample size. 

### Sampling procedure
\label{sec:SamplingProc}

For each task, we simulated experiments across 20 different sample sizes ($N$s), defined on a logarithmic interval between N=13 and N=313. We opted for a logarithmic interval given the decreasing information gained at higher $N$ values. To simulate $k$=1000 experiments at each of our chosen $N$, we sampled $N$ participants from $N_{max}$ over $k$ iterations. The relevant analysis was applied to each of the samples. Specific details regarding which analyses were applied to each $k$ sample are detailed below for each paradigm. Sampling with replacement ensured that the samples carried the Markov property. It may be of concern that any reductions in observed effect size variablility that may be attributable to saturation as the simulated $N$ approaches the maximum ($N_{max}$=313). Specifically, it could be that as $N$ approaches 313, the overlap of participants between subsamples may be greater than when $N$ equals a lower number such as 13. It follows then that any decreasing variability in effect size estimates at higher $N$s could be due to the decrease in variability of the subsamples, rather than the improved estimate of the population variance that should come with a larger $N$. We have run simulations that suggest that this is not the case and these are presented in appendix i.

_Effect Sizes_
For each task, we report the following information from the observed effect size densities: to assess the best estimate of the effect size and its variability, given our large dataset, we report the median and the standard deviation observed for our highest $N$ (apart from in one case of multimodality, where we report the points of highest probability). These values can be used to motivate power calculations for future studies. To test whether the best estimate differs from what is representative for the field, we next report the summary statistics for the $N$ that is closest to the median sample size from our survey of the literature. We use a qqplot to determine if the two distributions are different. To determine imprecision in effect size estimates across each level of $N$, we compute the Kullback-Leibler Divergence ($D_{KL}$) to determine how much information would be lost if the effect size distributions observed at each level of $N$ were used to approximate the distribution that reflects our best estimate, i.e. that observed with $N_{313}$.To quantify the relationship between information loss and $N$, we correlate $D_{KL}$ and $N$ using Spearman's Rho. 

To provide an estimate of the effect size that would be yielded through a meta-analysis of the published literature, we compute summary statistics for all analyses that yielded a statistically significant result (p<.05), we then present the difference between our observed effect sizes and our estimate of the meta-analytic effect size, for each level of $N$. 

_p Values_
To determine the $N$ required to achieve 90% power to reject the null hypothesis, we report the $N$ for which over 90% of p-values pass the threshold for significance for the effects of interest ($\alpha$=.05). To assess the range of p-values that one can expect to observe at some given $N$, i.e. the confidence for the most likely observed p-value, or certainty of the test outcome, we computed the difference between the the .025 and .975 quantiles ($q$) of the observed p-values at each level of $N$. We then computed the ratio between that range and that observed for the median $N$ for the field $N$ ($\frac{q_{N}}{q_{medianN}}$). Values below 1 suggest that the certainty over the p-value is lower for the median $N$ for the field, relative to the given $N$. As p-values clustered close to 0 in many instances, we applied the probit transform to rescale the values on the range [-$\infty$, $\infty$]. This allowed for a clearer visualisation of the spread of p-values at each level of $N$.

## Attentional Blink (AB)
\label{sec:ABMeth}

### Protocol

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

<br>

### Statistical Approach

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\eta_{p}^2$ and the resulting $p$ value were taken for the main effect of lag. For this task, and all remaining ANOVA tests, models were fit using the anova_test() function from the [rstatix](https://rpkgs.datanovia.com/rstatix/index.html) package. Where possible, the models were fit using type 3 sum of squares, owing to the computational expediency and match to commercial statistical software packages. In some cases, models were unable to be fit using type 3 sum of squares, owing to rank deficiencies in the underlying design matrix (e.g. when one participant was drawn more than twice within a sample). In these cases, models were fit using type 1 sum of squares. However, as the experiment designs were fully balanced, each sum of squares type should yield the same results.

## Multitasking (MT)
\label{sec:MTMeth}

### Protocol

The MT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

### Statistical Analysis

To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: single vs multitask) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\eta_{p}^2$ and $p$ are reported for the main effect of task. 

## Contextual Cueing (CC)
\label{sec:CCMeth}

### Protocol

The CC protocol was the same as that reported by Nydam et al -@nydamCathodalElectricalStimulation2018 which is modeled on Chun and Jiang -@chunContextualCueingImplicit1998. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80]. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1:12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable.

### Statistical Approach

To ascertain whether participants speeded responses to repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated arrays), the data were subject to a block (1:12) x condition (repeat vs novel array) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\eta_{p}^2$ and $p$ for the block x condition interaction. 
As sometimes in the literature a significant main effect and not an interaction are reported motivating the notion that contextual cueing reflects single-shot learning, we also report the $\eta_{p}^2$ and $p$ for the main effect of condition.

## Serial Response Task (SRT)
\label{sec:SRTMeth}

### Protocol

The SRT was adapted from [@nissenAttentionalRequirementsLearning1987]. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: 255, 0, 0] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'repeats'. Of interest is the RT for Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).

### Statistical Approach

To ascertain whether participants learned the repeating sequences, RTs in the final block of repeats (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d$, converted to $r^2$, and $p$ value from each test. 

<br>

# Results
\label{sec:Results}

## Attentional Blink
\label{sec:ABRes}

```{r, AB_analysis, message=F}
abfname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
ab_res <- do.AB.analysis(abfname)
```

An overview for the findings for the AB task are presented in Figure \@ref(fig:ABFX). As expected, proportion accuracy for T2|T1 decreased (by around `r sprintf("%.2f", abs(ab_res[[2]][,3]$estimate))`) when T2 was presented at lag 2, relative to lag 7. A one-way ANOVA revealed that the effect of lag was statistically significant (F (`r sprintf("%.1f", ab_res[[1]]$DFn)`), `r sprintf("%.0f", ab_res[[1]]$DFd)`) = `r sprintf("%.0f", ab_res[[1]]$F)`, $\eta_{p}^2$ = `r sprintf("%.2f", ab_res[[1]]$pes)`, p = `r sprintf("%.2e", ab_res[[1]]$p)`). Post-hoc t-tests showed that accuracy at each lag differed statistically from accuracy at each of the other lags (all p's $\leq$ `r sprintf("%.2e", max(do.call(rbind, lapply(1:6, function(x) cbind(ab_res[[2]][,x]$p.value)))))`), with lower accuracy values at the shorter relative to the longer lags. Therefore we see that our implementation of the AB paradigm yielded the typically observed effects.

```{r, AB_results, message=F}
load("../data/AB/IMMABstats.RData")
```

```{r, ABFX, out.width='80%', fig.cap='AB Effects. A) Violin plot showing accuracy across subjects for T1 and T2|T1 by lag. As was expected, accuracy for T2|T1 was poorer at shorter relative to longer lags. B) Observed effect size densities for simulated experiments for select $N$, for the RM-ANOVA and LME analyses. C) QQ-plot of the density of effect sizes observed for the median $N$ (25) against that observed for $N_{max}$. D) KL divergenence when the effect size density for each $N$ is used to approximate the density for $N_{max}$. E) The difference between the observed and meta-analytic effect size was 0, across all $N$.'}

paradigm.fig.pth <- '../images/IMMAB_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

```{r ABinf, message = F, echo=F}
# compute inferential stats on AB effect size measures
kl_inputs <- list(origin = "313", dv = "dens_fx", sub_Ns = paste(round(exp(seq(log(13), log(313), length.out = 20)))))
AB_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(AB_KL_corrs) <- c("RM-AN", "LME")
```

_Effect sizes_ As can be seen in Figure \@ref(fig:ABFX), panel B, increasing $N$ both reduced the variability of observed effect sizes. Unsurprisingly, given the many demonstrations of the AB effect in the experimental psychology literature, the best estimate of the effect size (with $N_{313}$) was large (median $\eta_{p}^2$ = `r sprintf(".2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf(".2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$sd)`). This estimate was comparable, although slightly smaller for the typical N of the field ($N_{25}$, median $\eta_{p}^2$ = `r sprintf(".2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf(".2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$sd)`. This suggests that the long running median of effect sizes observed across AB studies would provide a reasonable estimate of the underlying effect. Examination of the QQ-plot between $\eta_{p}^2$ densities for $N_{25}$ and $N_{313}$ demonstrates higher variability for $N_{25}$, suggesting that imprecision in effect size estimates does exist in the field currently (Figure \@ref(fig:ABFX), panel C). In support of this, there was a strong negative relationship between $D_{KL}$ and $N$ ($r_{s}$(`r sprintf("%.0f", AB_KL_corrs[["RM-AN"]]$parameter)`) = `r sprintf("%.2f", AB_KL_corrs[["RM-AN"]]$estimate)`, p = `r sprintf("%.2e", AB_KL_corrs[["RM-AN"]]$p.value)`, see Figure \@ref(fig:ABFX), panel D). This suggests significant information loss when using experiments of lower N to produce effect size estimates for the AB. However, owing to the large effect under study, there were no differences between the observed effect size distributions and the estimated meta-analytic effect size, across all levels of $N$ (panel E), suggesting that the AB effect is large enough to yield consistent statistically significant results (and presumably publication). 

_p Values_ All observed p-values were < .05, even for the lowest sample size ($N_{13}$). (see Figure \@ref(fig:ABps), panel A), showing that decision-outcomes are likely the same across all AB experiments. Panel B shows the ratio between the 97.5 quantiles for $N_{med}$ relative to the other N's. Certainty around the most likely $p$ increased with increasing sample size up to a ratio of `r sprintf("%1.2f", diff(res[19,"stats_p"][[1]][,"RM-AN"]$qs)/diff(res['25',"stats_p"][[1]][,"RM-AN"]$qs))`. However, the median moved further from p=.05, so the certainty in the decision increases with increasing $N$. 

```{r, ABps, out.width='40%', fig.cap='Probit transformed p-values from the AB analysis, presented for selected N. A) p-values attained from the RM-ANOVA model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result, C) Ratio between the range of p-values observed for each level of N, relative to the median N (25) for the field'}
paradigm.fig.pth <- '../images/IMMAB_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## Multitasking

```{r, MT_F, message=F}
mtfname <- "../data/total_of_313_subs_SingDual_task_trial_level_data.csv"
mt_res <- do.MT.analysis(mtfname)
```

As was anticipated, the RTs were slowed on multitask relative to single task conditions (see Figure \@ref(fig:MTFX)). RTs were on average `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$estimate[[1]]))` (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[2]))`, `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[1]))`] seconds (s) slower on MT trials (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "trialtype"])`) = `r sprintf("%4.0f", mt_res[[1]]$F[mt_res[[1]]$Effect == "trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "trialtype"])`, p<.0001). There was also a significant task modality (sound or visual) x task (ST vs MT) interaction  (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "task:trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "task:trialtype"])`) = `r sprintf("%2.1f", mt_res[[1]]$F[mt_res[[1]]$Effect == "task:trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "task:trialtype"])`, p<.0001), with the MT cost (MT RT - ST RT) being larger for sound relative to visual tasks by on average `r sprintf("%.2f", abs(mt_res[[2]]$interaction$estimate[[1]]))` (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[1]))`, `r sprintf(sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[2])))`] s. This latter finding is typical in the multitasking literature (@hazeltineModalityPairingEffects2006, @garnerTransferabilityTrainingBenefits2015a), but is not pertinent to the focus of the current work which seeks to quantify the size of the main effect of multitasking cost.


```{r, MT_results, message=F}
load("../data/SD/SDstats.RData")
```


```{r, MTFX, out.width='80%', fig.cap='MT Effects. A) Violin plots of mean RTs for each multitasking condition broken down by task modality. As was expected, RTs were slower for multitasking relative to single-task conditions. B) Observed effect size densities for simulated experiments for select $N$. C) QQ-plot of $N_{med}$ = 25 against $N_{max}$. D) KL divergenence when the density for each $N$ is used to approximate the density for N=313. E) The difference between the observed and meta-analytic effect size was 0 across all N. S = single-task, M = multi-task, So = sound manual task, V = visual manual task'}

paradigm.fig.pth <- '../images/IMMSD_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

```{r MTinf, message = F}
kl_inputs <- list(origin = "313", dv = "dens_fx", sub_Ns = paste(round(exp(seq(log(13), log(313), length.out = 20)))))
MT_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(MT_KL_corrs) <- c("RM-AN", "LME")

```


_Effect sizes_ Similar to the AB findings, the effect size results for the MT paradigm show that increasing $N$ reduced both the median estimate and the variability of $\eta_{p}^2$ (see Figure \@ref(fig:MTFX)). The best estimate of the effect size (with $N_{313}$) was large (median $\eta_{p}^2$ = `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$sd)`). This estimate was comparable to that observed for the typical N of the field ($N_{25}$, mean $\eta_{p}^2$ = `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$sd)`. Therefore the median estimate across all experiments from the field likely provides a reasonable estimate of the effect size for multitasking costs. However, examination of the QQ-plot between $\eta_{p}^2$ densities for $N_{25}$ and $N_{313}$ demonstrates higher variability for $N_{25}$ (Figure \@ref(fig:ABFX), panel C). Corrobatively, a strong negative relationship between $D_{KL}$ and $N$ ($r_{s}$ =`r sprintf("%2.0f", MT_KL_corrs[["RM-AN"]]$parameter)`) = `r sprintf("%.2f", MT_KL_corrs[["RM-AN"]]$estimate)`, p = `r sprintf("%.2e", MT_KL_corrs[["RM-AN"]]$p.value)`, Figure \@ref(fig:MTFX), panel D), suggests that any given effect size from $N_{med}$ holds a higher potential to be a misleading estimate. Also similar to the AB results and owing to the large effect size under study, there were no differences between the observed effect size distributions and the estimated meta-analytic effect size, across all levels of $N$ (panel E). 

_p Values_ As with the AB, all observed p-values were < .05, even the lowest sample size ($N_{13}$), see Figure \@ref(fig:MTps), panel A for example $p$ distributions). As can be seen in panel B, certainty in the median estimate of $p$ for $N_{25}$ is higher `r sprintf("%1.2f", diff(res[19,"stats_p"][[1]][,"RM-AN"]$qs)/diff(res['25',"stats_p"][[1]][,"RM-AN"]$qs))` times higher than $N_{313}$. However certainty in the decision as manifest in distance from p=.05, increased with each $N$.

```{r, MTps, out.width='40%', fig.cap='Probit transformed p-values from the MT analysis, presented for selected N. A) p-values attained from the ANOVA model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result, C) Ratio between the range of p-values observed for each level of N, relative to the median N (25) for the field'}
paradigm.fig.pth <- '../images/IMMSD_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## Contextual Cueing
\label{sec:CCRes}

```{r, CC_interaction, message=F}
ccfname <- "../data/total_of_313_subs_CC_task_trial_level_data.csv"
cc_res <- do.CC.analysis(ccfname)
```

In accordance with the notion that RTs for repeat displays speed up more than for novel displays (i.e. that participants learn the repeat displays), a significant block x display interaction was found (F (`r cc_res[[1]]$DFn[3]`, `r cc_res[[1]]$DFd[3]`) = `r cc_res[[1]]$F[3]`, $\eta_{p}^2$ = `r cc_res[[1]]$pes[3]`, p = `r cc_res[[1]]$p[3]`). While there was no statistically significant difference between repeat and novel displays in the first block (t (`r cc_res[[2]][[1]]$parameter`) = `r cc_res[[2]][[1]]$statistic`, p = `r cc_res[[2]][[1]]$p.value`, $\mu$ difference = `r cc_res[[2]][[1]]$estimate/1000` sec), RT's to repeat displays were on average `r cc_res[[2]][[2]]$estimate/1000` sec faster than novel displays by block 12 (t (`r cc_res[[2]][[2]]$parameter`) = `r cc_res[[2]][[2]]$statistic`, p = `r cc_res[[2]][[2]]$p.value`, see Figure \@ref(fig:CCFX), panel A).


```{r, CC_results, message=F}
load("../data/CC/CCstats.RData")
```

```{r, CCinf, message = F}
# compute inferential stats on AB effect size measures

CC_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(CC_KL_corrs) <- c("RM-AN", "LME")



CC_best_vs_typical <- comp_z(res['313', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`,
                             res['313', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`)
CC_best_vs_typical_p <- 2*pnorm(q=abs(CC_best_vs_typical), lower.tail=FALSE)

```

```{r, CCFX, out.width='80%', fig.cap='CC Effects. A) Group mean RT plotted by block (x-axis) and display type (repeat vs novel). Participants became faster for repeat relative to random trials. B) Observed effect size densities for simulated experiments for select $N$, for the ANOVA and LME analyses. C) KL divergenence when the density for each $N$ is used to approximate the density for $N_{313}$. D) The difference between the observed and meta-analytic effect for both ANOVA and LME approaches, across all N. A value below 0 suggests an inflated estimate of effect size in the literature. E) Difference between the mean effect size observed for the ANOVA and LME analyses at each level of N'}

paradigm.fig.pth <- '../images/CC_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)
```


_Effect sizes_ As can be seen in Figure \@ref(fig:CCFX), panel B, increasing $N$ reduced the central tendency of the effect size distributions, to a greater extent than was observed for the AB and MT data. Also in contrast to the AB and MT data, the best estimate of the effect size was small ($N_{313}$ mean $\eta_{p}^2$ = `r res['313', 'stats_fx'][[1]][,1][[1]]`, sd: `r res['313', 'stats_fx'][[1]][,1][[2]]`), suggesting that the key manipulation oshould only generalise to 2% of the population. Moreover, this estimate was statistically smaller than that observed for the typical N of the field ($N_{25}$, mean $\eta_{p}^2$ = `r res['25', 'stats_fx'][[1]][,1][[1]]`, sd: `r res['25', 'stats_fx'][[1]][,1][[2]]`, $z$ = `r CC_best_vs_typical`, p < .0001). This suggests that effect size estimates from the published literature would result in a misinformed estimate of the true effect size. Again, there was a strong negative relationship between $D_{KL}$ and $N$ for both the ANOVA ($r_{s}$(`r CC_KL_corrs[["RM-AN"]]$parameter`) = `r CC_KL_corrs[["RM-AN"]]$estimate`, p = `r CC_KL_corrs[["RM-AN"]]$p.value`) and LME approaches ($r_{s}$(`r CC_KL_corrs[["LME"]]$parameter`) = `r CC_KL_corrs[["LME"]]$estimate`, p = `r CC_KL_corrs[["LME"]]$p.value` see Figure \@ref(fig:CCFX), panel C). This suggests significant information loss when using experiments of lower N to produce effect size estimates for CC. The LME approach resulted in consistent low effect size estimates over all levels of $N$, therefore, the overall information to be gained by increasing $N$ is lower, relative to the ANOVA approach. Given the small effect size observed, our estimate of the meta-analytic effect size was consistently larger than the observed effect size, up until $N_{97}$. This difference was present, but smaller for the LME approach. Although $r^2$ estimates from the LME analysis yielded lower effect size estimates relative to $\eta_{p}^2$ for lower $N$ values, this difference reduced at an exponential rate, moving towards convergence at $N_{313}$  (Figure \@ref(fig:CCFX), panel E). Thus, when effect sizes are small, and $N$ is sufficiently large, both modelling approaches will yield a comparable effect size estimate. We return to this observation in the discussion.

_p Values_ In contrast to the AB and MT results, 115 participants were required to achieve 90 % power to reject the null hypothesis using the ANOVA approach, assuming the null is false. For the LME appraoch, 90 % power was achieved with 265 participants (see Figure \@ref(fig:CCps)). In contrast to the AB and MT findings, $p$-values were lower for the ANOVA approach, therefore use of LME models results in lower certainty that the null hypothesis is unlikely to be true. As can be seen in panel C, and in contrast to the AB and MT data, uncertainty around $p$ for the median $N$ for the field ($N_{25}$) is lower than $N_{313}$ with a ratio of `r abs(diff(res['313',"stats_p"][[1]][[3,1]]))/abs(diff(res['25',"stats_p"][[1]][[3,1]]))` for the ANOVA approach. This was not the case for the LME approach, where uncertainty was instead lower for $N_{313}$ relative to $N_{25}$ (ratio: `r abs(diff(res['313',"stats_p"][[1]][[3,2]]))/abs(diff(res['25',"stats_p"][[1]][[3,2]]))`). From panel C, it can be seen that $N_{25}$ actually yields the lowest uncertainty in $p$-values when using ANOVA, relative to every other simulated $N$. We return to this result in the discussion.


```{r, CCps, out.width='40%', fig.cap='Probit transformed p-values from the CC analysis, presented for selected N. A) p-values attained from the RM-ANOVA model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result. C) Ratio between the range of p-values observed for each level of N, relative to the median N (25) for the field'}
paradigm.fig.pth <- '../images/CC_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## SRT

```{r, SRT_F, message=F}
srtfname <- "../data/total_of_313_subs_SRT_task_trial_level_data.csv"
srt_res <- do.SRT.analysis(srtfname)
```

In line with the notion that participants learn the repeating sequence, RTs were statistically lower for the repeat relative to the random condition (t(`r srt_res[[1]]$parameter[[1]]`) = `r srt_res[[1]]$statistic[[1]]`, $r^2$ = `r srt_res[[3]]`, p< .0001), by ~ `r srt_res[[1]]$estimate` s (95% CI [`r srt_res[[1]]$conf.int[[1]]`, `r srt_res[[1]]$conf.int[[2]]`]). This is in accordance with the standard findings in the SRT literature.

```{r, SRT_results, message=F}
load("../data/SRT/SRTstats.RData")
```

```{r SRTinf, message = F}
# compute inferential stats on AB effect size measures

SRT_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(SRT_KL_corrs) <- c("RM-AN", "LME")



SRT_best_vs_typical <- comp_z(res['313', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`,
                             res['313', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`)
SRT_best_vs_typical_p <- 2*pnorm(q=abs(CC_best_vs_typical), lower.tail=FALSE)

```


```{r, SRTFX, out.width='80%', fig.cap='SRT Effects. A) Group mean RT plotted by block (x-axis) and block type (random vs repeat). Participants became faster for repeat trials. B) Observed effect size densities for simulated experiments for select N, for the t-test and LME analyses. C) KL divergenence when the density for each $N$ is used to approximate the density for  = 313. Note, the KL divergence from the LME approach is plotted as points, given its close convergence to what was observed for the effect sizes yielded from the t-test approach. D) The difference between the observed and meta-analytic effect for both t-test and LME approaches, across all N. A value below 0 suggests an inflated estimate of effect size in the literature. E) Difference between the mean effect size observed for the t-test and LME approaches at each level of N'}

paradigm.fig.pth <- '../images/SRT_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)
```


_Effect sizes_ As can be seen in Figure \@ref(fig:SRTFX), panel B, the effect size densities yielded from the $t$ and LME approaches formed bimodal distributions at lower $N$ (see $N_{25}$, panel B), apparently converging on a trimodal distribution at higher $N$ (see densities for remaining $N$ on panel B). This suggests that the SRT task may actually tap multiple latent sources of variance. The points of highest probability for the trimodal distribution observed for $N_{313}$ were $r^2$ = .23, .18, and .28 (from highest $p$ to lowest). A different pattern was found for the typical N of the field ($N_{36}$), with the points of highest probability being either smaller ($r^2$ = .04, .08) or larger ($r^2$ = .29) than what was observed for $N_{313}$. This suggests that the current literature likely under or over-estimates the size of the effects that underlie responses to the SRT task. Again, there was a strong negative relationship between $D_{KL}$ and $N$ for both the ANOVA ($r_{s}$(`r SRT_KL_corrs[["RM-AN"]]$parameter`) = `r SRT_KL_corrs[["RM-AN"]]$estimate`, p = `r SRT_KL_corrs[["RM-AN"]]$p.value`) and LME approaches ($r_{s}$(`r SRT_KL_corrs[["LME"]]$parameter`) = `r SRT_KL_corrs[["LME"]]$estimate`, p = `r SRT_KL_corrs[["LME"]]$p.value` see Figure \@ref(fig:SRTFX), panel C). This shows significant information loss when using experiments of lower N to produce effect size estimates for SRT. The meta-analytic effect size was larger than the observed effect size, up until $N_{50}$, suggesting discrepancy between the true effect size and that reported in the literature. This difference was not present for the LME approach. Although $r^2$ estimates from the LME analysis yielded lower effect size estimates relative to those yielded from the $t$-statistic approach, this difference was consistently small, with a trend towards a reduction in difference at higher levels of $N$ (Figure \@ref(fig:SRTFX), panel E). This data opens the possibility that there is a mild filedrawer effect in the SRT literature, and shows for the first time to our knowledge that it is unlikely that a single effect underlies the influence of the manipulated variables on behaviour. We return to this point in the discussion.

_p Values_ For the SRT data, 13 participants was sufficient to achieve 90 % power to reject the null hypothesis using both analysis approaches (see Figure \@ref(fig:SRTps)). In line with what was observed for the AB and MT data, $p$-values were lower for the LME approach, therefore use of LME models results in higher certainty that the null hypothesis is unlikely to be true. As can be seen in panel C, and comparably to the CC data, uncertainty around $p$ for the median $N$ for the field ($N_{36}$) is lower than $N_{313}$ with a ratio of `r abs(diff(res['313',"stats_p"][[1]][[3,1]]))/abs(diff(res['36',"stats_p"][[1]][[3,1]]))` for the ANOVA approach. This was not the case for the LME approach, where uncertainty was lower for $N_{313}$ relative to $N_{25}$ (ratio: `r abs(diff(res['136',"stats_p"][[1]][[3,2]]))/abs(diff(res['36',"stats_p"][[1]][[3,2]]))`).


```{r, SRTps, out.width='40%', fig.cap='Probit transformed p-values from the SRT analysis, presented for selected N. A) p-values attained from the t-test model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result. C) Ratio between the range of p-values observed for each level of N, relative to the median N (36) for the field'}
paradigm.fig.pth <- '../images/SRT_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```


# Discussion

AB Discussion: taking published effect sizes to motivate power calculations will result in an inflated effect size estimate. Increasing N has a strong power/exponential relationship with precision of estimate, regardless of modelling choice. No file drawer effect. LME results in comparable NHST decision making, but simple model structure means a smaller estimate of the effect size. Model choice matters.


\clearpage

# References
\label{sec:Refref}