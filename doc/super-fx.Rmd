---
title: Quantifying effect sizes in implicit learning tasks; the role of some stuff
authors:
  - name: Kelly G. Garner
    thanks: Corresponding author
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, UK, B13 2TT
    email: getkellygarner@gmail.com
  - name: Christopher R. Nolan
    department: School of Psychology
    affiliation: University of New South Wales
    location: Sydney, Australia
    email: cnolan@cn.id.au
  - name: Abbey Nydham
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Zoie Nott
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Howard Bowman
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, Birmingham, B13 2TT
    email: h.bowman@bham.ac.uk
  - name: Paul E. Dux
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
    email: paul.e.dux@gmail.com
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: refs.bib
biblio-style: unsrt
output: 
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
    keep_tex: true
  bookdown::word_document2:
    toc: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
#opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
library(rstatix)
library(emmeans)
library(ggridges)
library(car)
library(parallel)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
source('../R/plotting.R') # this has some stats functions we want to use in the document
source('../R/do_infer_stats.R') # extra functions for inferential stats
```

# Introduction

~100 words per paragraph

The brain is the most complex organism known to humans, yet decision making regarding theory for its function tends to be made on binary (i.e. pass or fail) terms, at least in the experimental psychological sciences. Specifically, theories often propose experimental tests for the presence or absence of given effects, rather than quantifying the extent to which an effect should be observed, i.e. the anticipated effect size. The latter prediction is more risky, and therefore constitutes a more desirable prediction for theory testing [insert Popper reference]. In fact, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. 

For example, in the study of EF: [AB: theory of p/f vs Ragnaroc?], MT costs - .  

Likewise, in the study of IL: VSL, CC, SRT

To promote quantised theories in experimental psychology, one extra piece of pertinent information that is informative to theory development is - we normally do x (insert Cummings refs), but it is missing y.

This is also useful for experimental development. i.e. The other important thing is that to ensure that we provide sufficiently precise information, so that this can be used. To do that we need to perform power calculations, There are at least 3 ways to do this: have a sufficiently precise theory that quantifies the effect size of interest, arbitrarily assume an effect size of theoretical interest, but unmotivated, or create an estimate by sampling the field. The problem with the latter is that given the current sample sizes typically employed, we have no idea if that estimate is precise and should be used.

A method to determine the precision of effect  

For example, theories in the experimental psychological sciences have tended only to predict the presence or absence, rather than the extent of a given phenomena. For example [example from implicit learning]. However, clear to see that if we are to understand how such a complex system processes sensory information, coordinates tasks and acquires new behavioural repertoires, a precise mapping between theory and outcome is going to be necessary. [get some Vehicles notions in that previous sentence]. Therefore important to start thinking about the size of effects - however, current state of field, very little knowledge about anticipated effect sizes due to x, y, & z.


Using a large dataset we will address this gap. Data on x-tasks. We will apply a simulation analysis to determine x, y and z.

Moreover, we will address two pertinrent analytical gaps: i) A further development is the recent use of linear mixed effects models, and the recommendation that we use them instead of
ii) it is common to use t-test on accuracies against chance but Allefeld (VSL).

PUT IN INTRO
Additionally, given the documented advantages of linear mixed effects models (LME) over repeated-measures ANOVA [@muthAlternativeModelsSmall2016; @bagiellaMixedeffectsModelsPsychophysiology2000; @mccullochRepeatedMeasuresANOVA2005], and that only a conceptual proxy of $\eta_{p}^{2}$ is computable from these models [@brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014], and c) there exists no data that we know of that quantifies to what extent we can expect comparable outcomes between both methods, we (where relevant) opted to apply both the commonly used statistical model, and a LME model to each $k$ sample

INCLUDE Zoie's survey of the literature

# Methods

\label{sec:Method}


## Participants
\label{sec:Participants}

The current study uses a data set collected for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This data set contains performance measures from $N$ = 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by The University of Queensland Human Research Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

## Apparatus
\label{sec:Apparatus}

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 5 tasks; Attentional Blink (AB), Dual Task (DT), Contextual Cueing (CC), Serial Response Task (SRT), and Visual Statistical Learning (VSL). Task order was randomised for each participant, apart from the VSL task, which was presented last. This was because the recognition component of the task may have allowed participants to infer that other tasks were also assessing implicit learning. 

## Procedures
\label{sec:Procedures}

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure \@ref(fig:FigureParadigm). Further details regarding the task protocols are presented within each section below. We first provide an overview of the simulation procedures, before detailing the specific procedural and statistical methods for each task.


```{r, FigureParadigm, out.width='70%', fig.cap='Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters, B) Multitasking Paradigm (MT). Participants make a discriminate the colour of a disc, a complex tone, or both C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the blocks the stimulus follows a repeating sequence. E) Visual Statistical Learning Paradigm (VSL): i) 12 shapes are grouped into 6 base pairs. ii) Learning: three of the six pairs are presented as an array, this is repeated as participants passively view the displays. iii) Test: participants are presented with a base pair, and a novel pair formed from a recombination of the 12 shapes, and is asked which of the two pairs they have seen previously.'}

paradigm.fig.pth <- '../images/FigXXXX_alltasks.png'
knitr::include_graphics(paradigm.fig.pth)

```

All the [data]() and [code](https://github.com/kel-github/Super-Effects) used for the current analysis are available online. All data were analysed using R @rcoreteamLanguageEnvironmentStatistical2015 and RStudio -@rstudiocitation. The analysis of the data from each task followed two steps; first, to ascertain that we observed the typical findings for each of the paradigms, we applied the relevant conventional statistical model to the full dataset (N=313). The details of each analysis are presented below. Next, we implemented a simulation procedure to determine the effect size and p-values that would be attained over many experiments conducted at multiple levels of sample size. 

### Sampling procedure
\label{sec:SamplingProc}

For each task, we simulated experiments across 20 different sample sizes ($N$s), defined on a logarithmic interval between N=13 and N=313. We opted for a logarithmic interval given the decreasing information gained at higher $N$ values. To simulate $k$ experiments at each of our chosen $N$, we developed a sampling procedure that sought to leverage information from across the whole dataset while also protecting against any reductions in effect size variablility that may be attributable to saturation as $N$ approaches the maximum ($N_{max}$=313). Specifically, it could be that as $N$ approaches 313, the overlap of participants between subsamples may be greater than when $N$ equals a lower number such as 13. It follows then that any decreasing variability in effect size estimates at higher $N$s could be due to the decrease in variability of the subsamples, rather than the improved estimate of the population variance that should come with a larger $N$. 

To protect against this possibility we applied the following procedure; for each level of $N$ ($N_{1}, N_{2}, ...N_{20}$), we first selected a subsample from the total dataset _without_ replacement, e.g. $N$ of 13 unique samples from the total $N$=313. We refer to this from now on as the _parent subsample_. From this parent subsample, we sampled $k$ = 1000 times _with_ replacement - e.g. $N$=13 sampled 1000 times from $N$=13. These shall now be referred to as the _child subsamples_. The relevant analysis was then applied to each of the child subsamples. Sampling with replacement ensured that the child subsamples carried the Markov property. Given that this procedure reduces the heterogeneity of the parent sample (for example, in the case of $N$=13, all the child subsamples are derived from only 13 unique observations), we then repeated the entire process over $j$ = 1000 iterations. Therefore the presented data reflects $j * k$ simulated experiments for each level of $N$. We refer to this now as the two-step sampling procedure: $j$ = 1000, $k$ = 1000. It is worth noting that we compared this procedure to one where we performed the two-step sampling procedure with $j$ = 1, and to a one-step sampling procedure where we sampled $N$ with replacement from the entire dataset, i.e. $k$=1000 and $j$ = 0. Outcomes were broadly comparable between the sampling procedures, although for lower $N$ (e.g. 25) the two step procedure where $j$ = 1 and $k$ = 1000 appears more susceptible to produce a biased set of observations, which is unsurpising given the dependence on the composition of the parent sample. The two-step procedure ($j$ = 1000) offered better resolution of the resulting densities (see Figure \@ref(fig:figSuppSamp) for a representative example). 


```{r figSuppSamp, out.width='70%', fig.cap='Comparison between sampling procedures for the AB task data fit with a repeated measures ANOVA for selected $N$; densities of observed effect sizes for the two-step sampling procedure $j$=1000, $k$=1000, the two-step sampling procedure, $j$=1, $k$=1000, and the one-step procedure, $k$ = 1000', message=F}
# plot.d.by.samp(dat, px_rng_d, 1, mod) + xlab(bquote(eta['p']^2)) + theme(axis.text.x = element_text(size=11),
samp.fig.pth <- '../images/AB_sampling.pdf'
knitr::include_graphics(samp.fig.pth)      

```


Specific details regarding which analyses were applied to each $k$ (child) subsample are detailed below for each paradigm. Note that given the iterative fitting procedure that is inherent to the LME approach, the models specified were the simplest possible, as the computational requirements for fitting more complex models would have resulted in simulations running for over 11 months. In the current work we present examples of the simplest case, to provide a basis for comparison, or a set of minimum assumptions for researchers interested in applying LME models to their data on these or comparable paradigms. That being said, application of this model is not without precedent in the literature, given that this simple case is often taken as an analysis choice []. 

_Effect Sizes_
For each task, we report the following information from the observed effect size densities: to assess the best estimate of the effect size and its variability, given our large dataset, we report the central tendency and the standard deviation observed for our highest $N$ (apart from in one case of bimodality, where we report the two points of highest probability). These values can be used to motivate power calculations for future studies. To test whether the best estimate differs from what is representative for the field, we next report the summary statistics for the $N$ that is closest to the median sample size from our survey of the literature. We use a two-sample $z$ test to determine if the two observations are statistically different. To determine imprecision in effect size estimates across a range of $N$, we compute the Kullback-Leibler Divergence ($D_{KL}$) between the effect size distributions observed at each level of $N$, and the distribution that reflects our best estimate, i.e. that observed with $N_{313}$. This measure quantifies the information lost (in bits), should we use the distribution observed at any level of $N$ as an approximation for our best estimate. To quantify the relationship between information loss and $N$, we correlate $D_{KL}$ and $N$ using Spearman's Rho. 

To provide an estimate of the effect size that would be yielded through a meta-analysis of the published literature, we compute summary statistics for all analyses that yielded a statistically significant result (p<.05), we then compute two-sample $z$-tests between our observed effect sizes and our estimate of the meta-analytic effect size, for each level of $N$. To compare outcomes between the standard and LME approaches (or in the case of VSL, the t-test and prevalence statistic approaches, see below), we perform a $z$-test between the distributions yielded by each method, at each level of $N$. An FDR correction was used to control for multiple comparisons.  

_p Values_
To determine the $N$ required to achieve 90% power to reject the null hypothesis, we report the $N$ for which over 90% of p-values pass the threshold for significance ($\alpha$=.05). To assess the extent to which the range of p-values that one can expect to observe at a given $N$, i.e. the confidence for the most likely observed p-value, we report the .025 and .975 quantiles for the observed p-values for each N. To compare the standard and the LME approaches, we then determine whether there is agreement in the $N$ required to achieve greater than 90% statistical power. As p-values clustered close to 0 in many instances, we applied the probit transform to rescale the values on the range [-$\infty$, $infty$]. This allowed for a clearer visualisation of the spread of the p-value densities.

## Attentional Blink (AB)
\label{sec:ABMeth}

### Protocol

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

<br>

### Statistical Approach

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\eta_{p}^2$ and the resulting $p$ value were taken for the main effect of lag. For this task, and all remaining ANOVA tests, models were fit using the anova_test() function from the [rstatix](https://rpkgs.datanovia.com/rstatix/index.html) package. Where possible, the models were fit using type 3 sum of squares, owing to the computational expediency and match to commercial statistical software packages. In some cases, models were unable to be fit using type 3 sum of squares, owing to rank deficiencies in the underlying design matrix (e.g. when one participant was drawn more than twice within a child subsample). In these cases, models were fit using type 1 sum of squares. However, as the experiment designs were fully balanced, each sum of squares type should yield the same results.

We also applied a LME to estimate the fixed effect of lag, with a random intercept for each participant ($j$) and an intercept for random error ($i$): 

\begin{equation}
  y_{ij} \sim \beta_{0} + \beta_{1}lag + e_{j} + e_{ij}
  (\#eq:ABLME)
\end{equation}

This, and all subsequent LME's were fit using the lmer() function from the lme4 package @batesFittingLinearMixedEffects2015. To compute an effect size estimate from the LME approach, a proxy for $d$ can be calculated as the ratio of the estimated fixed effect of interest (lag) to the squareroot of the sum of the variance accounted for by the random effects in the model (see @brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014):

\begin{equation}
  d = \frac{lag}{\sqrt{{\sigma_{e_{i}} + \sigma_{e_{ij}}}}}
  (\#eq:LMEd)
\end{equation}

To facilitate comparison with $\eta_{p}^2$ (which is a generalisation of $r^2$, see Cohen -@cohenStatisticalPowerAnalysis1988), the resulting $d$ value was converted to $r^2$ using the following from Cohen -@cohenStatisticalPowerAnalysis1988:

\begin{equation}
  r^2 = \biggl(\frac{d}{\sqrt(d^2 + \frac{1}{pq})}\biggr)^2
  (\#eq:d2r)
\end{equation}

For this and for all subsequent applications of LME models, the $p$ value for the regressor of interest (in this case 'lag') was attained by applying Wald's chi-square test as implemented using the Anova function from the car package [@foxCompanionAppliedRegression2018] (note: all the results were comparable regardless of whether we used Wald's test or whether we performed a log-likelihood test between the model of interest and a null model, which for the current AB case contained only the two random intercept terms $e_{ij}$ and $e_{i}$). 

## Multitasking (MT)
\label{sec:MTMeth}

### Protocol

The MT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two possible coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two possible sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

### Statistical Analysis

To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: single vs dual) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\eta_{p}^2$ and $p$ are reported for the main effect of task. We also applied an LME model which included fixed effects regressors for task-modality, task and their interaction, and random intercepts for participants $e_{ij}$, and measurement error $e_{i}$. As above, $d$ was computed as the ratio of the estimated fixed effect for the task regressor to the square root of the summed variance of the estimated random effects, and was converted to $r^2$. Again, Wald's chi-square test was applied to attain a $p$ value for the main effect of task.

## Contextual Cueing (CC)
\label{sec:CCMeth}

### Protocol

The CC protocol was the same as that reported by Nydam et al -@nydamCathodalElectricalStimulation2018. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1-12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable of interest.

### Statistical Approach

To ascertain whether participants speeded responses to repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated arrays), the data were subject to a block (1:12) x condition (repeat vs novel array) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\eta_{p}^2$ and $p$ for the block x condition interaction. We also applied LME models, containing fixed effects regressors for block, condition and their interaction, and random intercepts for subjects and general measurement error. We report $r^2$ for the block x condition interaction and the resulting $p$ value from Wald's chi-square test.

## Serial Response Task (SRT)
\label{sec:SRTMeth}

### Protocol

The SRT was adapted from [@nissenAttentionalRequirementsLearning1987]. The task began with a []. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: ] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'repeats'. Of interest is the RT for Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).

### Statistical Approach

To ascertain whether participants learned the repeating sequences, RTs in the final block of repeats (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d$, converted to $r^2$, and $p$ value from each test. We also applied an LME approach where we fit models with a fixed effects regressor for block (3 vs 4), and included the same random intercepts as described above. We report the resulting $r^2$ and $p$ values for the fixed effect of block.

<br>

#### Visual Statistical Learning (VSL)
\label{sec:VSLMeth}

##### Protocol

The VSL task was adapted from Fiser and Aslin [-@fiserUnsupervisedStatisticalLearning2001]. For each participant, twelve abstract shapes were grouped into 6 base pairs. On each trial of the learning phase, three base pairs were randomly positioned onto a 3 x 3 grid, which subtended ?$^\circ$ of visual angle. Each shape subtended ?$^\circ$. A total of 144 of these displays were each presented for 2000 ms, interleaved with a blank screen presented for 1000 ms. Participants were instructed to pay attention to the displays. The test phase consisted of trials where two shape pairs were each presented for 2000 ms, with an intervening blank screen lasting 1000 ms. One pair was selected from the 6 base pairs for that participant, and one pair was formed by a novel recombination formed by randomly selecting two of the 12 shapes, with the constraint that a base pair could not be selected. Participants were instructed to report which of the two pairs was more familiar, using the 'z' and 'm' keys to denote the first or second pair respectively, with the exact mapping counterbalanced across participants. Paticipants completed a total of 24 trials, with each base pair presented 4 times. The order of base and novel pairs was counterbalanced across trials, and trial order was randomised for each participant. Response accuracy in the testing phase was taken as the dependent variable of interest.

<br>

##### Statistical Approach

To assess whether participants recognised familiar base pairs more than would be expected by chance, it is typical to apply a one-sample t-test to recognition test accuracies, against theoretical chance (p=.5) (e.g. @fiserUnsupervisedStatisticalLearning2001). Therefore we applied this test to our data to determine if our results corroborated that of the field. We applied this test to each $k$ sample, and we report the resulting $r^2$ and $p$ values.

It has recently been shown that the assumptions of the one-sample t-test are not met when data are from an information based measure such as classification accuracy [@allefeldValidPopulationInference2016]. Specifically, because the true value of such a measure can never be below chance (50 % in the case of a 2 alternative-forced-choice), application of the t-test, which carries the assumption that the null distribution can include values above and below 50 %, becomes a fixed effects analysis (see Allefeld et al [-@allefeldValidPopulationInference2016] for a full treatment). Instead, application of the prevalence statistic [@allefeldValidPopulationInference2016] allows for valid second-level inference with information-based measures such as classification accuracy. 

Implementing the prevalence statistic typically involves a two step permutation procedure. First, a null distribution is generated for each participant's accuracies (first level permutations), given their responses and the trials to which they were exposed. This is typically attained by shuffling the trial labels over multiple iterations (e.g. $m$ = 1000), and taking the participant's accuracy under each permutation (with the constraint that $m$ = 1 is the observed data) [@allefeldValidPopulationInference2016]. This yields $m$ accuracies per participant that would be attained given the null hypothesis ($H_0$) was true, where $H_0$ is that the participant is guessing. Secondly, to attain the second-level null distribution, each participant's first level permutations are sampled $z$ times (e.g. $z$ = 1000), with the constraint that $z_{1}$ contains the observed accuracies across participants. This results in $z$ sets of $N$ accuracies as would be expected given the second-level $H_0$. The minimum accuracy is then taken from each of the $z$ sets of permutated accuracies ($PA_{min}$). The proportion for which the observed minimum accuracy ($OA_{min}$) is greater than PA ($p = \frac{1}{i} \sum_{i=1}^z [OA_{min} > PA_{min}{z}]$) is taken as the probability that the observed accuracies are drawn from the distribution defined by $H_0$ (i.e. serves as the $p$ value of the second-level inferential test). 

Given that our sampling procedure already entailed $N*j*k$ permutations, and that such a permutation procedure as defined above would require $N*j*k*m*z$ permutations (which in our case is 2$e$+13), we opted to instead apply an analytical definition of the first- and second level distributions for $H_0$. 

To summarise this analytical distribution; each set of participant responses comes from a binary forced choice task with possible answers $a$ and $b$, consisting of $T$ trials with exactly half of the trials having each possible answer as the correct response (i.e. $A_a = A_b = \frac{1}{2}N$). For any given set of responses $R_a + R_b = N$, we assign the identity of $a$ and $b$ to the possible choices such that $R_a \le A_a$.

Now we can assemble a distribution for this configuration of responses by computing the number of possible combinations of $x$ responses in $A_a$ 'slots', multiplied by the number of $R_a - x$ responses in $A_b$ 'slots', for all x between zero and $R_a$. Or if $F(x)$ is the frequency for a given (not necessarily unique) accuracy, and $G(x)$ is the corresponding accuracy:

\begin{equation}
  F(x) = {{A_a}\choose {x}} \times {{A_b}\choose {R_a - x}}, G(x) = \frac{x + R_b - (R_a - x)}{R_a+R_b} , \forall x \in \mathbb{Z} \mid 0 \le x \le R_a
  (\#eq:FLPrev)
\end{equation}

To generate the second level null distribution, we then sampled each participant's first level distribution $z$ = 1000 times, to provide $z$ iterations of $N$ accuracies as would be expected under $H_0$. 

Having calculated the $p$ value for a given set of neutral accuracies, we can compute the proportion of the population expected to show the effect $\gamma$, given the observed $p$ statistic (i.e. an effect size), using the formula defined by Allefeld et al [-@allefeldValidPopulationInference2016]:

\begin{equation}
  \gamma = \frac{\alpha^\frac{1}{N} - p^\frac{1}{N}}{1-p^\frac{1}{N}}
  (\#eq:gamma)
\end{equation}


# Results
\label{sec:Results}

## Attentional Blink
\label{sec:ABRes}

```{r, AB_analysis, message=F}
abfname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
ab_res <- do.AB.analysis(abfname)
```

An overview for the findings pertaining to the AB are presented in Figure \@ref(fig:ABFX). As expected, accuracy for T2|T1 decreased (by around `r abs(ab_res[[2]][,3]$estimate)`) when T2 was presented at lag 2, relative to lag 7. A one-way ANOVA revealed that the effect of lag was statistically significant (F (`r ab_res[[1]]$DFn`, `r ab_res[[1]]$DFd`) = `r ab_res[[1]]$F`, $\eta_{p}^2$ = `r ab_res[[1]]$pes`, p = `r ab_res[[1]]$p`). Post-hoc t-tests showed that accuracy at each lag differed statistically from accuracy at each of the other lags (all p's $\leq$ `r max(do.call(rbind, lapply(1:6, function(x) cbind(ab_res[[2]][,x]$p.value))))`). Therefore we see that our implementation of the AB paradigm yielded the typically observed effects on the accuracy data.

```{r, AB_results, message=F}
load("../data/AB/ABstats.RData")
```


```{r, ABFX, out.width='80%', fig.cap='AB Effects. A) accuracy for T1 and T2|T1 by lag. As was expected, accuracy for T2|T1 was poorer at shorter relative to longer lags. B) Observed effect size densities for simulated experiments for select $N$, for the RM-ANOVA and LME analyses. C) KL divergenence when the density for each $N$ is used to approximate the density for N=313. D) The ratio between the observed and meta-analytic effect size was 1 for both RM-ANOVA and LME approaches, across all N. E) Ratio between the effect size densities observed for the RM-ANOVA and LME analyses at each level of N. Effect size densities were comparable between the methods across all levels of N'}

paradigm.fig.pth <- '../images/AB_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

```{r ABinf, message = F}
# compute inferential stats on AB effect size measures
kl_inputs <- list(origin = "313", dv = "dens_fx", sub_Ns = paste(round(exp(seq(log(13), log(313), length.out = 20)))))
AB_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(AB_KL_corrs) <- c("RM-AN", "LME")



AB_best_vs_typical <- comp_z(res['313', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`, 
                             res['313', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`, 
                             res['25', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`)
AB_best_vs_typical_p <- 2*pnorm(q=abs(AB_best_vs_typical), lower.tail=FALSE)
AB_meta <- do_z_tests(z_inputs = list(x = "meta", sub_Ns = kl_inputs$sub_Ns), res)
AB_model <- do_z_tests(z_inputs = list(x = "model", sub_Ns = kl_inputs$sub_Ns), res)

```

_Effect sizes_ As can be seen in Figure \@ref(fig:ABFX), panel B, increasing $N$ reduced the variability of effect sizes but had little impact on central tendency. Unsurprisingly, given the many demonstrations of the AB effect in the experimental psychology literature, the best estimate of the effect size (with $N_{313}$) was large (mean $\eta_{p}^2$ = `r res['313', 'stats_fx'][[1]][,1][[1]]`, sd: `r res['313', 'stats_fx'][[1]][,1][[2]]`). This estimate was statistically smaller than that observed for the typical N of the field ($N_{25}$, mean $\eta_{p}^2$ = `r res['25', 'stats_fx'][[1]][,1][[1]]`, sd: `r res['25', 'stats_fx'][[1]][,1][[2]]`, $z$ = `r AB_best_vs_typical`, p < .0001. This suggests that aggregating effect size observations with small $N$ would result in an inflated estimate of the size of the effect of the AB. Indeed, in line with the idea that existing effect size estimates are imprecise, there was a strong negative relationship between $D_{KL}$ and $N$ for both the ANOVA ($r_{s}$(`r AB_KL_corrs[["RM-AN"]]$parameter`) = `r AB_KL_corrs[["RM-AN"]]$estimate`, p = `r AB_KL_corrs[["RM-AN"]]$p.value`) and LME approaches ($r_{s}$(`r AB_KL_corrs[["LME"]]$parameter`) = `r AB_KL_corrs[["LME"]]$estimate`, p = `r AB_KL_corrs[["LME"]]$p.value` see Figure \@ref(fig:ABFX), panel C). This suggests significant information loss when using experiments of lower N to produce effect size estimates for the AB. However, there were minimal differences between the observed effect size distributions and the estimated meta-analytic effect size, across all levels of $N$ (panel D), suggesting that the AB effect is large enough to yield statistically significant results (and presumably publication) across any $N$. Interestingly, $r^2$ estimates yielded from the LME analysis yielded statistically lower effect size estimates across all $N$, relative to $\eta_{p}^2$ (Figure \@ref(fig:ABFX), panel E, all ps < .0001). This may be due to the simpler model structure of the LME resulting in larger residuals, which in turn would reduce the effect size estimate. 

_p Values_ All observed p-values were < .05, even the lowest sample size ($N$ = 13). This was the case for both the RM-ANOVA and LME approaches (see Figure \@ref(fig:ABps)). Similarly to the effect size densities, observed p-values were more variable for lower $N$ (e.g. RM-ANOVA: $\sigma\:p: N_{25}$ = `r res['25',"stats_p"][[1]][2][[1]]`, LME: $\sigma\:p: N_{25}$ = `r res['25',"stats_p"][[1]][[2,2]]`) than for higher $N$ (e.g. RM-ANOVA: $\sigma\:p: N_{313}$ = `r res['313',"stats_p"][[1]][2][[1]]`, LME: $\sigma\:p: N_{313}$ = `r res['313',"stats_p"][[1]][[2,2]]`). Unlike the effect size densities, the central tendency of the p-values also decreased with increasing $N$ (e.g. RM-ANOVA: $\mu\:p: N_{25}$ = `r res['25',"stats_p"][[1]][1][[1]]`, RM-ANOVA: $N _{313}$ = `r res['313',"stats_p"][[1]][1][[1]]`) and overall, were lower for the LME than the RM-ANOVA approach (LME: $\mu\:p: N_{25}$ = `r res['25',"stats_p"][[1]][[1,2]]`, $N_{313}$ = `r res['313',"stats_p"][[1]][[1,2]]`). Collectively these data show that reproducibility is sound with the AB, and that the certainty around any attained p-value is not only dependent on the $N$, but also the model employed to conduct the analysis, with the LME approach showing lower p-values. 


```{r, ABps, out.width='40%', fig.cap='Probit transformed p-values from the AB analysis, presented for selected N. A) p-values attained from the RM-ANOVA model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result'}
paradigm.fig.pth <- '../images/AB_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## Contextual Cueing
\label{sec:CCRes}

```{r, CC_interaction, message=F}
ccfname <- "../data/total_of_313_subs_CC_task_trial_level_data.csv"
cc_res <- do.CC.analysis(ccfname)
```

In accordance with the notion that RTs for repeat displays speed up more than for novel displays (i.e. that participants learn the repeat displays), a significant block x display interaction was found (F (`r cc_res[[1]]$DFn[3]`, `r cc_res[[1]]$DFd[3]`) = `r cc_res[[1]]$F[3]`, $\eta_{p}^2$ = `r cc_res[[1]]$pes[3]`, p = `r cc_res[[1]]$p[3]`). Specifically, while there was no statitically significant difference between repeat and novel displays in the first block (t (`r cc_res[[2]][[1]]$parameter`) = `r cc_res[[2]][[1]]$statistic`, p = `r cc_res[[2]][[1]]$p.value`, $\mu$ difference = `r cc_res[[2]][[1]]$estimate` sec), RT's to repeat displays were on average `r cc_res[[2]][[2]]$estimate` sec faster than RTs to novel displays by block 12 (t (`r cc_res[[2]][[2]]$parameter`) = `r cc_res[[2]][[2]]$statistic`, p = `r cc_res[[2]][[2]]$p.value`, see Figure \@ref(fig:CCFX), panel A).

```{r, CC_results, message=F}
load("../data/CC/CCstats.RData")
```

```{r CCinf, message = F}
# compute inferential stats on AB effect size measures

CC_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(CC_KL_corrs) <- c("RM-AN", "LME")



CC_best_vs_typical <- comp_z(res['313', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`, 
                             res['313', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`,
                             res['25', 'stats_fx'][[1]][1, "RM-AN"]$`RM-AN`, 
                             res['25', 'stats_fx'][[1]][2, "RM-AN"]$`RM-AN`)
CC_best_vs_typical_p <- 2*pnorm(q=abs(CC_best_vs_typical), lower.tail=FALSE)
CC_meta <- do_z_tests(z_inputs = list(x = "meta", sub_Ns = kl_inputs$sub_Ns), res)
CC_model <- do_z_tests(z_inputs = list(x = "model", sub_Ns = kl_inputs$sub_Ns), res)

```

```{r, CCFX, out.width='80%', fig.cap='CC Effects. A) RT plotted by block (x-axis) and display type (repeat vs novel). Participants became faster for repeat. B) Observed effect size densities for simulated experiments for select $N$, for the RM-ANOVA and LME analyses. C) KL divergenence when the density for each $N$ is used to approximate the density for N=313. D) The ratio between the observed and meta-analytic effect for both RM-ANOVA and LME approaches, across all N. A value above 1 suggests an inflated estimate of effect size in the literature E) Ratio between the effect size densities observed for the RM-ANOVA and LME analyses at each level of N. Effect size densities were comparable between the methods across all levels of N'}

paradigm.fig.pth <- '../images/CC_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

_Effect sizes_ As can be seen in Figure \@ref(fig:CCFX), panel B, increasing $N$ reduced the variability of effect sizes, and in contrast to the AB, also impacted on central tendency. The mean $\eta_{p}^2$ observed for $N_{max}$ was `r res['313', 'stats_fx'][[1]][,1][[1]]`, whereas for $N$ closest to the median sample size for the field (25 & 24 respectively) was $\mu\:\eta_{p}^2$ = `r res['25', 'stats_fx'][[1]][,1][[1]]`. In contrast the standard deviations were more varied; `r res['313', 'stats_fx'][[1]][,1][[2]]` and `r res['25', 'stats_fx'][[1]][2,1][[1]]` respectively. Interestingly, the pattern of results was different for the LME approach. Mean effect sizes were consistently low across all levels of $N$ (e.g.$N_{25}\:\mu\:r^2$ = `r res['25', 'stats_fx'][[1]][,2][[1]]`, $N_{313}\:\mu\:r^2$ = `r res['313', 'stats_fx'][[1]][,2][[1]]`. Moreover, the variability in observed effect size densities was higher for the RM-ANOVA approach than the LME approach across all levels of $N$ (Figure \@ref(fig:CCFX), panel E). Given the increasing variability and increasing effect size with decreasing N, it is unsurprising that there is a strong negative relationship between sample size and information loss for the RM-ANOVA approach (Figure \@ref(fig:CCFX), panel C). Collectively, these data show that the fixed effect size for the contextual cueing effect may be lower than is reported in the published literature, suggesting a bias towards positive effects in the published literature. 

_p Values_ In contrast to the AB results, 115 participants were required to achieve 90 % power to reject the null hypothesis using the RM-ANOVA approach, assuming the null is false. For the LME appraoch, 90 % power was achieved with 265 participants (see Figure \@ref(fig:CCps)). Also in contrast to the AB results, observed p-values were comparably variable across $N$ (e.g. RM-ANOVA: $\sigma\:p: N_{25}$ = `r res['25',"stats_p"][[1]][2][[1]]`, $N_{313}$ = `r res['313',"stats_p"][[1]][2][[1]]`, LME: $\sigma\:p: N_{25}$ = `r res['25',"stats_p"][[1]][[2,2]]`, $\sigma\:p: N_{313}$ = `r res['313',"stats_p"][[1]][[2,2]]`). The central tendency of the p-values did also decrease to some extent with increasing $N$ (e.g. RM-ANOVA: $\mu\:p: N_{25}$ = `r res['25',"stats_p"][[1]][1][[1]]`, $N_{313}$ = `r res['313',"stats_p"][[1]][1][[1]]`, LME: $\mu\:p: N_{25}$ = `r res['25',"stats_p"][[1]][[1,2]]`, $N_{313}$ = `r res['313',"stats_p"][[1]][[1,2]]`). Collectively these data show that reproducibility is challenging with the contextual cueing paradigm. 


```{r, CCps, out.width='40%', fig.cap='Probit transformed p-values from the CC analysis, presented for selected N. A) p-values attained from the RM-ANOVA model. B) p-values attained from the LME model. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line reflects a statistically significant result'}
paradigm.fig.pth <- '../images/CC_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

# Discussion

AB Discussion: taking published effect sizes to motivate power calculations will result in an inflated effect size estimate. Increasing N has a strong power/exponential relationship with precision of estimate, regardless of modelling choice. No file drawer effect. LME results in comparable NHST decision making, but simple model structure means a smaller estimate of the effect size.


\clearpage

# References
\label{sec:Refref}