---
title: Quantifying effect sizes in implicit learning tasks; the role of some stuff
authors:
  - name: Kelly G. Garner
    thanks: Corresponding author
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, UK, B13 2TT
    email: getkellygarner@gmail.com
  - name: Christopher R. Nolan
    department: School of Psychology
    affiliation: University of New South Wales
    location: Sydney, Australia
    email: cnolan@cn.id.au
  - name: Abbey Nydham
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Zoie Nott
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
  - name: Howard Bowman
    department: School of Psychology
    affiliation: University of Birmingham
    location: Edgbaston, Birmingham, B13 2TT
    email: h.bowman@bham.ac.uk
  - name: Paul E. Dux
    department: School of Psychology
    affiliation: University of Queensland
    location: St. Lucia, Australia, 4072
    email: paul.e.dux@gmail.com
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: refs.bib
biblio-style: unsrt
output: 
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
    keep_tex: true
  bookdown::word_document2:
    toc: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
#opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
# options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
#         knitr.kable.NA = '')
# knit_hooks$set(inline = function(x) {
#   x <- sprintf("%1.2f", x)
#   paste(x, collapse = ", ")
# })
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
library(rstatix)
library(emmeans)
library(ggridges)
library(car)
library(parallel)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
source('../R/plotting.R') # this has some stats functions we want to use in the document
source('../R/do_infer_stats.R') # extra functions for inferential stats

```

# Introduction

~100 words per paragraph

Things to add
- why we want to know what each measure tells us
- certainty over p-values

- computation: is taking a handful of studies ok?

The brain is the most complex organism known to humans, yet decision making regarding theory for its function tends to be made on binary (i.e. pass or fail) terms, at least in the experimental psychological sciences. Specifically, theories often propose experimental tests for the presence or absence of given effects, rather than quantifying the extent to which an effect should be observed, i.e. the anticipated effect size. The latter prediction is more risky, and therefore constitutes a more desirable prediction for theory testing [insert Popper reference]. In fact, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. 

For example, in the study of EF: [AB: theory of p/f vs Ragnaroc?], MT costs - .  

Likewise, in the study of IL: VSL, CC, SRT

To promote quantised theories in experimental psychology, one extra piece of pertinent information that is informative to theory development is - we normally do x (insert Cummings refs), but it is missing y.

This is also useful for experimental development. i.e. The other important thing is that to ensure that we provide sufficiently precise information, so that this can be used. To do that we need to perform power calculations, There are at least 3 ways to do this: have a sufficiently precise theory that quantifies the effect size of interest, arbitrarily assume an effect size of theoretical interest, but unmotivated, or create an estimate by sampling the field. The problem with the latter is that given the current sample sizes typically employed, we have no idea if that estimate is precise and should be used.

A method to determine the precision of effect  

For example, theories in the experimental psychological sciences have tended only to predict the presence or absence, rather than the extent of a given phenomena. For example [example from implicit learning]. However, clear to see that if we are to understand how such a complex system processes sensory information, coordinates tasks and acquires new behavioural repertoires, a precise mapping between theory and outcome is going to be necessary. [get some Vehicles notions in that previous sentence]. Therefore important to start thinking about the size of effects - however, current state of field, very little knowledge about anticipated effect sizes due to x, y, & z.


Using a large dataset we will address this gap. Data on x-tasks. We will apply a simulation analysis to determine x, y and z.

Moreover, we will address two pertinent analytical gaps: i) A further development is the recent use of linear mixed effects models, and the recommendation that we use them instead of
ii) it is common to use t-test on accuracies against chance but Allefeld (VSL).

PUT IN INTRO
Additionally, given the documented advantages of linear mixed effects models (LME) over repeated-measures ANOVA [@muthAlternativeModelsSmall2016; @bagiellaMixedeffectsModelsPsychophysiology2000; @mccullochRepeatedMeasuresANOVA2005], and that only a conceptual proxy of $\eta_{p}^{2}$ is computable from these models [@brysbaertPowerAnalysisEffect2018; @westfallStatisticalPowerOptimal2014], and c) there exists no data that we know of that quantifies to what extent we can expect comparable outcomes between both methods, we (where relevant) opted to apply both the commonly used statistical model, and a LME model to each $k$ sample

INCLUDE Zoie's survey of the literature

# Methods

\label{sec:Method}


## Participants
\label{sec:Participants}

The current study uses a data set collected for a previous [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This data set contains performance measures from $N$ = 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being female sex, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by The University of Queensland Human Research Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

## Apparatus
\label{sec:Apparatus}

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 7 tasks; Attentional Blink (AB), Multitasking (MT), Contextual Cueing (CC), Serial Response Task (SRT), Visual Statistical Learning (VSL), Operation Span task and a Stop Signal Inhibition task. Only the data from the AB, MT, CC and SRT are reported here. 

## Procedures
\label{sec:Procedures}

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure \@ref(fig:FigureParadigm). Further details regarding the task protocols are presented within each section below. We first provide an overview of the simulation procedures, before detailing the specific procedural and statistical methods for each task.


```{r, FigureParadigm, out.width='70%', fig.cap='Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters, B) Multitasking Paradigm (MT). Participants make a discriminate the colour of a disc, a complex tone, or both C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task where they search for a rotated T among L distractors. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the blocks the stimulus follows a repeating sequence.'}

paradigm.fig.pth <- '../images/FigXXXX_alltasks.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

All the [data]() and [code](https://github.com/kel-github/Super-Effects) used for the current analysis are available online. All data were analysed using R -@rcoreteamLanguageEnvironmentStatistical2015 and RStudio -@rstudiocitation. The analysis of the data from each task followed two steps; first, to ascertain that we observed the typical findings for each of the paradigms, we applied the relevant conventional statistical model to the full dataset (N=313). The details of each analysis are presented below. Next, we implemented a simulation procedure to determine the effect size and p-values that would be attained over many experiments conducted at multiple levels of sample size. 

### Sampling procedure
\label{sec:SamplingProc}

For each task, we simulated experiments across 20 different sample sizes ($N$s), defined on a logarithmic interval between N=13 and N=313. We opted for a logarithmic interval given the decreasing information gained at higher $N$ values. To simulate $k$=1000 experiments at each of our chosen $N$, we sampled $N$ participants from $N_{max}$ over $k$ iterations. The relevant analysis was applied to each of the samples. Specific details regarding which analyses were applied to each $k$ sample are detailed below for each paradigm. Sampling with replacement ensured that the samples carried the Markov property. One potential concern is that any reductions in observed effect size variability may be attributable to saturation as the simulated $N$ approaches the maximum ($N_{max}$=313), rather than a genuine reduction in variance of the estimate of the effect. Specifically, it could be that as $N$ approaches 313, the overlap of participants between subsamples may be greater than when $N$ equals a lower number such as 13. It follows then that any decreasing variability in effect size estimates at higher $N$s could be due to the decrease in variability of the subsamples, rather than the improved estimate of the population variance that should come with a larger $N$. We have run simulations that argue against this explanation and these can be found in appendix i.

_Effect Sizes_
For each task, we report the following information from the observed effect size densities: to assess the best estimate of the effect size and its variability, given our large dataset, we report the median and the standard deviation observed for our highest $N$ (apart from in one case of multimodality, where we report the points of highest probability, the median and the .025 and .975 quantiles). These values can be used to motivate power calculations for future studies. To test whether the best estimate differs from what is representative for the field, we next report the summary statistics for the $N$ that is closest to the median sample size from our survey of the literature. We use Q-Q plots to determine the extent of convergence between the densities observed at median and maximal $N$s. To determine information loss cross each level of $N$, we compute the Kullback-Leibler Divergence ($D_{KL}$) to quantify how much information would be lost if the effect size distribution observed for each $N$ were used to approximate the distribution that reflects our best estimate, i.e. that observed with $N_{313}$.To quantify the relationship between information loss and $N$, we correlate $D_{KL}$ and $N$ using Spearman's Rho. To provide an estimate of the effect size that would be yielded through a meta-analysis of the published literature, we compute summary statistics for all analyses that yielded a statistically significant result (p<.05), we then present the difference between the mean observed effect size minus the mean estimate of the meta-analytic effect size, for each level of $N$. 

_p Values_
To determine the $N$ required to achieve 90% power to reject the null hypothesis, we report the $N$ for which over 90% of p-values pass the threshold for significance for the effects of interest ($\alpha$=.05). As p-values clustered close to 0 in many instances, we applied the probit transform to rescale the values on the range [-$\infty$, $\infty$]. This allowed for a clearer visualisation of the spread of p-values at each level of $N$. To assess the range of p-values that one can expect to observe at some given $N$, i.e. the confidence for the most likely observed p-value, or certainty of the test outcome, we computed the difference between the the .025 and .975 quantiles ($q$) of the observed p-values at each level of $N$. We then computed the ratio between that range and that observed for the median $N$ for the field $N$ ($\frac{q_{N}}{q_{medianN}}$). Values below 1 suggest that the certainty over the p-value is lower for the median $N$ for the field, relative to the given $N$. 

## Attentional Blink (AB)
\label{sec:ABMeth}

### Protocol

The AB protocol was the same as that reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letters targets and digit distracters were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 2.7$^\circ$ visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

<br>

### Statistical Approach

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\eta_{p}^2$ and the resulting $p$ value were taken for the main effect of lag. For this task, and all remaining ANOVA tests, models were fit using the anova_test() function from the [rstatix](https://rpkgs.datanovia.com/rstatix/index.html) package. Where possible, the models were fit using type 3 sum of squares, owing to the computational expediency and match to commercial statistical software packages. In some cases, models were unable to be fit using type 3 sum of squares, owing to rank deficiencies in the underlying design matrix (e.g. when one participant was drawn more than twice within a sample). In these cases, models were fit using type 1 sum of squares. However, as the experiment designs were fully balanced, each sum of squares type should yield the same results.

## Multitasking (MT)
\label{sec:MTMeth}

### Protocol

The MT protocol was previously reported in [@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The DT consisted of 4 blocks of 36 trials, with each trial type (ST visual, ST auditory or DT) randomly mixed within blocks. Participants completed the DT task after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

### Statistical Analysis

To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: single vs multitask) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\eta_{p}^2$ and $p$ are reported for the main effect of task. 

## Contextual Cueing (CC)
\label{sec:CCMeth}

### Protocol

The CC protocol was the same as that reported by Nydam et al -@nydamCathodalElectricalStimulation2018 which is modeled on Chun and Jiang -@chunContextualCueingImplicit1998. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80]. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key). The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1:12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable.

### Statistical Approach

To ascertain whether participants speeded responses to repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated arrays), the data were subject to a block (1:12) x condition (repeat vs novel array) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\eta_{p}^2$ and $p$ for the block x condition interaction. 
As sometimes in the literature a significant main effect and not an interaction are reported motivating the notion that contextual cueing reflects single-shot learning, we also report the $\eta_{p}^2$ and $p$ for the main effect of condition.

## Serial Response Task (SRT)
\label{sec:SRTMeth}

### Protocol

The SRT was adapted from [@nissenAttentionalRequirementsLearning1987]. Four square placeholders were presented across the horizontal meridian, subtending w$^\circ$ x h$^\circ$. A red circle [RGB: 255, 0, 0] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the placeholder within which the circle appeared, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'repeats'. Of interest is the RT for Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).

### Statistical Approach

To ascertain whether participants learned the repeating sequences, RTs in the final block of sequence trials (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d$, converted to $r^2$, and $p$ value from each test. 

<br>

# Results
\label{sec:Results}

## Attentional Blink
\label{sec:ABRes}

```{r, AB_analysis, message=F}
abfname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
ab_res <- do.AB.analysis(abfname)
```

An overview for the findings for the AB task are presented in Figure \@ref(fig:ABFX). As expected, proportion accuracy for T2|T1 decreased (by around p = `r sprintf("%.2f", abs(ab_res[[2]][,3]$estimate))`) when T2 was presented at lag 2, relative to lag 7. A one-way ANOVA revealed that the effect of lag was statistically significant (F (`r sprintf("%.1f", ab_res[[1]]$DFn)`), `r sprintf("%.0f", ab_res[[1]]$DFd)`) = `r sprintf("%.0f", ab_res[[1]]$F)`, $\eta_{p}^2$ = `r sprintf("%.2f", ab_res[[1]]$pes)`, p = `r sprintf("%.2e", ab_res[[1]]$p)`). Post-hoc t-tests showed that accuracy at each lag differed statistically from accuracy at each of the other lags (all p's $\leq$ `r sprintf("%.2e", max(do.call(rbind, lapply(1:6, function(x) cbind(ab_res[[2]][,x]$p.value)))))`), with lower accuracy values at the shorter relative to the longer lags. Therefore we see that our implementation of the AB paradigm yielded the typically observed effects.

```{r, AB_results, message=F}
load("../data/AB/IMMABstats.RData")
```

```{r, ABFX, out.width='80%', fig.cap='AB Effects. A) Violin plot showing accuracy across subjects for T1 and T2|T1 by lag. As was expected, accuracy for T2|T1 was poorer at shorter relative to longer lags. B) Observed effect size densities for simulated experiments for select $N$, for the RM-ANOVA and LME analyses. C) QQ-plot of the density of effect sizes observed for the median $N$ (25) against that observed for $N_{max}$. D) KL divergenence when the effect size density for each $N$ is used to approximate the density for $N_{max}$. E) The difference between the observed and meta-analytic effect size was 0, across all $N$.'}

paradigm.fig.pth <- '../images/IMMAB_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

```{r ABinf, message = F, echo=F}
# compute inferential stats on AB effect size measures
kl_inputs <- list(origin = "313", dv = "dens_fx", sub_Ns = paste(round(exp(seq(log(13), log(313), length.out = 20)))))
AB_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(AB_KL_corrs) <- c("RM-AN", "LME")
```

_Effect sizes_ As can be seen in Figure \@ref(fig:ABFX) (panel B) increasing $N$ reduced the variability of observed effect sizes. Unsurprisingly, given the many demonstrations of the AB effect in the experimental psychology literature, the best estimate of the effect size (with $N_{313}$) was large (median $\eta_{p}^2$ = `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$sd)`). This estimate was comparable, although slightly smaller for the typical N of the field ($N_{25}$, median $\eta_{p}^2$ = `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$sd)`. This suggests that the long running median of effect sizes observed across AB studies would provide a reasonable estimate of the effect size. Examination of the Q-Q plot between $N_{25}$ and $N_{313}$ demonstrates higher variability for $N_{25}$, suggesting that imprecision in effect size estimates does exist in the field (Figure \@ref(fig:ABFX), panel C). In corroboration, there was a strong negative relationship between $D_{KL}$ and $N$ ($r_{s}$(`r sprintf("%.0f", AB_KL_corrs[["RM-AN"]]$parameter)`) = `r sprintf("%.2f", AB_KL_corrs[["RM-AN"]]$estimate)`, p = `r sprintf("%.2e", AB_KL_corrs[["RM-AN"]]$p.value)`, see Figure \@ref(fig:ABFX), panel D). This suggests significant information loss when using experiments of lower N to produce effect size estimates for the AB. However, owing to the large effect under study, there were no differences between the observed effect size distributions and the estimated meta-analytic effect size, across all levels of $N$ (panel E), suggesting that the AB effect is large enough to yield consistent statistically significant results, even with small $N$. 

_p Values_ All observed p-values were < .05, even for the lowest sample size ($N_{13}$). (see Figure \@ref(fig:ABps), panel A), showing that decision-outcomes are likely the same across all AB experiments. Panel B shows the ratio between the 97.5 quantiles for $N_{25}$ relative to the other $N$s. Certainty around the most likely $p$ increased with increasing sample size up to a ratio of `r sprintf("%1.2f", diff(res[19,"stats_p"][[1]][,"RM-AN"]$qs)/diff(res['25',"stats_p"][[1]][,"RM-AN"]$qs))`. However, the median estimate moved further from p=.05, so the certainty in the decision increases with $N$. 

```{r, ABps, out.width='40%', fig.cap='p-values from the AB analysis. A) Probit transformed p-values for selected $N$. B) Ratio between the .025 and .975 quantiles of p-values observed for each level of N, relative to $N_{25}$'}
paradigm.fig.pth <- '../images/IMMAB_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## Multitasking

```{r, MT_F, message=F}
mtfname <- "../data/total_of_313_subs_SingDual_task_trial_level_data.csv"
mt_res <- do.MT.analysis(mtfname)
```

As was anticipated, RTs were slowed for multitask relative to single task conditions (see Figure \@ref(fig:MTFX)). Mean RTs were on average `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$estimate[[1]]))` (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[2]))`, `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[1]))`] seconds (s) slower on MT trials (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "trialtype"])`) = `r sprintf("%4.0f", mt_res[[1]]$F[mt_res[[1]]$Effect == "trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "trialtype"])`, p<.0001). There was also a significant task modality (sound or visual) x task (ST vs MT) interaction  (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "task:trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "task:trialtype"])`) = `r sprintf("%2.1f", mt_res[[1]]$F[mt_res[[1]]$Effect == "task:trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "task:trialtype"])`, p<.0001), with the MT cost (MT RT - ST RT) being larger for  the sound task relative to the visual task by on average `r sprintf("%.2f", abs(mt_res[[2]]$interaction$estimate[[1]]))` s (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[1]))`, `r sprintf(sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[2])))`]). This latter finding is typical in the multitasking literature (@hazeltineModalityPairingEffects2006, @garnerTransferabilityTrainingBenefits2015a), but is not pertinent to the focus of the current work which seeks to quantify the effect size for the main effect of multitasking cost.


```{r, MT_results, message=F}
load("../data/SD/IMMSDstats.RData")
```


```{r, MTFX, out.width='80%', fig.cap='MT Effects. A) Violin plots of mean RTs for each multitasking condition broken down by task modality. As was expected, RTs were slower for multitasking relative to single-task conditions. B) Observed effect size densities for simulated experiments for select $N$. C) QQ-plot of the median $N$ ($N_{42}$) against $N_{313}$. D) KL divergenence when the density for each $N$ is used to approximate the density for N=313. E) The difference between the observed and meta-analytic effect size was 0 across all N. S = single-task, M = multi-task, So = sound manual task, V = visual manual task'}

paradigm.fig.pth <- '../images/IMMSD_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)

```

```{r MTinf, message = F}
kl_inputs <- list(origin = "313", dv = "dens_fx", sub_Ns = paste(round(exp(seq(log(13), log(313), length.out = 20)))))
MT_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(MT_KL_corrs) <- c("RM-AN", "LME")

```


_Effect sizes_ Similar to the AB findings, the effect size distributions for the MT paradigm show that increasing $N$ reduced the variability of $\eta_{p}^2$ (see Figure \@ref(fig:MTFX)). The best estimate of the effect size (with $N_{313}$) was large (median $\eta_{p}^2$ = `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$sd)`). This estimate matches that observed for the typical N of the field ($N_{42}$, mean $\eta_{p}^2$ = `r sprintf("%.2f", res['42', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['42', 'stats_fx'][[1]][,"RM-AN"]$sd)`. Therefore collating currently published data would likely produce a reasonable estimate of the true effect size for multitasking costs. However, examination of the Q-Q plot between $\eta_{p}^2$ densities for $N_{42}$ and $N_{313}$ demonstrates higher variability for $N_{42}$ (Figure \@ref(fig:MTFX), panel C). Moreover, a strong negative relationship between $D_{KL}$ and $N$ ($r_{s}$ =`r sprintf("%2.0f", MT_KL_corrs[["RM-AN"]]$parameter)`) = `r sprintf("%.2f", MT_KL_corrs[["RM-AN"]]$estimate)`, p = `r sprintf("%.2e", MT_KL_corrs[["RM-AN"]]$p.value)`, Figure \@ref(fig:MTFX), panel D), suggests that any single effect size observed with an experiment of $N_{42}$ is more likely to be a misleading estimate. Also similar to the AB results and owing to the 100% power attained at $N_{13}$ (see below), there were no differences between the observed effect size distributions and the estimated meta-analytic effect size, across all levels of $N$ (panel E). 

_p Values_ As with the AB, all observed p-values were < .05 (see Figure \@ref(fig:MTps), panel A for example $p$ distributions). As can be seen in panel B, distributions of p-values were narrower with lower $N$. Variability of $p$ values was `r sprintf("%1.2f", diff(res[19,"stats_p"][[1]][,"RM-AN"]$qs)/diff(res['42',"stats_p"][[1]][,"RM-AN"]$qs))` times higher for $N_{max}$ than for $N_{42}$. However distance from p=.05 increased with increasing $N$.

```{r, MTps, out.width='40%', fig.cap='p-values from the MT analysis. A) Probit transformed p-values for selected $N$. B) Ratio between the .025 and .975 quantiles of p-values observed for each level of N, relative to $N_{42}$'}
paradigm.fig.pth <- '../images/IMMSD_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## Contextual Cueing
\label{sec:CCRes}

```{r, CC_interaction, message=F}
ccfname <- "../data/total_of_313_subs_CC_task_trial_level_data.csv"
cc_res <- do.CC.analysis(ccfname)
```

In accordance with the notion that participants learned the repeat displays over the course of the experiment, the RT data showed a significant block x condition interaction (F (`r sprintf("%2.2f", cc_res[[1]]$DFn[3])`, `r sprintf("%4.1f", cc_res[[1]]$DFd[3])`) = `r sprintf("%1.2f", cc_res[[1]]$F[3])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[3])`, p = `r sprintf("%.2e", cc_res[[1]]$p[3])`). Specifically, there was no statistically significant difference between RTs for repeat and novel displays in the first block (t (`r sprintf("%3.0f", cc_res[[2]][[1]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[1]]$statistic)`, p = `r sprintf("%.2e", cc_res[[2]][[1]]$p.value)`, $\mu$ difference = `r sprintf("%.2f", cc_res[[2]][[1]]$estimate/1000)` s, sd: `r sprintf("%.2f", (cc_res[[2]][[1]]$stderr/1000)*sqrt(313))`). However, by block 12, RTs for repeat displays were on average `r sprintf("%.2f", cc_res[[2]][[2]]$estimate/1000)` s faster than novel displays (sd: `r sprintf("%.2f", (cc_res[[2]][[2]]$stderr/1000)*sqrt(313))`, t (`r sprintf("%3.0f", cc_res[[2]][[2]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[2]]$statistic)`, p = `r sprintf("%.2e", cc_res[[2]][[2]]$p.value)`, see Figure \@ref(fig:CCbeh), panel A). There was also a significant and large main effect of block (F(`r sprintf("%.2f", cc_res[[1]]$DFn[1])`, `r sprintf("%.2f", cc_res[[1]]$DFd[1])`) = `r sprintf("%1.2f", cc_res[[1]]$F[1])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[1])`, p = `r sprintf("%.2e", cc_res[[1]]$p[1])`). Of relevance for the subsequent discussion on the $\eta_{p}^2$ results, there was also a significant main effect of condition (F(`r sprintf("%.2f", cc_res[[1]]$DFn[2])`, `r sprintf("%.2f", cc_res[[1]]$DFd[2])`) = `r sprintf("%1.2f", cc_res[[1]]$F[2])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[2])`, p = `r sprintf("%.2e", cc_res[[1]]$p[2])`).

```{r, CCbeh, out.width='90%', fig.cap='Contextual Cueing Performance. A) Group mean RT plotted by block (x-axis) and condition (novel vs repeat display). Error bars reflect SEM. B) As in A but presenting violin plots for each block x condition. Examination of the dispersion of data suggests that the interaction is small (although it is statistcially significant).'}

paradigm.fig.pth <- '../images/CC_behav.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

```{r, CC_results, message=F}
load("../data/CC/IMMCCstats.RData")
```

```{r, CCinf, message = F}
# compute inferential stats on AB effect size measures

CC_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(CC_KL_corrs) <- c("int", "ME")

```

_Effect sizes_ The observed $\eta_{p}^2$ densities are presented in Figure \@ref(fig:CCFX) for the block x condition interaction (A) and for the main effect of condition (B). Examination of the median $\eta_{p}^2$ for the block x condition interaction showed disparity between the median $N$ ($N_{25}$, median $\eta_{p}^2$ = `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"RM-AN"]$sd)`) and the best estimate from $N_{313}$ (median $\eta_{p}^2$ = `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, sd: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$sd)`). In contrast for the main effect, the median effect size was consistent between $N_{25}$ and $N_{313}$  ($N_{25}$ median $\eta_{p}^2$ = `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"LME"]$med)`, sd: `r sprintf("%.2f", res['25', 'stats_fx'][[1]][,"LME"]$sd)`, $N_{313}$ median $\eta_{p}^2$ = `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"LME"]$med)`, sd: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"LME"]$sd)`). In line with the finding of disparity between the effect size estimates for key $N$ for the block x condition interaction, examination of the qq-plots between $N_{25}$ and $N_{313}$ shows little convergence between the two $\eta_{p}^2$ densities. This suggests that 1000 experiments run with the median $N$ for the field is not sufficient to estimate the size of the interaction effect, at least modelled using repeated measures ANOVA. In contrast, although the density of $\eta_{p}^2$ observations from the main effect shows higher skew for $N_{25}$, there is greater convergence with $N_{313}$, suggesting that this estimate from the current field may be more representative of the true effect size. Again, there was a strong negative relationship between $D_{KL}$ and $N$ for both the block x condition interaction ($r_{s}$(`r sprintf("%2.0f", CC_KL_corrs[["int"]]$parameter)`) = `r sprintf("%.2f", CC_KL_corrs[["int"]]$estimate)`, p = `r sprintf("%.2e", CC_KL_corrs[["int"]]$p.value)`) and LME approaches ($r_{s}$(`r sprintf("%2.0f", CC_KL_corrs[["ME"]]$parameter)`) = `r sprintf("%.2f", CC_KL_corrs[["ME"]]$estimate)`, p = `r sprintf("%.2e", CC_KL_corrs[["ME"]]$p.value)` see Figure \@ref(fig:CCFX), panel D). This suggests significant information loss when using experiments of lower N to produce effect size estimates for CC. Moreover, meta-analytic (p<.05) effect sizes were larger than observed effect sizes up until $N_{115}$, suggesting inflated estimates in the field currently.

```{r, CCFX, out.width='80%', fig.cap='CC Effects. A) Effect size densities for the block x condition interaction (b x c) at selected $N$. B) Effect size densities for the main effect of condition (c) at selected $N$. C) QQ-plots of densities observed at median $N$ ($N_{25}$) against $N_{313}$. D) KL divergenence when the density for each $N$ is used to approximate the density for $N_{313}$. E) The difference between the mean observed and meta-analytic effect, across all N. A value below 0 suggests an inflated estimate of effect size.'}

paradigm.fig.pth <- '../images/IMMCC_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

_p Values_ In contrast to the AB and MT results, 82 participants were required to achieve > 90 % power to reject the null hypothesis for the block x condition interaction. For the main effect, > 90 % power was achieved with 136 participants (see Figure \@ref(fig:CCps), panel A). Again, the dispersion of p-values increased with increasing $N$ (see Figure \@ref(fig:CCps), panel C).

```{r, CCps, out.width='40%', fig.cap='Probit transformed p-values from the CC analysis, presented for selected $N$. A) p-values attained for the block x condition interaction. B) p-values attained from main effect of condition. The black dotted vertical line reflects alpha = .05 (probit transformed). Anything to the left of this line is a statistically significant result. C) Ratio between the range of p-values observed for each level of N, relative to the median $N_{42}$ for the field'}
paradigm.fig.pth <- '../images/IMMCC_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```

## SRT

```{r, SRT_F, message=F}
srtfname <- "../data/total_of_313_subs_SRT_task_trial_level_data.csv"
srt_res <- do.SRT.analysis(srtfname)
```

In line with the notion that participants learned the repeating sequence, RTs were on average `r sprintf("%.3f", srt_res[[1]]$estimate/1000)` s faster (95% CI [`r sprintf("%.3f", srt_res[[1]]$conf.int[[1]]/1000)`, `r sprintf("%.3f", srt_res[[1]]$conf.int[[2]]/1000)`]) for the sequence relative to the random condition (t(`r sprintf("%3.0f", srt_res[[1]]$parameter[[1]])`) = `r sprintf("%2.2f", srt_res[[1]]$statistic[[1]])`, $d$ = `r sprintf("%1.2f", srt_res[[2]])`, p = `r sprintf("%.2e", srt_res[[1]]$p.value)`). 

```{r, SRT_results, message=F}
load("../data/SRT/IMMSRTstats.RData")
```

```{r SRTinf, message = F}
# compute inferential stats on AB effect size measures

SRT_KL_corrs <- do_KL_and_spearmans(kl_inputs, res)
names(SRT_KL_corrs) <- c("RM-AN", "LME")

```


```{r, SRTFX, out.width='80%', fig.cap='SRT Effects. A) Violin plots of mean RT across subjects plotted for random vs sequence conditions from the final two blocks of the experiment. Participants were faster for sequence trials. B) Observed effect size densities ($d$) for simulated experiments for select N. C) QQ-plot of the density observed for the median $N$ ($N_{36}$) against $N_{313}$. D) KL divergenence for when the density from each $N$ is used to approximate the density for $N_{313}$. E) The difference between the mean observed and meta-analytic effect, across all N. A value below 0 suggests that considering only published results would result in an inflated estimate of the effect.'}

paradigm.fig.pth <- '../images/IMMSRT_fx_main.pdf'
knitr::include_graphics(paradigm.fig.pth)
```
_Effect sizes_ The observed $d$ densities are presented in Figure \@ref(fig:SRTFX), panel B. Interestingly, all densities were bimodal. The density for the median N ($N_{36}$) showed 2 peaks, occurring over ~$d$ = 0.6 and $d$ = 1.3 respectively. Bimodality was still evident at $N_{313}$, with peaks over $d$ = 0.7 and $d$ = 1.22. Owing to this shifting bimodality, there was also disparity in the median estimate across simulations ($N_{36}$ median = `r sprintf("%1.2f", res['36', 'stats_fx'][[1]][,"RM-AN"]$med)` 95% CI: `r sprintf("%.2f", res['36', 'stats_fx'][[1]][,"RM-AN"]$qs[1])`, `r sprintf("%.2f", res['36', 'stats_fx'][[1]][,"RM-AN"]$qs[2])`, $N_{313}$ = `r sprintf("%1.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$med)`, 95% CI: `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$qs[1])`, `r sprintf("%.2f", res['313', 'stats_fx'][[1]][,"RM-AN"]$qs[2])`). In line with the finding of disparity between the median effect sizes for $N_{36}$ and $N_{313}$, the QQ-plot (Figure \@ref(fig:SRTFX), panel C) suggests that the two distributions converge prior to p=.5. Again, there was a strong negative relationship between $D_{KL}$ and $N$ ($r_{s}$(`r sprintf("%2.0f", SRT_KL_corrs[["RM-AN"]]$parameter)`) = `r sprintf("%.2f", SRT_KL_corrs[["RM-AN"]]$estimate)`, p = `r sprintf("%.2e", SRT_KL_corrs[["RM-AN"]]$p.value)`, Figure \@ref(fig:SRTFX), panel D). This suggests significant information loss when using experiments of lower N to produce effect size estimates for CC. Meta-analytic (p<.05) effect sizes were larger than observed effect sizes up until (and not including) $N_{30}$, suggesting that collation of SRT studies with an $N$ less than 30 would produce an inflated estimate of the effect size. 

_p Values_ For the SRT data, 13 participants was sufficient to achieve `r sprintf("%.2f", res[,"sig"][[13]][1])` % power to reject the null hypothesis (see Figure \@ref(fig:SRTps) for example $N$s). As can be seen in panel B, and comparably to the other tasks, uncertainty around $p$ for the median $N$ for the field ($N_{36}$) is lower than $N_{313}$ with a ratio of `r abs(diff(res['313',"stats_p"][[1]][[3,1]]))/abs(diff(res['36',"stats_p"][[1]][[3,1]]))`, however, distance from the decision point increased with each $N$.


```{r, SRTps, out.width='40%', fig.cap='Probit transformed p-values from the SRT analysis, presented for selected N. A) p-values attained from the t-test model.B) Ratio between the 95% CI of p-values observed for each level of N, relative to the median $N$ (36) for the field'}
paradigm.fig.pth <- '../images/IMMSRT_ps.pdf'
knitr::include_graphics(paradigm.fig.pth)
```


# Discussion

### CC Data - trade off of effect sizes with the ANOVA / partial eta square
### CC Data - would never get 'true' effect size with interaction
### CC Data - need more power to get the main effect, likely because it is a poor model of the data
### What is significance of a regressor?

\clearpage

# References
\label{sec:Refref}