---
title             : "Quantifying error in effect size estimates in attention, executive function and implicit learning"
shorttitle        : "Quantifying error in effect size estimates"

author: 
  - name          : "*Kelly G. Garner"
    affiliation   : "1,3"
    corresponding : yes    # Define only one corresponding author
    email         : "getkellygarner@gmail.com"
  - name          : "Christopher R. Nolan"
    affiliation   : "2"
  - name          : "Abbey Nydam"
    affiliation   : "3"
  - name          : "Zoie Nott"
    affiliation   : "3"
  - name          : "Howard Bowman"
    affiliation   : "1"
  - name          : "Paul E. Dux"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "School of Psychology, University of Birmingham, UK"
  - id            : "2"
    institution   : "School of Psychology, University of New South Wales, Australia"
  - id            : "3"
    institution   : "School of Psychology, The University of Queensland, Australia"
    
authornote: |
  *denotes corresponding author: getkellygarner@gmail.com
  
  This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 796329, awarded to Kelly Garner, and ARC Discovery Projects DP180101885 & DP210101977 awarded to Paul Dux.

keywords          : "effect size, statistical power, executive function, implicit learning, attentional blink, multitasking, contextual cueing, serial response task"
wordcount         : "8873"

bibliography      : ["supfx_refs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

Data: https://doi.org/10.48610/b63ecc2 Garner & Nolan [-@garnerQuantifyingErrorEffect2022a] <p>
Code: https://github.com/kel-github/Super-Effects Garner, Knott & Nolan [-@garnerQuantifyingErrorEffect2022] <p>

\clearpage

# Abstract
Accurate quantification of effect sizes has the power to motivate theory, and reduce misinvestment of scientific resources by informing power calculations during study planning. However, a combination of publication bias and small sample sizes (~$N$ = 25) hampers certainty in current effect size estimates. We sought to determine the extent to which sample sizes may produce error in effect size estimates for four commonly used paradigms assessing attention, executive function and implicit learning (Attentional Blink (AB), Multitasking (MT), Contextual Cueing (CC), Serial Response Task (SRT)). We combined a large data-set with a bootstrapping approach to simulate 1000 experiments across a range of $N$ (13-313). Beyond quantifying the effect size and statistical power that can be anticipated for each study design, we demonstrate that experiments with lower values of N can potentially double or triple information loss. Furthermore, we identify the probability that sampling a similar study will provide a reasonable effect size estimate, and show that using such an approach for power calculations will lead to an imprecise estimate between 40-67% of the time, given commonly used sample sizes. [INSERT RE SKEW]. We conclude with practical recommendations for researchers and demonstrate how our simulation approach can yield theoretical insights that are not readily achieved by other methods; such as identifying the information gained from rejecting the null hypothesis, and quantifying the contribution of individual variation to error in effect size estimates.

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
#opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# knit_hooks$set(inline = function(x) {
#   x <- sprintf("%1.2f", x)
#   paste(x, collapse = ", ")
# })
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
library(rstatix)
library(emmeans)
library(ggridges)
library(car)
library(parallel)
library(kableExtra)
library(moments)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
source('../R/plotting.R') # this has some stats functions we want to use in the document
source('../R/do_infer_stats.R') # extra functions for inferential stats
source('../R/get_summary_stats_for_fx_distributions.R')

```

\clearpage

# Introduction

Despite the complexity involved in disentangling the processes that underpin cognition, decision making regarding experimental outcomes is often made on binary (i.e. pass or fail) terms, across the psychological, neuroscientific and biomedical sciences [@szucsWhenNullHypothesis2017]. Theoretical predictions are often specified in terms of the presence or absence of a given effect, and a yes/no decision is made about whether the null hypothesis (usually a hypothesis of null differences) can be rejected. It seems unlikely that such binary decision-making will be sufficient to disentangle the myriad functional systems that comprises the brain's processes. An alternate approach is to develop theory and models that predict the magnitude of the effect. Such magnitudes are often characterised as an effect size: a standardised measure that reflects the extent to which an effect, such as a mean difference between two conditions, is expected to generalise to the population [Cohen, -@cohenStatisticalPowerAnalysis1988].

A prediction of effect magnitude is easier to disprove than a binary outcome, and therefore constitutes a more desirable prediction for theory testing [@popperLogicScientificDiscovery1959]. To move towards theories that predict changes in effect size magnitude, it is helpful to gain an understanding of how much insight is yielded from our current effect size estimates; i.e. how well are we currently quantifying effect sizes, and should we increase sample sizes to quantify them better? Indeed, recent work suggests that insufficiently powered studies are at increased risk of producing effect size estimates that are either inflated in magnitude, or are in the incorrect direction [@gelmanPowerCalculationsAssessing2014; @chenHandlingMultiplicityNeuroimaging2019]. Here we seek to address how well we currently characterise effect sizes in the study of cognition, using some established paradigms in the fields of attention, executive function and implicit learning; namely the Attentional Blink [AB, @raymondTemporarySuppressionVisual1992], Multitasking [MT, @schumacherVirtuallyPerfectTime2001], Serial Response Task [SRT, @nissenAttentionalRequirementsLearning1987], and Contextual Cueing [CC, @chunContextualCueingImplicit1998] paradigms. 

Accurate quantification of effect sizes is also desirable for study planning, as effect sizes form the foundation of _a priori_ power calculations [Cohen -@cohenStatisticalPowerAnalysis1988]. Here the researcher determines the sample size ($N$) required to achieve sufficient power to correctly reject the null hypothesis. The importance - and difficulty - of accurately determining the anticipated effect size has been considered extensively elsewhere [Cohen, -@cohenStatisticalPowerAnalysis1988; @gelmanPowerCalculationsAssessing2014;, @eggerBiasMetaanalysisDetected1997; @lakensCalculatingReportingEffect2013; @guoSelectingSampleSize2013; @westfallStatisticalPowerOptimal2014; @szucsWhenNullHypothesis2017; @albersWhenPowerAnalyses2018; @cummingNewStatisticsWhy2014]. Standard approaches of determining an anticipated effect size involve consulting a meta-analysis, basing effect-size estimates on a few similar studies (incomplete sampling), or determining the smallest effect that is of theoretical relevance [e.g. @gelmanPowerCalculationsAssessing2014]. What remains somewhat less considered is the utility of knowing how effect size estimates may vary across replications of an experiment [e.g. @cummingNewStatisticsWhy2014; @lorca-pulsImpactSampleSize2018], i.e. what are the distributional properties of the effect size, given a field that uses a comparable $N$ across experiments? 

The answer to this question can facilitate both study planning and theory development. A paradigm that elicits a small effect that manifests with low variability across replications may be considered a more desirable target for theory and model development than a paradigm that produces the same mean effect size but with wider variability. With regard to study planning, identifying the lower bound of an expected effect size facilitates computation of the $N$ required to achieve sufficient statistical power under the worst case scenario [@gelmanPowerCalculationsAssessing2014]. Understanding how effect sizes vary across replications with a given $N$ also allows computation of the likelihood that any single study has produced a reasonably accurate estimate, which can inform the researcher who may be computing anticipated effect sizes on the basis of one or a few similar studies. There is also utility in knowing to what extent variability in effect size observations reduces when larger $N$ are used instead. There may be an upper bound on the accuracy with which a particular effect can be estimated, for example, when the construction of a paradigm introduces a certain level of noise or measurement error that is larger than variation at the level of the individual. Consequently, there may be a point of diminishing returns, where the cost of recruiting extra $N$ will outweigh the gains in accuracy of effect size estimation. 

Quantifying the range of effect sizes that may be observed across experimental replications is not trivial. Indeed, it has been noted that the largest challenge in experimental design is the prior identification of a plausible range of effect sizes [@gelmanPowerCalculationsAssessing2014]. Meta-analytic and incomplete sampling approaches for determining an expected effect size are hampered by the quality of the existing literature [@laneEstimatingEffectSize1978; @brandAccuracyEffectSize2008; @gelmanPowerCalculationsAssessing2014; @fristonTenIronicRules2012; @lorca-pulsImpactSampleSize2018]. A recent survey of 900 effect sizes across psychology disciplines showed that effects from non-pre-registered studies were much larger than pre-registered studies ($r$ = 0.36 vs 0.16, @schaferMeaningfulnessEffectSizes2019) suggesting that prior to pre-registration, under-powered studies were contributing inflated effect size estimates to the psychology literature. It is also difficult to determine, on the basis of existing literature, how conclusions about effect sizes would differ if a given field of study was different, e.g. how much published literature is likely to be missing if a larger $N$ was used as standard?

Simulation studies offer the opportunity to ask how well a field is currently quantifying effect sizes, and how a field's estimate of an effect size would change with differing levels of statistical power. Typically, simulation studies generate data under some simplifying assumptions about the data generation process [e.g. @albersWhenPowerAnalyses2018; @troncososkidmoreBiasPrecisionClassical2013; @laneEstimatingEffectSize1978; @hedgesEstimationEffectSize1982; @westfallStatisticalPowerOptimal2014]. Although this work is necessary for informing how effect size estimates behave under varying conditions where ground truth is known, it is challenging to anticipate all the complexities of data from the repeated-measures designs used across a range of phenomena and processes, such as in the study of attention, executive function and implicit learning. Such data are often not normally distributed and carry varying levels of covariance between conditions. Thus, there remains a question mark over the extent to which the results from simulation work generalizes to real-world data. An alternative method is to simulate experimental outcomes by bootstrapping smaller samples from larger, real data-sets [e.g. @lorca-pulsImpactSampleSize2018]. This approach offers the opportunity to characterize the distributional qualities of effect sizes estimated from high-dimensional data-sets, using varying levels of $N$, while maintaining ecological validity.

In the current study, we applied such a simulation approach to characterize effect size distributions yielded from the study of cognition. Participants ($N$ = 313) completed a battery of cognitive tasks (AB, MT, SRT and CC) originally assembled to test the relationship between attention, executive function and implicit learning. For each paradigm, we simulated 1000 bootstrapped experiments across 20 $N$s ranging from 13 to 313. For each paradigm and from each set of simulations, we determined the impact of $N$ on error in effect size estimates. We asked how much variability of effect size estimates changes as a function of $N$, and sought to identify a point at which increasing $N$ may offer lower gains for improving effect size estimates. We next determined how likely it is that a study will produce an effect size estimate with sufficiently low error, as a function of $N$. \textcolor{blue}{We also} sought to determine the impact of $N$ on the potential for missing literature for each paradigm, given the case of publication bias. \textcolor{blue}{Last, we identified data features that predict error in effect size estimates, beyond the mean and standard deviation measures of which they are a function. Such features may serve as a flag for whether data from a single experiment may be susceptible to error in effect size estimates. We focused on the skew and kurtosis of inter- and intra-subject effects, as such measures can bias mean and variance estimates when datasets violate normality assumptions, yet remain undiscussed in simulation studies that assume normality. The results motivate guidelines for study design and interpretation, not only for future AB, MT, SRT and CC studies, but also more broadly for the investigation of cognition}. 

# Methods

\label{sec:Method}


## Participants
\label{sec:Participants}

The current study used a data set collected for a different [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This data set contains performance measures from $N$ = 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being female, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by The University of Queensland Human Research Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

## Apparatus
\label{sec:Apparatus}

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 7 tasks; Attentional Blink (AB), Multitasking (MT), Contextual Cueing (CC), Serial Response Task (SRT), Visual Statistical Learning (VSL), Operation Span task and a Stop Signal Inhibition task. Only the data from the AB, MT, CC and SRT are reported here. We opted not to report the VSL, OSPAN or Stop Signal data as their design did not lend themselves to the computation of a standardised effect size. 

## Procedures
\label{sec:Procedures}

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure \@ref(fig:FigureParadigm). Details regarding each of the task protocols are presented within each section below. 


```{r, FigureParadigm, out.width='70%', fig.cap='Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters. B) Multitasking Paradigm (MT). Participants discriminate the colour of a disc, a complex tone, or both. C) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the experimental blocks, the stimulus follows a repeating sequence. D) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task where they search for a rotated T among L distractors. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. '}

paradigm.fig.pth <- '../images/FigXXXX_alltasks.pdf'
knitr::include_graphics(paradigm.fig.pth)

```


### Attentional Blink (AB)
\label{sec:ABMeth}

The AB task taps limitations in the deployment of visual information processing over time. Participants are instructed to detect two targets from a rapidly presented series of visual items. Accuracy for the second target is poorer if it appears closer in time to the first target (at early lags, from lag 2 onwards), relative to further apart in time [@raymondTemporarySuppressionVisual1992]. 

#### Protocol

The AB protocol was the same as that reported in Bender et al [-@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letter targets and digit distractors were presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was presented third in the series (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 1.72 x 2.31 $^\circ$ (w x h) visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

### Multitasking (MT)
\label{sec:MTMeth}

MT paradigms tap the performance costs incurred when individuals attempt to perform more than one task concurrently. Participants are instructed to complete two simple sensorimotor tasks as accurately and quickly as possible under single or multitask conditions. RTs to the constituent tasks are typically slowed for multitask relative to single task conditions (see Pashler [-@pashlerDualtaskInterferenceSimple1994a], for a review).

#### Protocol

The MT protocol was previously reported in  Bender et al [-@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two sounds (complex tones taken from Dux, Ivanoff, Asplund, & Marois, [-@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all tasks as quickly and accurately as possible, by using the appropriate key presses ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The MT protocol consisted of 4 blocks of 36 trials, with each trial type (single-task [ST] visual, ST auditory or MT) randomly mixed within blocks. Participants completed the MT protocols after completing two ST blocks as practice, one for the visual task and one for the auditory task. We analysed mean response times (RTs) to each task x modality condition.

### Serial Response Task (SRT)
\label{sec:SRTMeth}

The SRT paradigm taps sensorimotor sequence learning; specifically the extent to which individuals speed up responses when cue stimuli follow a predictable sequence, relative to when cue stimuli are presented randomly [@nissenAttentionalRequirementsLearning1987]. As participants receive no explicit instructions or cues regarding the sequence, it has been assumed that the SRT taps implicit sequence learning [@nissenAttentionalRequirementsLearning1987], although the extent to which performance gains reflect implicit or explicit learning mechanisms continues to be debated [@goschkeImplicitLearningPerceptual1998; @cleggSequenceLearning1998]. Participants are instructed to make a button press response to one of four spatially compatible target stimuli as quickly and accurately as possible. Unknown to the participants, the presentation of the target stimuli will on occasions follow a repeating rather than a random sequence.

#### Protocol
The SRT was adapted from Nissen & Bullemer [-@nissenAttentionalRequirementsLearning1987]. Four square placeholders were presented across the horizontal meridian. A red circle [RGB: 255, 0, 0] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the target circle, using the relevant 'j', 'k', 'l' or ';' keys. The subsequent target stimulus appeared 500 ms after a correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'Random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'Sequence' blocks. Learning in the SRT is tested by comparing mean RTs between Sequence and Repeat blocks in the latter half of the experiment (block 4 vs 3).
<br>

### Contextual Cueing (CC)
\label{sec:CCMeth}

CC tasks tap how the visual system exploits statistical regularities to guide visual search (Sisk, Remington and Jiang, [-@siskMechanismsContextualCueing2019]; Jiang and Sisk [-@jiangContextualCueing2020]). Participants are typically asked to report the orientation of a rotated 'T' target presented among an array of distractor 'L's. Participants are not informed that a set of the displays are repeated throughout the course of the experiment, while the remaining displays are novel to each trial. Typically RTs to the repeat displays become faster than novel displays throughout the course of the experiment [e.g. @chunContextualCueingImplicit1998; @nydamCathodalElectricalStimulation2018]. Participants are typically poor at recognising repeat displays in a subsequent recognition test (Sisk, Remington and Jiang, [-@siskMechanismsContextualCueing2019]; Jiang and Sisk [-@jiangContextualCueing2020]), which has prompted the conclusion that CC reflects a process of implicit learning [but see @vadilloUnderpoweredSamplesFalse2016; @vadilloUnconsciousUnderpoweredProbabilistic2020; @vadilloRaisingAwarenessMeasurement2021].

#### Protocol

The CC protocol was the same as that reported by Nydam et al [-@nydamCathodalElectricalStimulation2018] which is modeled on Chun and Jiang [-@chunContextualCueingImplicit1998]. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80]. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key), as quickly and accurately as possible. The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1:12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable.

## Statistical Approach

All the [data](https://doi.org/10.48610/b63ecc2) and [code](https://github.com/kel-github/Super-Effects) used for the current analyses are available online. All data were analysed using R -@rcoreteamLanguageEnvironmentStatistical2015 and RStudio [@rstudiocitation]. The analysis of the data from each task followed two steps; first, to ascertain that we observed the typical findings for each of the paradigms, we applied the relevant conventional statistical model to the full dataset ($N$=313). Next, we implemented a simulation procedure to determine the effect sizes and p-values that would be attained over many experiments conducted at multiple levels of sample size. 

### Simulation procedure
\label{sec:SamplingProc}

For each paradigm, we simulated experiments across 20 different sample sizes ($N$), defined on a logarithmic interval between $N_{13}$ and $N_{313}$ ($N$ = [13, 15, 18, 21, 25, 30, 36, 42, 50, 59, 69, 82, 97, 115, 136, 160, 189, 224, 265, 313]). We opted for a logarithmic interval given that changes in effect size variability should be greater across changes of $N$ when $N$ is lower, relative to when $N$s are higher. To simulate $k$=1000 experiments at each of our chosen $N$, we sampled $N$ participants from $N_{max}$ ($N_{313}$) over $k$ iterations. The relevant analysis was applied to each of the samples. Details regarding which analyses were applied to each $k$ sample are listed below for each paradigm. Sampling with replacement ensured that the samples carried the Markov property. One potential concern is that any reductions in observed effect size variability may be attributable to saturation as the simulated $N$ approaches the maximum ($N_{313}$), rather than a genuine reduction in variance of the estimate of the effect. Specifically, it could be that as $N$ approaches 313, the overlap of participants between samples is greater than when $N$ equals a lower number such as 13. It follows then that any decreasing variability in effect size estimates at higher $N$s could be due to the decrease in variability of the samples, rather than the improved estimate of the population variance that should come with a larger $N$. We have run simulations that argue against this explanation (see appendix i).

### Effect Sizes
For each paradigm, we report the following information from the simulated effect size distributions; first we used simulations using $N_{313}$ to provide a best estimate of the effect size distribution. We therefore report, for each paradigm, the mean (*M*), median (*Mdn*: when different to the *M*), standard deviation (*SD*), the .025 (lower bound, *LB*) and .975 (upper bound, *UB*) quantiles. These values can be used to define, _a priori_, the range of anticipated effect sizes for future experiments, and consequently, can be used to inform study design.

We next determined to what extent using an $N$ that is typical for the field impacts the effect size distribution. We report the same summary statistics as above, from the simulation using the $N$ that is closest to the typical $N$ for that task ($N_{med}$). To identify the typical $N$, we conducted a survey of the recent literature and computed the median $N$ for each paradigm (see below). We next computed the _precision loss_ incurred from using $N_{med}$ by taking the ratio of the difference between the LB and UB quantiles for $N_{med}$ and $N_{313}$:

$$
qq{\text -}ratio = \frac{ UB_{N_{med}} - LB_{N_{med}}}{UB_{N_{313}} - LB_{N_{313}}}
$$

We refer to this measure from now as the qq-ratio. The qq-ratio indicates how under- or over-inflated effect size estimates may be - a qq-ratio of 2 would suggest that effect sizes may be twice as low or high as the LB or UB of the best estimate. For each task, we also report the largest observed qq-ratio and the $N$ for which the qq-ratio reaches less than double. Note that although we expect qq-ratios to decrease as some function of $\frac{1}{N}$ (given that variance depends on this term), the exact relationship between $N$ and precision loss will be dependent on population variance and measurement error for any given paradigm. We also present qq-ratios across all $N$'s, to provide an idea of potential precision gains from increasing sample size.

Next we computed estimates regarding the extent to which precision loss in effect size estimates may lead a researcher awry during study planning. To determine how often sampling one or two similar studies with $N_{med}$ may induce biases in power calculations, we computed for each task and $N$, the proportion of simulated observations that fell within the LB and UB quantiles of the best estimate ($N_{313}$). This provides the probability that sampling one study will provide an accurate estimate of the true effect size. We refer to this as the probability of attaining a hit, given the sample size (p(hit|$N_{x}$)). (As above, although we expect this to change as a function of $\frac{1}{N}$, the exact relationship is dependent on measurement noise). We next estimate effect size biases that result from aggregating across experiments with statistically significant results (p<.05), under the assumption that the published literature is more likely to only contain significant findings. We computed the difference between the mean effect size from significant results and the mean effect size from all results, and refer to this value as the _inflation bias_. Effectively, this analysis is assessing the severity of the file-drawer effect for different sizes of $N$. To inform understanding of potential file-drawer effects, we also report the proportion of studies that rejected the null hypothesis ($p$ < .05) for $N_{med}$, and the $N$ where this value reached 90% (note: this is related to the observed effect size, but we report it here for clarity).

\textcolor{blue}{Last, we sought metrics that may inform whether an experiment has yielded an imprecise effect size estimate. Effect sizes are a function of the variability of the effect across individuals, as well as intra-individual variability over trials} [@rouderPowerDominanceConstraint2018]. \textcolor{blue}{If either of these stem from a non-normal distribution, mean and standard deviation estimates -and consequently effect size computations- may be impacted. We thus determined whether the skewness and kurtosis of this data could predict error in effect size estimates.} 

\textcolor{blue}{Error in effect sizes were defined for each task as the difference between the expected value for $N_{313}$ and each observed effect size from $N_{med}$. For each $N_{med}$ simulation, we calculated the key behavioural effect for each participant (in raw units) and computed the Pearson's skewness and kurtosis coefficients of the resulting distribution of effects. We also sought to characterise the skewness and kurtosis of intra-individual variability; for each simulated experiment, we computed the variability, skew and kurtosis from each participant's performance across trials, and took the means of these measures across participants. The resulting variables (effect skewness, effect kurtosis, mean intra-individual variance, skewness, and kurtosis) served as predictors in a multiple regression analysis, using effect size error as the criterion variable. If any of the regressors themselves showed high levels of skew then a log transformation was applied. All model residuals were checked for homoscedasticity. Note that although we present the full models below, performing stepwise regression yielded the same pattern of results.} 

\textcolor{blue}{To protect against interpreting over-fitted models, we performed k-fold cross-validation for each multiple regression model, where k=10, and we report the mean} $r_{cv}^2$ \textcolor{blue}{ (and standard deviation) across folds. Next, we identified potential general predictors by determining which regressors consistently predicted effect size error across the four tasks. We then sought to identify which values of such predictors suggest a problematic effect size error (defined as effect size errors that were less or more than the .025 and .975 quantiles for} $N_{313}$ \textcolor{blue}{). We achieved this using simple regression, as we sought to simulate how much variability may be accounted for when a researcher uses a single piece of information to estimate effect size imprecision.}

#### Computing Effect Sizes

To compute effect sizes for the paradigms analysed using a repeated-measures ANOVA (AB, MT and CC), we computed partial epsilon squared ($\epsilon_{p}^2$), as this measure is unbiased, unlike $\eta_{p}^2$ [@okadaOmegaSquaredLess2013]. (Indeed, an earlier version of our manuscript showed that $\eta_{p}^2$ estimates are biased on average, even for sample 
sizes of $N$=313, ^[See  for Supplemental Figures documenting this analysis: https://github.com/kel-github/Super-Effects/tree/master/doc/supp-figs. Note: we thank a helpful reviewer for drawing our attention to this]). We use the formula for $\epsilon_{p}^2$ as defined in [@carrollSamplingCharacteristicsKelley1975, eq 11]:


\begin{equation}
\epsilon_{p}^{2} = \frac{F-1}{F + \frac{df_w}{df_b}}
\end{equation}

where $F$ is the F statistic for the effect, $df_{w}$ is the degrees of freedom within groups, and $df_{b}$ is the degrees of freedom between groups. The SRT paradigm instead uses a paired-samples design. For this paradigm we computed Cohen's $d_{z}$ (see @lakensCalculatingReportingEffect2013, eq 6):

\begin{equation}
d_{z} = \frac{M_{diff}}{\sqrt{\frac{\sum(X_{diff} - M_{diff})^2}{N-1}}}
\end{equation}

where $M_{diff}$ is the mean difference between groups, and $X_{diff}$ is the difference score for one subject. 

To facilitate our interpretation of effect sizes as small, medium or large, we refer to Cohen [-@cohenPowerPrimer1992] for $\epsilon_{p}^2$ and to Gignac & Szodorai, [-@gignacEffectSizeGuidelines2016] for $d_z$.

#### Representative N

To attain an $N$ that reflects what is commonly used for each paradigm, we surveyed the three most relevant _Journal of Experimental Psychology_ journals (_General_, _Human Perception & Performance_ and _Learning, Memory & Cognition_) for all articles mentioning use of any of the current paradigms. We searched back for a total of 60 experiments or back from today to 2005, whichever occurred first. We then computed the median sample size used across all experiments found from the survey. The results from the survey are presented in Table 1.

```{r, survey, message = FALSE, warning = FALSE}

survey_dat <- data.frame(task = c("AB", "MT", "CC", "SRT"),
                         e = c(60, 60, 49, 60),
                         med = c(24, 40, 24, 34))
knitr::kable(survey_dat, col.names = c("task", "n exp", "med N"), align = "lcc", digits = c(0, 0, 0, 0), "simple", caption = "Typical N found from literature survey. n exp = number or experiments, med N = median N")
```

### Analysis of Experimental Tasks

#### Attentional Blink

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\epsilon_{p}^2$ and the resulting $p$ value were taken for the main effect of lag. For this task, and all remaining ANOVA tests, models were fit using the anova_test() function from the [rstatix](https://rpkgs.datanovia.com/rstatix/index.html) package. Where possible, the models were fit using type 3 sum of squares, owing to the computational expediency and match to commercial statistical software packages. In some cases, models were unable to be fit using type 3 sum of squares, owing to rank deficiencies in the underlying design matrix (e.g. when one participant was drawn more than twice within a sample). In these cases, models were fit using type 1 sum of squares. However, as the experiment designs were fully balanced, each sum of squares type should yield the same results.

#### Multitasking
To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: ST vs MT) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\epsilon_{p}^2$ and $p$ are reported for both the main effect of task and the task-modality x task interaction. 

#### Serial Response Task

To ascertain whether participants learned the repeating sequences, RTs in the final block of sequence trials (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d_{z}$, and $p$ value from each test. 

#### Contextual Cueing

To ascertain whether participants became faster for repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated displays), the data were subject to a block (1:12) x condition (repeat vs novel display) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\epsilon_{p}^2$ and $p$ for the block x condition interaction.

As some studies from the contextual cueing literature suggest that the effect is better characterised by a main effect of condition thereby implying rapid learning of the statistical regularities [e.g. @petersonAttentionalGuidanceEyes2001; @travisRoleWorkingMemory2013], we also report the $\epsilon_{p}^2$ and $p$ for the main effect of condition.

<br>

# Results
\label{sec:Results}

We first present the results from the standard analyses used for each task, to show that we replicate the classic findings from each task. The key behavioural data are presented in Figure \@ref(fig:behavResults). 


```{r, behavResults, out.width='70%', fig.cap='Behavioural Results. A) Attentional Blink Paradigm (AB). Accuracy (acc) for T2|T1 was lower at early lags, relative to later lags. Note that T1 accuracy is also plotted. B) Multitasking Paradigm (MT). RTs were slowed for multitask (M) conditions, relative to single-tasks (S). This difference was larger for sound tasks (So) than for visual (V) tasks. C) Serial Response Task (SRT). In the second half of the experiment, RTs were faster in the sequence (S) relative to the random (R) condition. D) Contextual Cueing (CC). RTs were faster for the repeat (R) than for the novel (N) displays, and this difference became larger throughout the course of the experiment.'}

behav.results.fig.pth <- '../images/EPS_all_tasks_behav.pdf'
knitr::include_graphics(behav.results.fig.pth)

```

## Behavioural Results

### Attentional Blink
\label{sec:ABRes}

```{r, AB_analysis, message=F}
abfname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
ab_res <- do.AB.analysis(abfname)
```

The AB data are presented in Figure \@ref(fig:behavResults)A. Accuracy for T2|T1 was lower for early relative to late lags; accuracy for T2|T1 decreased (by around p = `r sprintf("%.2f", abs(ab_res[[2]][,3]$estimate))`) when T2 was presented at lag 2, relative to lag 7. A one-way ANOVA revealed that the effect of lag was statistically significant (F (`r sprintf("%.1f", ab_res[[1]]$DFn)`, `r sprintf("%.0f", ab_res[[1]]$DFd)`) = `r sprintf("%.0f", ab_res[[1]]$F)`, $\epsilon_{p}^2$ = `r sprintf("%.2f", ab_res[[3]][["lag"]])`, p = `r sprintf("%.2e", ab_res[[1]]$p)`). Post-hoc t-tests showed that accuracy at each lag differed statistically from accuracy at each of the other lags (all p's $\leq$ `r sprintf("%.2e", max(do.call(rbind, lapply(1:6, function(x) cbind(ab_res[[2]][,x]$p.value)))))`). Therefore, the AB paradigm yielded the typically observed effects.

### Multitasking

```{r, MT_F, message=F}
mtfname <- "../data/total_of_313_subs_SingDual_task_trial_level_data.csv"
mt_res <- do.MT.analysis(mtfname)
```

As anticipated, RTs were slowed for multitask relative to single task conditions (see Figure \@ref(fig:behavResults)B). Mean RTs were on average `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$estimate[[1]]))` (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[2]))`, `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[1]))`]) seconds (s) slower on MT trials (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "trialtype"])`) = `r sprintf("%4.0f", mt_res[[1]]$F[mt_res[[1]]$Effect == "trialtype"])`,  $\epsilon_{p}^2$ = `r sprintf("%.2f", mt_res[[3]][["trialtype"]])`, p<.0001). There was also a significant task modality (sound or visual) x task (ST vs MT) interaction  (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "task:trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "task:trialtype"])`) = `r sprintf("%2.1f", mt_res[[1]]$F[mt_res[[1]]$Effect == "task:trialtype"])`,  $\epsilon_{p}^2$ = `r sprintf("%.2f", mt_res[[3]][["task:trialtype"]])`, p<.0001). The MT cost (MT RT - ST RT) was larger for  the sound task relative to the visual task by on average `r sprintf("%.2f", abs(mt_res[[2]]$interaction$estimate[[1]]))` s (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[1]))`, `r sprintf(sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[2])))`]). This latter finding has been reported previously [@hazeltineModalityPairingEffects2006]. We continue to interrogate this effect, as it serves as an example of an interaction with a small effect size. This facilitates comparisons to the contextual cueing task, as reported below.

### SRT

```{r, SRT_F, message=F}
srtfname <- "../data/total_of_313_subs_SRT_task_trial_level_data.csv"
srt_res <- do.SRT.analysis(srtfname)
```

The results from the SRT paradigm are presented in Figure \@ref(fig:behavResults)C. Participants learned the repeating sequence; RTs were on average `r sprintf("%.3f", srt_res[[1]]$estimate/1000)` s faster (95% CI [`r sprintf("%.3f", srt_res[[1]]$conf.int[[1]]/1000)`, `r sprintf("%.3f", srt_res[[1]]$conf.int[[2]]/1000)`]) for the sequence relative to the random condition (t(`r sprintf("%3.0f", srt_res[[1]]$parameter[[1]])`) = `r sprintf("%2.2f", srt_res[[1]]$statistic[[1]])`, $d_z$ = `r sprintf("%1.2f", srt_res[[2]])`, p = `r sprintf("%.2e", srt_res[[1]]$p.value)`). 

### Contextual Cueing
\label{sec:CCRes}

```{r, CC_interaction, message=F}
ccfname <- "../data/total_of_313_subs_CC_task_trial_level_data.csv"
cc_res <- do.CC.analysis(ccfname)
```

Participants learned the repeat displays over blocks (see Figure \@ref(fig:behavResults)D); the RT data showed a significant albeit small block x condition interaction (F (`r sprintf("%2.2f", cc_res[[1]]$DFn[3])`, `r sprintf("%4.1f", cc_res[[1]]$DFd[3])`) = `r sprintf("%1.2f", cc_res[[1]]$F[3])`, $\epsilon_{p}^2$ = `r sprintf("%.2f", cc_res[[3]][["block:type"]])`, p = `r sprintf("%.2e", cc_res[[1]]$p[3])`). There was no statistically significant difference between RTs for repeat and novel displays for block 1: (t (`r sprintf("%3.0f", cc_res[[2]][[1]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[1]]$statistic)`, p = `r sprintf("%.2f", cc_res[[2]][[1]]$p.value)`, $\mu$ difference = `r sprintf("%.2f", cc_res[[2]][[1]]$estimate/1000)` s, sd: `r sprintf("%.2f", (cc_res[[2]][[1]]$stderr/1000)*sqrt(313))`). However, by block 12, RTs for repeat displays were on average `r sprintf("%.2f", cc_res[[2]][[2]]$estimate/1000)` s faster than novel displays (sd: `r sprintf("%.2f", (cc_res[[2]][[2]]$stderr/1000)*sqrt(313))`, t (`r sprintf("%3.0f", cc_res[[2]][[2]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[2]]$statistic)`, p = `r sprintf("%.2e", cc_res[[2]][[2]]$p.value)`. There was also a significant and larger main effect of block (F(`r sprintf("%.2f", cc_res[[1]]$DFn[1])`, `r sprintf("%.2f", cc_res[[1]]$DFd[1])`) = `r sprintf("%1.2f", cc_res[[1]]$F[1])`, $\epsilon_{p}^2$ = `r sprintf("%.2f", cc_res[[3]][["block"]])`, p = `r sprintf("%.2e", cc_res[[1]]$p[1])`). and a significant main effect of condition (F(`r sprintf("%.2f", cc_res[[1]]$DFn[2])`, `r sprintf("%.2f", cc_res[[1]]$DFd[2])`) = `r sprintf("%1.2f", cc_res[[1]]$F[2])`, $\epsilon_{p}^2$ = `r sprintf("%.2f", cc_res[[3]][["type"]])`, p = `r sprintf("%.2e", cc_res[[1]]$p[2])`).


## Effect Sizes

```{r, attEFfx, out.width='70%', fig.cap='Effect size distributions for the AB and MT paradigms. A) AB: Partial epsilon sq distributions for selected N for the main effect of lag. B) Showing the mean partial epsilon squared, and the UB and LB quantiles [.025, .975], for the main effect of lag, across N (AB). C) MT: Same as in A, but for the main effect of task condition (MT). D) Same as in B, for the main effect of task condition (MT), E) As in C, but for the task x modality interaction (MT), E) As D, but for the MT task x modality interaction'}

attEF.fx.fig.pth <- '../images/EPS_EF_tasks_fx_sz.pdf'
knitr::include_graphics(attEF.fx.fig.pth)

```

### Summary Statistics and Precision Loss

Across tasks, we observed a range of small to large effect sizes ($epsilon_{p}^2$: .01 -.9), thus we are able to characterize the extent of precision loss across a range of effect size scenarios. For studies run with $N_{med}$, the range of precision losses we observed was 1.78 - 4.16, suggesting that caution is warranted when basing power calculations on the outcomes of a small number of studies. The $N$ required to reduce precision loss to < 2 ranged from 36 - 82. For both the interaction effects currently studied (MT and CC), the effect size distributions for $N_{med}$ spanned from below to above zero, suggesting that differing conclusions may be reached across studies. Specifically, when the effect size is less than zero, the direction of the effect has the opposite sign. The observed power to reject the null hypothesis ranged from p=.35 - 1, suggesting areas where there may be missing literature owing to publication bias. We next report these details for each task.

```{r, get_sum_stats, echo=F, message=F}
sum_fx_p_dat <- get_summary_data_fx_p(FALSE)

# note: interpret effect size estimates using Cohen's 1992 power primer
# https://cran.r-project.org/web/packages/effectsize/vignettes/interpret.html
```

#### Attentional Blink

```{r, set_AB_med}

# get relevant stats
get_stats_4_fx <- function(dat, task, mod, meas, medN, ns){
  # get relevant stats for reporting
  best_mu <-  dat$mu[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_med <- dat$med[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_sd <- dat$sd[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_LB <- dat$LB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_UB <- dat$UB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_sk <- dat$sk[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  best_ku <- dat$ku[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]
  
  medN_mu <- dat$mu[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_med <- dat$med[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_sd <- dat$sd[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_LB <- dat$LB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_UB <- dat$UB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_sk <-  dat$sk[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  medN_ku <- dat$ku[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == medN]
  
  qq_ratio <- do.call(rbind, lapply(ns, function(x) 
                abs(dat$LB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == x]-
                    dat$UB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == x])/
                abs(dat$LB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313]-
                    dat$UB[dat$task == task & dat$mod == mod & dat$meas == meas & dat$n == 313])))
  
  rownames(qq_ratio) <- ns
   
  out <- list(best_mu, best_med, best_sd, best_LB, best_UB, best_sk, best_ku, 
              medN_mu, medN_med, medN_sd, medN_LB, medN_UB, medN_sk, medN_ku, qq_ratio)
  names(out) <- c("best_mu", "best_med", "best_sd", "best_LB", "best_UB", "best_sk", "best_ku",
                  "medN_mu", "medN_med", "medN_sd", "medN_LB", "medN_UB", "medN_sk", "medN_ku",
                  "qq_ratio")
  out
  
}


# get the bias values
get_p_data <- function(res, m, t_n){
  ps <- do.call(rbind, lapply(1:t_n, function(x) res[,"sig"][[x]][[m]])) 
  rownames(ps) <- c(13, 15, 18, 21, 25, 30, 36, 42, 50, 59, 69, 82, 97, 115, 136, 160, 189, 224, 265, 313)
  ps
}

t_n <- 20 # for all 
AB_med <- 25
AB_fx_stats <- get_stats_4_fx(sum_fx_p_dat, task="AB", mod="RM-AN", meas="esz", medN=25, ns=unique(sum_fx_p_dat$n))
load("../data/AB/EPSABstats.RData")
AB_ps <- get_p_data(res, "RM-AN", t_n)

```


The AB effect was large (see Figure \@ref(fig:attEFfx)A); $N_{313}\ \epsilon_{p}^2\ M$ = `r sprintf("%.2f", AB_fx_stats[["best_mu"]])` (*SD*: `r sprintf('%.2f', AB_fx_stats[["best_sd"]])`, *LB*: `r sprintf("%.2f", AB_fx_stats[["best_LB"]])`, *UB*: `r sprintf("%.2f", AB_fx_stats[["best_UB"]])`). The simulated effect sizes for $N_{med}$ ($N_{25}$) produced the same mean effect size estimate (*M*: `r sprintf("%.2f", AB_fx_stats[["medN_mu"]])`, *SD*: `r sprintf("%.2f", AB_fx_stats[["medN_sd"]])`, *LB*: `r sprintf("%.2f", AB_fx_stats[["medN_LB"]])`, *UB*: `r sprintf("%.2f", AB_fx_stats[["medN_UB"]])`, see Figure \@ref(fig:attEFfx)B). With regard to extent of precision loss; the qq-ratio for $N_{med}$ was `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["25", "2.5%"])`. The qq-ratio for small $N$ was ~3 ($N_{13}$ = `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["15", "2.5%"])`), and reached < 2 at $N_{42}$ ($N_{36}$ = `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["36", "2.5%"])`, $N_{42}$ = `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["42", "2.5%"])`). The remaining qq-ratios are presented in Figure \@ref(fig:fxqq). 

Across all $N$, the probability of rejecting the null hypothesis was 1.


#### Multitasking

```{r, get_MT_fxstats}

MT_med <- 42
MT_fx_stats_ME <- get_stats_4_fx(sum_fx_p_dat, task="SD", mod="tt", meas="esz", medN=MT_med, ns=unique(sum_fx_p_dat$n))
MT_fx_stats_int <- get_stats_4_fx(sum_fx_p_dat, task="SD", mod="ta*tt", meas="esz", medN=MT_med, ns=unique(sum_fx_p_dat$n))

load("../data/SD/EPSSDstats.RData")
MT_ps_ME <- get_p_data(res, "RM-AN", t_n)
MT_ps_int <- get_p_data(res, "LME", t_n)

```

##### Main effect of task condition

For the MT paradigm, the main effect of task condition was large ($N_{313}\ \epsilon_{p}^2\ M$ = `r sprintf("%.2f", MT_fx_stats_ME[["best_mu"]])`, *SD*: `r sprintf('%.2f', MT_fx_stats_ME[["best_sd"]])`, *LB*: `r sprintf("%.2f", MT_fx_stats_ME[["best_LB"]])`, *UB*: `r sprintf("%.2f", MT_fx_stats_ME[["best_UB"]])`), and the simulated effect sizes for $N_{med}$ ($N_{42}$) produced the same mean effect size estimate (*M*: `r sprintf("%.2f", MT_fx_stats_ME[["medN_mu"]])`, *SD*: `r sprintf("%.2f", MT_fx_stats_ME[["medN_sd"]])`, *LB*: `r sprintf("%.2f", MT_fx_stats_ME[["medN_LB"]])`, *UB*: `r sprintf("%.2f", MT_fx_stats_ME[["medN_UB"]])`, see Figure \@ref(fig:attEFfx)D). With regard to precision loss, the qq-ratio for $N_{med}$ was `r sprintf("%.2f", MT_fx_stats_ME[["qq_ratio"]]["42", "2.5%"])`.  Comparable to the AB, qq-ratio for small $N$ was ~3 ($N_{13}$ = `r sprintf("%.2f", MT_fx_stats_ME[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", MT_fx_stats_ME[["qq_ratio"]]["15", "2.5%"])`), and was < 2 for $N_{36}$ ($N_{30}$ = `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["30", "2.5%"])`, $N_{36}$ = `r sprintf("%.2f", MT_fx_stats_ME[["qq_ratio"]]["36", "2.5%"])`). The remaining qq-ratios are presented in Figure \@ref(fig:fxqq).

Across all $N$, the probability of rejecting the null hypothesis was 1.

##### Task condition by modality interaction

The task condition x modality interaction achieved a medium effect size ($N_{313}\ \epsilon_{p}^2\ M$ = `r sprintf("%.2f", MT_fx_stats_int[["best_mu"]])`, *SD*: `r sprintf('%.2f', MT_fx_stats_int[["best_sd"]])`, *LB*: `r sprintf("%.2f", MT_fx_stats_int[["best_LB"]])`, *UB*: `r sprintf("%.2f", MT_fx_stats_int[["best_UB"]])`, see Figure \@ref(fig:attEFfx)E), and the simulated effect sizes for $N_{med}$ produced the same mean effect size estimate (*M*: `r sprintf("%.2f", MT_fx_stats_int[["medN_mu"]])`, *Mdn*: `r sprintf("%.2f", MT_fx_stats_int[["medN_med"]])`, *SD*: `r sprintf('%.2f', MT_fx_stats_int[["medN_sd"]])`). However, the *LB* and *UB* quantiles from $N_{med}$ crossed zero (*LB*: `r sprintf("%.2f", MT_fx_stats_int[["medN_LB"]])`, *UB*: `r sprintf("%.2f", MT_fx_stats_int[["medN_UB"]])`, see Figure \@ref(fig:attEFfx)F), suggesting that using $N_{med}$ will sometimes produce differing inferences with regard to the effect size, compared to $N_{313}$. With regard to precision loss, the qq-ratio for $N_{med}$ was `r sprintf("%.2f", MT_fx_stats_int[["qq_ratio"]]["42", "2.5%"])`. The qq-ratio for small $N$ was ~2.75 ($N_{13}$ = `r sprintf("%.2f", MT_fx_stats_int[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", MT_fx_stats_int[["qq_ratio"]]["15", "2.5%"])`), and reached < 2 at $N_{36}$ ($N_{30}$ = `r sprintf("%.2f", MT_fx_stats_int[["qq_ratio"]]["30", "2.5%"])`, $N_{36}$ = `r sprintf("%.2f", MT_fx_stats_int[["qq_ratio"]]["36", "2.5%"])`). The remaining qq-ratios are presented in Figure \@ref(fig:fxqq).

The probability of rejecting the null hypothesis at $N_{med}$ was `r sprintf("%.2f", MT_ps_int[paste(MT_med),])`. A sample size of $N_{82}$ was required to achieve statistical power of > 90 % ($N_{69}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(69),])`, $N_{82}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(82),])`).

#### Serial Response Task

```{r, get_SRT_fxstats}

SRT_med <- 36
SRT_fx_stats <- get_stats_4_fx(sum_fx_p_dat, task="SRT", mod="RM-AN", meas="esz", medN=SRT_med, ns=unique(sum_fx_p_dat$n))
load("../data/SRT/EPSSRTstats.RData")
SRT_ps <- get_p_data(res, "RM-AN", t_n)

```


For the SRT, the effect of sequence vs random was large ($N_{313}\ d_z\ M$: `r sprintf("%.2f", SRT_fx_stats[["best_mu"]])`, *SD*: `r sprintf('%.2f', SRT_fx_stats[["best_sd"]])`, *LB*: `r sprintf("%.2f", SRT_fx_stats[["best_LB"]])`, *UB*: `r sprintf("%.2f", SRT_fx_stats[["best_UB"]])`, Figure \@ref(fig:ILfx)A). Here, there was disagreement between $N_{313}$ and $N_{med}$ ($N_{36}$) regarding the means of the simulated effect size distributions ($N_{med}\ d_z\ M$ = `r sprintf("%.2f", SRT_fx_stats[["medN_mu"]])`, *SD*: `r sprintf("%.2f", SRT_fx_stats[["medN_sd"]])`, *LB*: `r sprintf("%.2f", SRT_fx_stats[["medN_LB"]])`, *UB*: `r sprintf("%.2f", SRT_fx_stats[["medN_UB"]])`, see Figure \@ref(fig:ILfx)B). With regard to precision loss, the qq-ratio for $N_{med}$ was `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["36", "2.5%"])`. The remaining qq-ratios are presented in Figure \@ref(fig:fxqq). The qq-ratio for small $N$ was ~3.5 ($N_{13}$ = `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["15", "2.5%"])`), and reached under 2 at $N_{42}$ ($N_{36}$ = `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["36", "2.5%"])`, $N_{42}$ = `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["42", "2.5%"])`).

Across all sampled $N$, the probability of rejecting the null hypothesis was 1.

```{r, ILfx, out.width='70%', fig.cap='Effect size distributions observed for the SRT and CC paradigms. A) SRT: Cohens dz for the effect of sequence learning, for selected N. B) Showing the mean dz, and the UB and LB quantiles [.025, .975], for the effect of sequence, across N (SRT). C) CC: Same as in A, but for the block x condition interaction. D) Same as in B, for the block x condition interaction (CC), E) As in C, but for the main effect of condition (CC), E) As D, but for the main effect of condition (CC)'}

IL.fx.fig.pth <- '../images/EPS_IL_tasks_fx_sz.pdf'
knitr::include_graphics(IL.fx.fig.pth)

```

#### Contextual Cueing

```{r, get_CC_fxstats}

CC_med <- 25
CC_fx_stats_int <- get_stats_4_fx(sum_fx_p_dat, task="CC", mod="b*c", meas="esz", medN=CC_med, ns=unique(sum_fx_p_dat$n))
CC_fx_stats_ME <- get_stats_4_fx(sum_fx_p_dat, task="CC", mod="c", meas="esz", medN=CC_med, ns=unique(sum_fx_p_dat$n))

load("../data/CC/EPSCCstats.RData")
CC_ps_int <- get_p_data(res, "RM-AN", t_n)
CC_ps_ME <- get_p_data(res, "LME", t_n)
```

##### Block x Condition Interaction

The block x condition interaction effect was on the boundary between very small and small ($N_{313}$ $\epsilon_{p}^2$ *M*: `r sprintf("%.2f", CC_fx_stats_int[["best_mu"]])`, *SD*: `r sprintf("%.2f", CC_fx_stats_int[["best_sd"]])`, *LB*: `r sprintf("%.2f", CC_fx_stats_int[["best_LB"]])`, *UB*: `r sprintf("%.2f", CC_fx_stats_int[["best_UB"]])`, Figure \@ref(fig:ILfx)C). There was a minor discrepancy between the $N_{313}$ and $N_{med}$ ($N_{25}$) means, but the $N_{med}$ *Mdn* agreed (*M*: `r sprintf("%.2f", CC_fx_stats_int[["medN_mu"]])`, *Mdn*: `r sprintf("%.2f", CC_fx_stats_int[["medN_med"]])`, *SD*: `r sprintf("%.2f", CC_fx_stats_int[["medN_sd"]])`). Similar to the SRT task, the effect size distribution for $N_{med}$ included zero ($N_{med}$ *LB*: `r sprintf("%.2f", CC_fx_stats_int[["medN_LB"]])`, *UB*: `r sprintf("%.2f", CC_fx_stats_int[["medN_UB"]])`), thus experiments with $N_{med}$ may sometimes motivate different conclusions to $N_{313}$. Specifically, when the effect size is below zero, it would be concluded that repeating displays leads to a slowing of RTs (rather than speeding RTs), relative to novel displays. There was also a greater extent of precision loss at $N_{med}$ than was observed for other tasks (qq-ratio: `r sprintf("%.2f", CC_fx_stats_int[["qq_ratio"]]["25", "2.5%"])`). The qq-ratio for small $N$ was ~6 ($N_{13}$ = `r sprintf("%.2f", CC_fx_stats_int[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", CC_fx_stats_int[["qq_ratio"]]["15", "2.5%"])`), and reached under 2 at $N_{82}$ ($N_{69}$ = `r sprintf("%.2f", CC_fx_stats_int[["qq_ratio"]]["69", "2.5%"])`, $N_{82}$ = `r sprintf("%.2f", CC_fx_stats_int[["qq_ratio"]]["82", "2.5%"])`). The remaining qq-ratios are presented in Figure \@ref(fig:fxqq). 

The probability of rejecting the null hypothesis at $N_{med}$ was $p$ = `r sprintf("%.2f", CC_ps_int[paste(CC_med),])`. A sample size of $N_{82}$ was required to achieve statistical power of > 90 % ($N_{69}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(69),])`, $N_{82}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(82),])`).

##### Main Effect of Condition

The main effect of condition was large ($N_{313}$ $\epsilon_{p}^2$ *M*: `r sprintf("%.2f", CC_fx_stats_ME[["best_mu"]])`, *SD*: `r sprintf("%.2f", CC_fx_stats_ME[["best_sd"]])`, *LB*: `r sprintf("%.2f", CC_fx_stats_ME[["best_LB"]])`, *UB*: `r sprintf("%.2f", CC_fx_stats_ME[["best_UB"]])`, see Figure \@ref(fig:ILfx)E). There was a minor discrepancy between the mean estimates for $N_{313}$ and $N_{med}$ (*M*: `r sprintf("%.2f", CC_fx_stats_ME[["medN_mu"]])`, *Mdn*: `r sprintf("%.2f", CC_fx_stats_ME[["medN_med"]])`, *SD*: `r sprintf("%.2f", CC_fx_stats_ME[["medN_sd"]])`, *LB*: `r sprintf("%.2f", CC_fx_stats_ME[["medN_LB"]])`, *UB*: `r sprintf("%.2f", CC_fx_stats_ME[["medN_UB"]])`, see Figure \@ref(fig:ILfx)F). Precision loss was comparable to the SRT (qq-ratio: `r sprintf("%.2f", CC_fx_stats_ME[["qq_ratio"]]["25", "2.5%"])`). The qq-ratio for small $N$ was ~2.8 ($N_{13}$ = `r sprintf("%.2f", CC_fx_stats_ME[["qq_ratio"]]["13", "2.5%"])`, $N_{15}$ = `r sprintf("%.2f", CC_fx_stats_ME[["qq_ratio"]]["15", "2.5%"])`), and reached under 2 at $N_{36}$ ($N_{30}$ = `r sprintf("%.2f", SRT_fx_stats[["qq_ratio"]]["30", "2.5%"])`, $N_{36}$ = `r sprintf("%.2f", CC_fx_stats_ME[["qq_ratio"]]["36", "2.5%"])`). The remaining qq-ratios are presented in Figure \@ref(fig:fxqq). 

The probability of rejecting the null hypothesis at $N_{med}$ was $p$ = `r sprintf("%.2f", CC_ps_ME[paste(CC_med),])`. A sample size of $N_{136}$ was required to achieve statistical power of > 90 % ($N_{115}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(115),])`, $N_{136}\ p$ = `r sprintf("%.2f", MT_ps_int[paste(136),])`).

```{r, QQratioplotdata}

# first concatenate the qq-ratios
stats <- list(AB_fx_stats, MT_fx_stats_ME, MT_fx_stats_int,
              SRT_fx_stats, CC_fx_stats_int, CC_fx_stats_ME)
qqrats_acrss_tsks <- unlist(lapply(stats, function(x) x[["qq_ratio"]]))
qqrats_acrss_tsks <- tibble(task = rep(c("AB", "tc", "tc*m", "SRT", "b*c", "c"), each=20),
                            N = rep(c(13, 15, 18, 21, 25, 30, 36, 42, 50, 59, 69, 82, 97, 115, 136, 160, 189, 224, 265, 313), times=6),
                            qqrat = qqrats_acrss_tsks)
# now you can run the code in R/plot_metrics_by_N_across_tasks.R to generate the qq ratio plot
```

```{r, fxqq, out.width='70%', fig.cap='QQ-ratios plotted by N for each task effect. AB: Attentional Blink, tc: main effct of task condition from the MT paradigm, tc*m: trial condition x modality interaction, SRT: Serial Response Task, b*c: block x condition interaction from the CC task, c: main effect of condition from the CC task.'}

qqrat.fx.fig.pth <- '../images/EPS_all_tasks_qqratio.pdf'
knitr::include_graphics(qqrat.fx.fig.pth)

```


### Impacts of imprecision and missing literature

```{r, get_p_hit_dat}

tasks <- c("AB", "CC", "SRT", "SD")
ftmplt = "../data/%s/%s_phitbest.RData"

########### LOAD DATASETS ################
get_prct_dats <- function(ftmplt, task){
  load(sprintf(ftmplt, task, task))
  if(task == "AB" | task == "SRT"){ 
    out$mod <- NA
    out$mod <- task
  }
  out
}

phit_dat <- do.call(rbind, lapply(tasks, get_prct_dats, ftmplt=ftmplt))

```

Having characterized the effect size distributions for each task, we next sought to determine the impact of effect size imprecision when basing power calculations on a similar study that uses $N_{med}$, and the extent to which effect size estimates could be inflated in cases where there may be missing information owing to publication bias. For the former, we computed p(hit|$N$); for the AB, MT and SRT paradigms, the p(hit|$N_{med}$) was ~0.66 (AB: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "AB" & phit_dat$n == AB_med])`, MT tc: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "tt" & phit_dat$n == MT_med])`, MT tc x m: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "ta*tt" & phit_dat$n == MT_med])`, SRT: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "SRT" & phit_dat$n == SRT_med])`). This suggests that sampling a similar study will produce a reasonable _a priori_ effect size estimate 2/3 of the time (Note: it is interesting that the AB, MT and SRT fields appear to have converged on an $N_{med}$ that puts them on a comparable footing for hitting the best effect size. Indeed, if the MT and SRT fields used the same sample size as the AB field, the p(hit|$N_{25}$) ratios for the three effects would be ~0.57 (MT tc: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "tt" & phit_dat$n == AB_med])`, MT tc x m: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "ta*tt" & phit_dat$n == AB_med])`, SRT: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "SRT" & phit_dat$n == AB_med])`)). For the CC paradigm, the p(hit|$N_{med}$ = ~.48 (b x c: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "b*c" & phit_dat$n == CC_med])`, c: `r sprintf("%.2f", phit_dat$p_upper[phit_dat$mod == "c" & phit_dat$n == CC_med])`). This suggests that basing effect size estimates on a similar CC study will result in an appropriately powered study 50% of the time. The remaining p(hit|$N_x$) are presented in Figure \@ref(fig:fxphit).


```{r, fxphit, out.width='70%', fig.cap='probability of a single study producing an effect size estimates that are within the LB and UB for the best estimate (p(hit|N)), plotted by N for each task effect. AB: Attentional Blink, tc: main effct of task condition from the MT paradigm, tc*m: trial condition x modality interaction, SRT: Serial Response Task, b*c: block x condition interaction from the CC task, c: main effect of condition from the CC task.'}

phit.fx.fig.pth <- '../images/EPS_all_tasks_phit.pdf'
knitr::include_graphics(phit.fx.fig.pth)

```

```{r, get_fdrws}

# get the bias values
get_meta_bias <- function(res, m){
  bias <- do.call(rbind, lapply(1:t_n, function(x) res[,"stats_sig"][[x]]["mu", m][[1]]-
                                  res[,"stats_fx"][[x]]["mu", m][[1]])) 
  rownames(bias) <- c(13, 15, 18, 21, 25, 30, 36, 42, 50, 59, 69, 82, 97, 115, 136, 160, 189, 224, 265, 313)
  bias
}

load("../data/SD/EPSSDstats.RData")
t_n <- 20
MT_t <- get_meta_bias(res, "RM-AN")
MT_tm <- get_meta_bias(res, "LME")

load("../data/CC/EPSCCstats.RData")
CC_bc <- get_meta_bias(res, "RM-AN")
CC_c <-  get_meta_bias(res, "LME")
         
```

Next, we estimate the _inflation bias_ that is incurred by using a given $N$. Here we focus on the MT and CC paradigms, as they contained effects where the null was not consistently rejected at $N_{med}$. For the MT task, the task condition x modality inflation bias for $N_{med}$ was `r sprintf("%.2f", MT_tm[paste(MT_med),])` $\epsilon_p^2$. No inflation bias was present for the main effect of task condition (all $N$ = 0). For the CC, the block x condition interaction inflation bias at $N_{med}$ was `r sprintf("%.2f", CC_bc[paste(CC_med),])` $\epsilon_p^2$, for the main effect of condition the $N_{med}$ inflation bias was nominal (`r sprintf("%.3f", CC_c[paste(CC_med),])` $\epsilon_p^2$). These and the remaining inflation bias estimates are presented in Figure \@ref(fig:infbias). 

```{r, infbias, out.width='70%', fig.cap='Inflation bias scores plotted by N for the A) the task condition and task condition x modality interactions for the MT paradigm, and B) the block x condition interaction and main effect of condition from the CC paradigm. IB: Implicit Bias, tc: task condition, tc*m: task condition x modality, b*c: block x condition interaction, c: main effect of condition. Error bars reflect pooled standard error of the difference.'}

infbias.fx.fig.pth <- '../images/EPS_meta_mu_CCSD.pdf'
knitr::include_graphics(infbias.fx.fig.pth)

```

### Predicting error in effect size estimates

```{r, err_pred, warning=FALSE}

# get the data
load("../data/predicted_error_by_task.RData")

get_cv_dat <- function(dat){
  out <- data.frame(mu=dat$cv$results$Rsquared,
                    sd=dat$cv$results$RsquaredSD)
}
CV_vars <- do.call(rbind, lapply(names(predictions_all_tasks),
                                 function(x) get_cv_dat(predictions_all_tasks[[x]]))) 
rownames(CV_vars) <- names(predictions_all_tasks)

# get the fstat details for the cc int
ccintstats <-  summary(predictions_all_tasks[["CC_int"]]$full)

# get the adjusted R^2 for the skew regressions
get_adj_R <- function(dat){
  tmp = summary(dat)
  tmp$adj.r.squared
}
skewR2 <- do.call(rbind, lapply(c("AB", "SRT", "SD_ME", "SD_int"),
                                function(x) get_adj_R(predictions_all_tasks[[x]]$skew)))
rownames(skewR2) <- c("AB", "SRT", "SD_ME", "SD_int")

kurR2 <- do.call(rbind, lapply(c("AB", "SRT", "SD_ME", "SD_int"),
                                function(x) get_adj_R(predictions_all_tasks[[x]]$k)))
rownames(kurR2) <- c("AB", "SRT", "SD_ME", "SD_int")

muskewR2 <- do.call(rbind, lapply(names(predictions_all_tasks), 
                                        function(x) get_adj_R(predictions_all_tasks[[x]]$mu_skew)))
rownames(muskewR2) <- names(predictions_all_tasks)

# function to get problematic skew values
Xskews <- do.call(rbind, lapply(c("AB", "SRT", "SD_ME"), 
                      function(x) predictions_all_tasks[[x]]$X))

```

\textcolor{blue}{Last, we determined which aspects of the data were predictive of erroneous effect size estimates. Multiple-regression analysis showed that between ~9-40}% \textcolor{blue}{of the variance in effect size errors were predicted by effect skewness, effect kurtosis, mean intra-individual variance, mean intra-individual skewness, and mean intra-individual kurtosis (*M*} $r_{cv}^2$s (*SD*): AB: `r sprintf("%.2f", CV_vars["AB", "mu"])` (`r sprintf("%.2f", CV_vars["AB", "sd"])`), MT main effect of task: `r sprintf("%.2f", CV_vars["SD_ME", "mu"])`, (`r sprintf("%.2f", CV_vars["SD_ME", "sd"])`), MT task x modality interaction: `r sprintf("%.2f", CV_vars["SD_int", "mu"])` (`r sprintf("%.2f", CV_vars["SD_int", "sd"])`), SRT: `r sprintf("%.2f", CV_vars["SRT", "mu"])` (`r sprintf("%.2f", CV_vars["SRT", "sd"])`), CC main effect of condition: `r sprintf("%.2f", CV_vars["CC_ME", "mu"])` (`r sprintf("%.2f", CV_vars["CC_ME", "sd"])` \textcolor{blue}{), all model ps < .001), apart from for the block x condition effect from the CC task, where the model accounted for a negligible proportion (~1}% \textcolor{blue}{) of effect size error (F(} `r sprintf("%d", ccintstats$fstatistic["numdf"])`,`r sprintf("%d", ccintstats$fstatistic["dendf"])` \textcolor{blue}{) =} `r sprintf("%.2f", ccintstats$fstatistic["value"])` \textcolor{blue}{, p = .04). This suggests that both inter- and intra-individual skewness and kurtosis predict variability in effect size errors, apart from when effect sizes are very small, such as is the case the CC block x condition interaction (} $\epsilon_{p}^2$ ~ `r sprintf("%.2f", CC_fx_stats_int[["best_LB"]])` - `r sprintf("%.2f", CC_fx_stats_int[["best_UB"]])` \textcolor{blue}{). We therefore do not include this latter effect in the subsequent analysis}. 

\textcolor{blue}{The resulting regression equations (see appendix ii) are useful for researchers using the tasks studied here, who wish to predict the extent to which their own experiment may have yielded an imprecise effect size estimate. However, what is more widely useful is understanding which regressors significantly predict effect size imprecision across tasks. We therefore determined which regressors showed significant predictive power across tasks, applying Bonferronni correction for multiple comparisons. For the AB, MT, and SRT tasks, effect skewness and kurtosis were significant predictors of effect size error (all ps <= .005, see appendix ii). Mean intra-individual skew was a significant predictor across all four tasks (all ps < .008), apart from for the MT task x condition interaction (p=.08).}

\textcolor{blue}{Having identified the regressors that suggest imprecision in effect size estimates across tasks, we next sought to determine which predictors could be used as a marker of imprecision when a researcher is unable to hold the other predictors constant. Such a finding would suggest that use of a single piece of information (e.g. effect skewness) could act as a marker for whether a single experiment has yielded an imprecise effect size estimate. Simple regressions between each predictor and effect size errors showed that effect skewness tended to predict a higher proportion of the variance (Adjusted} $R^2$ \textcolor{blue}{s:} `r sprintf("%.2f", skewR2["SRT", 1])` - `r sprintf("%.2f", skewR2["AB", 1])`, all ps < .001) \textcolor{blue}{than kurtosis (Adjusted} $R^2$: `r sprintf("%.2f", kurR2["SD_ME", 1])` - `r sprintf("%.2f", kurR2["SRT", 1])`, all ps < .7), \textcolor{blue}{apart from for the MT condition x task interaction (skewness: Adjusted} $R^2$ \textcolor{blue}{:} `r sprintf("%.4f", skewR2["SD_int", 1])` \textcolor{blue}{p<.01, kurtosis: Adjusted} $R^2$ \textcolor{blue}{:} `r sprintf("%.2f", kurR2["SD_int", 1])` \textcolor{blue}{p<.001). Although mean within-participant skewness predicted higher amounts of error variance for the AB (Adjusted} $R^2$: `r sprintf("%.2f", muskewR2["AB", 1])` \textcolor{blue}{) and CC main effect of condition (Adjusted} $R^2$: `r sprintf("%.2f", muskewR2["CC_ME", 1])` \textcolor{blue}{), its predictive power was poor for the remaining tasks (Adjusted} $R^2$ \textcolor{blue}{s: <= .02, all ps < .17). This suggests that effect skewness is the best potential general proxy of effect size imprecision, when not controlling for other influences.}

\textcolor{blue}{As effect skewness is the best candidate for predicting variance in effect size error across tasks, we next determined which values of effect skewness predict problematic levels of effect size error (defined as values falling outside the .025 and .075 quantiles for} $N_{313}$\textcolor{blue}{). Across tasks, moderate to large negative effect skewness (}`r sprintf("%.2f", max(Xskews[,"97.5%"]))` - `r sprintf("%.2f", min(Xskews[,"97.5%"]))`\textcolor{blue}{) predicted erroneous over-estimates of effect size, whereas large positive effect skewness (}`r sprintf("%.2f", min(Xskews[,"2.5%"]))` - `r sprintf("%.2f", max(Xskews[,"2.5%"]))`\textcolor{blue}{) predicted erroneous under-estimates. Thus, if data from a single experiment shows moderate to large values of effect skewness, caution in interpreting effect size estimates may be warranted.}

# Discussion
\label{sec:Discussion}

We simulated 1000 bootstrapped experiments across 20 $N$s ranging from 13 to 313. For each paradigm and from each set of simulations, we determined the impact of $N$ on error in effect size estimates. In doing so, we were able to quantify a range of effect sizes that researchers can consider when performing power analyses, particularly when using the AB, MT, SRT or CC paradigms. We determined precision loss in effect size estimates as a function of $N$ and found that decreasing $N_{max}$ to $N_{med}$ inflated the range of effect sizes by factors ranging between 1.78-4.16. We also computed the probability of attaining an accurate effect size estimate (defined as falling between the .025 and .975 quantiles of $N_{max}$), and found that sampling a single study would result in a reasonable estimate on between 40-67% of samples. Last we computed the inflation bias for effects that carried less than 90% power at $N_{med}$. We found that inflation biases ranged from a nominal to small effect ($\epsilon_{p}^2$: -.003-.03). These findings can inform study planning, study interpretation and theory development. 

### Study Planning
Our findings have practical relevance for study planning. A researcher planning a study using the Attentional Blink, who only has resources to test 50 participants, can now _a priori_ determine that they have 100% power to reject the null hypothesis. They can also determine that their observed effect size may be inflated by a factor of `r sprintf("%.2f", AB_fx_stats[["qq_ratio"]]["50",])`, and that their effect size estimate will be comparable to a study with several hundred people `r sprintf("%.0f", phit_dat$p_upper[phit_dat$mod == "AB" & phit_dat$n == 50]*100)`% of the time. Thus, the researcher can move to designing studies that produce an effect size estimate that they believe is sufficiently accurate to be a useful contribution to the field. They are also able to identify points of diminishing returns, beyond which testing extra participants may produce incremental gains. For example, by examining the relationship between the qq-ratio and $N$, they can determine the point at which they believe the cost in resources outweighs the benefits of precision gain. The information presented above allows such informed decision-making to be conducted for the AB, MT, SRT and CC tasks.

These findings complement the insights offered by previous simulation studies into the factors influencing effect size estimates. Previous simulation work has highlighted conditions that cause bias in effect size estimates [e.g. @laneEstimatingEffectSize1978; @troncososkidmoreBiasPrecisionClassical2013; @okadaOmegaSquaredLess2013; @gelmanPowerCalculationsAssessing2014] and the consequences for power calculations [@albersWhenPowerAnalyses2018; @andersonSampleSizePlanningMore2017], by generating data-sets under simplifying conditions such as using between subjects designs or using lower and fewer samples of $N$. Collectively, these studies have determined which effect size measures provide unbiased estimates (e.g. $\epsilon_{p}^2$ vs $\eta_{p}^2$), that effect size estimates are likely to be inflated due to publication bias and low statistical power, and that the process of study design should account for uncertainty in the magnitude and direction of anticipated effect sizes. However, it can be challenging to determine the uncertainty around effect size estimates and the impact of differing $N$ on that uncertainty without quantifications of the expected effect size, and the variability around that effect size, for a given field of study. By taking the current step away from simplifying data generating conditions, and instead simulating experiments based on data from specific paradigms with more complex designs, we provide insight into the uncertainty regarding effect size estimates for ecologically valid data taken from the AB, MT, SRT and CC paradigms. 

### Study Interpretation
Our findings also offer insight into the interpretation of existing studies using the AB, MT, SRT and CC paradigms. Researchers evaluating existing studies can use the current findings to estimate the potential imprecision of a given effect size, and can accordingly weight their belief in consequent theoretical assertions. The current findings also enable (largely positive) evaluations of the broader literature for each paradigm. Statistical power was largely very strong, apart from for interactions, which involved small or medium effects. This suggests that the published literature will likely cumulatively reflect a reasonable effect size estimate, across all $N$, when the effect under study is a main effect. However, for interaction effects (for which we only saw very small to medium effect sizes [$\epsilon_{p}^2$: .02-.17]), we consistently found that ~82 participants were required to achieve > 90% power, which was far above the $N_{med}$ for each paradigm. It follows that interactions would be relatively under-powered since data is being divided into more bins, and this accords with other observations that current practices result in low statistical power for interaction effects [e.g. @lakensSimulationBasedPowerAnalysis2021]. However, our survey of the field suggests that investigation of interaction effects with low $N$ remains common practice when measuring attention, executive function and implicit learning. The current findings demonstrate that cumulative approaches would be hampered by current practices in characterizing interaction effects (at least in the case of MT and CC).

We believe these findings offer new insights when considering what constitutes a well powered study for investigations into attention, executive function and implicit learning. The current findings show that achieving statistical power to reject the null hypothesis is either trivially easy, or, in the case of very small effects (as we observed for CC b x c), is inevitable with sufficient $N$. Therefore, demonstrating rejection of the null hypothesis has relatively little to offer if the goal is to develop theory and leverage insights from cumulative science [@gelmanPowerCalculationsAssessing2014; @cummingNewStatisticsWhy2014; @chenHandlingMultiplicityNeuroimaging2019; @lorca-pulsImpactSampleSize2018]. Here we show that if a given field can pool data, or collectively provide the appropriate simulation parameters, then it is possible to plan research studies with the aim of producing an effect size estimate that has an acceptable level of precision. Of course, there are no pre-defined rules regarding what is a tolerable level of precision. This is something that may need to be defined on a case by case basis. 

Just as knowing about the distributional properties of effect sizes observed across many replications provides information about study design and interpretation, so too can considering the distributional qualities of observed p-values. The p-value is itself a random variable that will vary from experiment to experiment [e.g. @chenHandlingMultiplicityNeuroimaging2019], yet this variation is rarely considered when researchers report a single p-value for each reported effect. Understanding exactly how a p-value may vary across replications can help identify where there may be missing literature owing to publication bias, or uncertainty regarding the rejection of the null hypothesis [e.g. @nolanEvidenceDetectabilityHippocampal2018]. Moreover, although it is known that p-values are inversely related to effect size, the relationship is both non-linear and non-trivial to compute as it depends on other factors such as the sample size, the underlying data type (e.g. independent vs dependent) and the statistical test [@faulPowerFlexibleStatistical2007]. The current simulation approach could also be employed to better map the relationship between $N$ and p-values, for varying effects. This can yield insights into uncertainty over p-values and assist with interpretation of research findings. We provide the p-value data from the current simulations as Supplemental figures ^[See https://github.com/kel-github/Super-Effects/tree/master/doc/supp-figs] to help with this endeavor. 

### Theory Development
The current simulation approach can also inform theory development. In the case of implicit learning, our results showed that for the CC paradigm, the block x condition interaction effect was very small ($\epsilon_{p}^2$: .01-.04). This may be because the effect is very small across all variations of the paradigm, or that the current design parameters may not effectively measure the effect. The current paradigm was modeled on the seminal demonstration [@chunContextualCueingImplicit1998]. Nonetheless, there may be critical design parameters that with modification, elicit a larger (and more positive) range of interaction effects. Applying the current simulation approach to data collected across varying implementations of the CC paradigm can yield insights into what produces the effect, and consequently can help refine theory regarding the causes of the effect.

The current approach of using a large data-set also offers insight into the impact of increasing individual variation while holding measurement error relatively constant, for each paradigm under study here. Hopefully, at $N_{313}$ the contribution of individual variation is relatively low compared to the measurement error. Given this, the currently observed comparable rates of change for the qq-ratio and p(hit|$N$) values across paradigms may be unsurprising. This consistency may be of some value when quantifying the impact of individual variation on predicted effect magnitudes. Furthermore, the range of effect sizes observed for experiments at $N_{313}$ provides an estimate of measurement error that could be built into quantitative predictions for the AB, MT, SRT and CC effects.

### Limitations
It remains an open question whether the current findings generalize beyond the paradigms and participant pool used here. There are some suggestions of generalizability of the current observations \textcolor{blue}{across tasks} that should be investigated in future research. Across all the $\epsilon_{p}^2$ findings, the standard deviations at $N_{313}$ were small (*SD*s: .01-.03), and each *SD* doubled or tripled as a function of moving from $N_{313}$ to $N_{med}$. Therefore, it is possible that effect sizes such as $\epsilon_{p}^2$ will show a comparable reduction in variability as $N$ increases to the hundreds, across all paradigms. If this were found to be true, then researchers could apply the rates of change observed here to effect size estimates from their own field of study in order to determine the N required to achieve a tolerable level of precision. Moreover, changes in p(hit|$N$) and qq-ratio rates were comparable across $N$ for all effects, regardless of size, suggesting invariance to the measurement differences across paradigms. Future research should determine the extent to which these rates were dependent upon the current sample of $N_{313}$, which was arguably homogeneous with regard to population characteristics. \textcolor{blue}{Indeed, it is pertinent to determine the extent to which our results would hold with more heterogeneous samples. For example, estimates of effect sizes may be more variable under less constrained conditions, such as when community sample participants complete online studies. Future work should determine the extent to which study design choices may hamper precise effect size estimates in such groups.}

A further limitation is that the p(hit|$N$) and qq-ratio values were dependent on the range of effect sizes observed at $N_{313}$. The results may be different if we had sampled $N_{1000}$ (for example). Thus interpretation of the current findings is dependent on how willing the researcher is to assume that several hundred participants is a sufficient representation of 'as good as it gets'. Given the small ranges of effect sizes observed for $N_{313}$, we certainly think this is a reasonable place to start. 

## Conclusions
By simulating experiments across varying $N$ for popular paradigms from the study of attention, executive function and implicit learning, we are able to provide insights into the precision of effect size estimates that are unknowable from simulation approaches that make simplifying assumptions regarding the data. Using the current approach, we can identify the mean effect size and the variability of that effect size, under the best case scenario. This allows us to quantify the change in precision of effect size estimates with varying $N$. We identify that using a typical $N$ can double imprecision of effect size estimates, and characterize to what extent this reduces the chances that a single study will provide a reasonable effect size estimate. In the case of the small effect sizes observed here, inflation bias can amount to the equivalent of a small effect size. Amassing large data-sets to allow characterisation of error in effect size estimates is a useful exercise when seeking to plan studies that facilitate cumulative science.


\clearpage

# References
\label{sec:Refref}

