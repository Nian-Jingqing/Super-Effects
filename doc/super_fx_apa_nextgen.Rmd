---
title             : "Quantifying error in effect size estimates in attention, executive function and implicit learning"
shorttitle        : "Quantifying error in effect size estimates"

author: 
  - name          : "*Kelly G. Garner"
    affiliation   : "1,3"
    corresponding : yes    # Define only one corresponding author
    email         : "getkellygarner@gmail.com"
  - name          : "Christopher R. Nolan"
    affiliation   : "2"
  - name          : "Abbey Nydam"
    affiliation   : "3"
  - name          : "Zoie Nott"
    affiliation   : "3"
  - name          : "Howard Bowman"
    affiliation   : "1"
  - name          : "Paul E. Dux"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "School of Psychology, University of Birmingham, UK"
  - id            : "2"
    institution   : "School of Psychology, University of New South Wales, Australia"
  - id            : "3"
    institution   : "School of Psychology, The University of Queensland, Australia"
    
authornote: |
  *denotes corresponding author: getkellygarner@gmail.com
  
  This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 796329, awarded to Kelly Garner, and ARC Discovery Projects DP180101885 & DP210101977 awarded to Paul Dux.

keywords          : "effect size, statistical power, executive function, implicit learning, attentional blink, multitasking, contextual cueing, serial response task"
wordcount         : "8873"

bibliography      : ["supfx_refs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

Data: https://doi.org/10.48610/1e6bf9a Garner & Nolan [-@garnerQuantifyingErrorEffect2022a] <p>
Code: https://github.com/kel-github/Super-Effects Garner, Knott & Nolan [-@garnerQuantifyingErrorEffect2022] <p>

\clearpage

# Abstract
THIS NEEDS TO BE RE-WRITTEN. An accurate quantification of effect sizes has the power to motivate theory, and reduce misinvestment in scientific resources by informing power calculations during study planning. Such a quantification could theoretically be achieved by a meta-analysis. However, a combination of publication bias and small sample sizes (~$N$ = 25) hampers certainty that such an analysis would yield a non-erroneous estimate. We sought to determine the extent to which each of these caveats may produce error in effect size estimates for 4 commonly used paradigms assessing attention, executive function and implicit learning (attentional blink (AB), multitasking (MT), contextual cueing (CC), serial response task (SRT)). We combined a large dataset with a bootstrapping approach to simulate 1000 experiments across a range of $N$ (13-313). Beyond quantifying the effect size and statistical power that can be anticipated for each study design, we demonstrate that experiments with lower values of N can lead to problematic information loss, potentially biasing power calculations. Furthermore, we show that for the CC and SRT, a meta-analysis of experiments with lower $N$ is unlikely to ever converge on the true effect size, owing to underspecification of the mapping between theory and statistical model. We conclude with practical recommendations for researchers and demonstrate how our simulation approach can yield theoretical insights that are not readily achieved by other methods; such as identifying when qualitative individual differences exist in response to an experimental manipulation.

```{r knitr_options, echo=FALSE}
library(knitr)
# rstudio will set the folder where .Rmd file seats as work directory
# set it back to the folder where .Rproj seats
#opts_knit$set(root.dir = normalizePath("../")) 
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# knit_hooks$set(inline = function(x) {
#   x <- sprintf("%1.2f", x)
#   paste(x, collapse = ", ")
# })
```

```{r loadpackagesandfunctions, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(wesanderson)
library(ggridges)
library(cowplot)
library(rstatix)
library(emmeans)
library(ggridges)
library(car)
library(parallel)
library(kableExtra)
source('../R/efilids_functions.R')
source('../R/R_rainclouds.R')
source('../R/doc_functions.R') # some specific plotting functions for the document
source('../R/plotting.R') # this has some stats functions we want to use in the document
source('../R/do_infer_stats.R') # extra functions for inferential stats
#source('../R/KL_linear_piecewise.R')

```

\clearpage

AIMS:

1. To identify the consequence of sample size on effect size estimates; i.e. how much error there may be in effect size estimates  (fx size figure and probability curves)
2. To use the simulation approach to identify circumstances where there may be missing information about effect sizes, which could potentially hamper sample-size computations based on incomplete information.
3. To determine across tasks, if there is a way to identify an optimal N for common EF and IL tasks.
4. To identify commonalities, or rules of thumb by comparing across tasks.
4. To identify the consequence of sample size on p-value estimates; i.e. can knowing about the relationship between N and variation in p-value estimates further inform inference for null hypothesis testing?
5. A happy consequence of this is that in so-doing, we can provide a well informed estimate for the specific tasks, to anchor future experiments

# Introduction

Despite the complexity involved in disentangling the circuits that underpin cognition, decision making regarding experimental outcomes is often made on binary (i.e. pass or fail) terms, across the psychological, neuroscientific and biomedical sciences [@szucsWhenNullHypothesis2017]. Theoretical predictions are often specified in terms of the presence or absence of a given effect, and a yes/no decision is made about whether the null hypothesis (usually a hypothesis of null differences) can be rejected. Considering the complexity of the human brain, it seems unlikely that such pass/fail decision-making will be sufficient to disentangle the myriad functional systems that the brain has developed over millions of years of evolution. An alternate approach is to develop theory and models that attempt to quantify the magnitude of the effect, i.e. the anticipated effect size; a standardised measure that reflects the extent to which a given effect, such as a mean difference between two conditions, is expected to generalise to the population [@cohenStatisticalPowerAnalysis1988].

A prediction of effect magnitude is easier to disprove than presence/absence, and therefore constitutes a more desirable prediction for theory testing [@popperLogicScientificDiscovery1959]. To move towards theories that predict changes in effect size magnitude, it is essential that we gain an understanding of exactly how much insight is yielded from our current effect size estimates; i.e. how well are we currently quantifying effect sizes, and should we increase current sample sizes in order to quantify them better? Indeed, recent work suggests that studies conducted with insufficient statistical power are at increased risk of producing effect size estimates that are either inflated in magnitude, or are in the incorrect direction [@gelmanPowerCalulcationsAssessing2014; @chenHandlingMultiplicityNeuroimaging2019]. Here we seek to address how well we are currently characterizing effect sizes, and how well we could hope to characterise them, in three well studied disciplines in cognitive psychology; attention, executive function and implicit learning. 

Accurate quantification of effect sizes is also desirable for study planning, as effect sizes form the foundation of _a priori_ power calculations [@cohenStatisticalPowerAnalysis1988]; where the researcher determines the sample size $N$ required to achieve sufficient power to correctly reject the null hypothesis. The importance - and difficulty - of accurately determining the anticipated effect size has been considered extensively elsewhere [@cohenStatisticalPowerAnalysis1988; @gelmanPowerCalculationsAssessing2014;, @eggerBiasMetaanalysisDetected1997; @lakensCalculatingReportingEffect2013; @guoSelectingSampleSize2013; @westfallStatisticalPowerOptimal2014; @szucsWhenNullHypothesis2017; @albersWhenPowerAnalyses2018; @cummingNewStatisticsWhy2014]. What remains somewhat less considered is the utility of knowing how variable anticipated effect size estimates may be across replications of an experiment [e.g. @cummingNewStatisticsWhy2014], i.e. what are the distributional qualities of the effect size across experiments, given a field typically uses a restricted range of $N$? We also consider here the utility of characterising how the distributional qualities of effect size estimates would change if the field were to use larger $N$'s as standard; specifically, how much more precision is to be gained in the study of attention, executive function and implicit learning by increasing our sample sizes? 

The answers to these questions can facilitate both study planning and theory development. For example, a theory that predicts a small effect, that manifests with low variability across replications may be considered more informed than theory predicting an effect that has the same mean but that also varies widely (and unexpectedly) across replications; with the latter indicating the presence of poorly mapped sources of individual variance and observation error. With regard to study planning; identifying the lower bound of an expected effect size facilitates computation of the $N$ required to achieve sufficient statistical power under the worst case scenario, and allows the researcher to compute the probability that their experiment will produce a comparable result [@gelmanPowerCalculationsAssessing2014]. Furthermore, knowing how effect sizes vary across replications with a given $N$ allows computation of the likelihood that any single study has produced a reasonably accurate estimate, which can inform the researcher who may be computing anticipated effect sizes on the basis of one or a few similar studies. There is also utility in knowing to what extent variability in effect size observations reduces when larger $N$ are used instead. There may be an upper bound on the accuracy with which a particular effect can be estimated. Consequently, there may be a point of diminishing returns, where the cost of recruiting extra $N$ will outweigh the gains in accuracy of effect size estimation. 

Quantifying the range of effect sizes that one may expect to observe across experimental replications is not trivial. Indeed, it has been noted that the largest challenge in experimental design calculations is the prior identification of a plausible range of effect sizes that one may expect to observe [@gelmanPowerCalculationsAssessing2014]. A few options are available to the researcher seeking to identify a range of anticipated effect sizes for a given experiment; the researcher can identify the minimal effect size that is deemed to be large enough to be of theoretical interest, but this is challenging without a detailed model of the data generating process. Alternatively, the researcher can be guided by the results of an appropriate meta-analysis. Such a result may be artificially inflated; smaller effect sizes from non-statistically significant results are less likely to be available in the published literature [@mcshaneAdjustingPublicationBias2016]. Indeed, a recent survey of 900 effect sizes across psychology disciplines showed that effects from non-pre-registered studies were much larger than pre-registered studies ($r$ = 0.36 vs 0.16, @schaferMeaningfulnessEffectSizes2019) suggesting that prior to pre-registration, under-powered studies were contributing inflated effect size estimates to the field - although well-developed correction methods can address this issue in meta-analytic analysis under certain circumstances [e.g. @duBayesianFillinMethod2017; @bartosAdjustingPublicationBias2022]. 

Often, an appropriate meta-analysis is not available. In this case, the researcher could also sample some similar studies from the literature for relevant effect sizes and their confidence intervals. Incompletely sampling the literature for similar studies is also likely to result in bias [@andersonSampleSizePlanningMore2017]; the problem of publication processes favouring inflated effect size estimates [@laneEstimatingEffectSize1978; @brandAccuracyEffectSize2008; @gelmanPowerCalculationsAssessing2014; @fristonTenIronicRules2012; @lorca-pulsImpactSampleSize2018] is exacerbated by the unknown precision of any given study. Effect size estimates based on incomplete information, i.e. based on only a few similar samples, are more susceptible to noise. Incomplete sampling approaches, as well as meta-analytic approaches, are limited by the quality of the existing literature. It is difficult to directly determine how conclusions about effect sizes would differ if a given field of study was different, i.e. what is the probability that incomplete sampling would lead the researcher astray if a larger $N$ was used as standard?

Simulation studies offer the opportunity to ask how well a field is currently quantifying effect sizes, and how a field's estimate of an effect size would differ across differing levels of statistical power. Typically, simulation studies examining the distributional qualities of effect size measures generate data under some simplifying assumptions about the data generation process [e.g. @albersWhenPowerAnalyses2018; @troncososkidmoreBiasPrecisionClassical2013; @laneEstimatingEffectSize1978; @hedgesEstimationEffectSize1982; @westfallStatisticalPowerOptimal2014]. Although such work is necessary for informing how effect size estimates behave under varying conditions where ground truth is known, it is challenging to anticipate all the complexities of data collected in the repeated-measures designs often used in the study of attention, executive function and implicit learning. For example, data are often not normally distributed, with varying levels of covariance between conditions; thus there remains a question mark over the extent to which the results from simulation work generalises to real-world data. An alternative method is to simulation experimental outcomes by bootstrapping smaller samples from larger, real datasets [e.g. @lorca-pulsImpactSampleSize2018]. This approach offers the opportunity to investigate the distributional qualities of effect sizes estimated from high-dimensional datasets, using varying levels of $N$, while maintaining ecological validity.

WHY COMPARING THESE TASKS

IN AIMS: ADD CONSEQUENTLY WE PROVIDE A GOOD ESTIMATE OF EFFECT SIZE FOR THE FIELD TO USE IN POWER CALCULATIONS

Here, we address these questions by simulating 1000s of experiments using a large dataset ($N$ = 313), where participants completed a battery of tasks well known to the cognitive psychology literature; namely the attentional blink (AB, @raymondTemporarySuppressionVisual1992), a multitasking paradigm (MT, @schumacherVirtuallyPerfectTime2001), a contextual cueing task (CC, @chunContextualCueingImplicit1998) and the serial reaction time task (SRT, @nissenAttentionalRequirementsLearning1987).

In the current study, we applied a comparable approach using an existing behavioural dataset. Participants ($N$ = 313) completed a battery of cognitive tasks originally assembled to test the relationship between attention, executive function and implicit learning. For each paradigm, we simulated 1000 bootstrapped experiments across 20 $N$s ranging from 13 to 313. For each paradigm and from each set of simulations, we sought to provide an estimate of the most likely effect size (using $N_{313}$) and quantified how much the best estimate would differ if it was derived from 1000 experiments using the $N$ that is typical for the field. We next determined the extent to which information was lost by approximating effect sizes with lower values of $N$, and whether any inflation in estimates occurred as a consequence of basing calculations on only significant findings. We show that for the AB and MT, use of the currently published literature is likely to produce accurate quantification of effect sizes. In contrast, effect sizes for CC are small and would only be quantified accurately in meta-analysis if the field uses an $N$ much higher than what is typical. Examination of distributions of effect sizes for the SRT shows that individuals cluster around either small or large effect sizes, thus informing theory. We propose a potential diagnostic for determining whether any field is likely to suffer from critical information loss when consisting of studies with lower $N$. Last, we conclude with practical recommendations for researchers planning cognitive experiments and outline implications for current mappings of theory to statistical models.

Just as knowing about the distributional qualities of effect sizes observed across many replications provides information about study design and interpretation, so too can considering the distributional qualities of observed p-values. The p-value is itself a random variable that will vary from experiment to experiment [e.g. @chenHandlingMultiplicityNeuroimaging2019], yet this variation is rarely considered when researchers report a single p-value for each reported effect. Understanding exactly how a p-value may vary across replications can help identify where there may be missing literature owing to publication bias, or uncertainty regarding the rejection of the null hypothesis [e.g. @nolanEvidenceDetectabilityHippocampal2018]. Moreover, although it is known that p-values are inversely and negatively related to effect size, the relationship is both non-linear and non-trivial to compute as it depends on other factors such as the sample size, the underlying data type (e.g. independent vs dependent) and the statistical test [@faulPowerFlexibleStatistical2007]. Here we consider what mapping the variability in p-values across experiment replications with differing $N$ can reveal about potential sources [POWER]


# Methods

\label{sec:Method}


## Participants
\label{sec:Participants}

The current study reused a data set collected for a different [pre-registered](https://osf.io/nxysg) project examining the relationship between executive function and implicit learning. This data set contains performance measures from $N$ = 313 participants. Participants were undergraduate students, aged 18 to 35 years old (mean = 20.14 yrs, sd = 3.46). Of the total sample, 208 reported being female, and 269 reported being right handed. Participants received course credits as compensation. All procedures were approved by The University of Queensland Human Research Ethics Committee and adhered to the [National Statement on Ethical Conduct in Human Research](https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018).

## Apparatus
\label{sec:Apparatus}

Experimental procedures were run on an Apple Mac Minicomputer (OS X Late 2014, 2.8 GHz Intel Core i5) with custom code using the Psychophysics toolbox (v3.0.14) [@brainardPsychophysicsToolbox1997; @pelliVideoToolboxSoftwareVisual1997] in Matlab v2015b. Participants completed 7 tasks; Attentional Blink (AB), Multitasking (MT), Contextual Cueing (CC), Serial Response Task (SRT), Visual Statistical Learning (VSL), Operation Span task and a Stop Signal Inhibition task. Only the data from the AB, MT, CC and SRT are reported here. We opted not to report the VSL, OSPAN or Stop Signal data as their design did not lend themselves to the computation of a standardised effect size. 

## Procedures
\label{sec:Procedures}

Across all tasks, participants sat approximately 57 cm from the monitor. An overview of the task procedures is presented in Figure \@ref(fig:FigureParadigm). Details regarding each of the task protocols are presented within each section below. We first provide an overview of the experimental tasks, before detailing the specific simulation and statistical methods.


```{r, FigureParadigm, out.width='70%', fig.cap='Task battery. A) Attentional Blink Paradigm (AB). Participants report the two letter targets from the rapid serial visual presentation of numbers and letters. B) Multitasking Paradigm (MT). Participants discriminate the colour of a disc, a complex tone, or both. C) Contextual Cueing Paradigm (CC). i) Participants perform an inefficient visual search task where they search for a rotated T among L distractors. ii) Unknown to participants, half of the search arrays are repeated throughout the course of the experiment. D) Serial reaction time task (SRT). Participants respond to one of four stimuli, each mapped to a spatially-compatible button press. Unknown to participants, for half of the experimental blocks, the stimulus follows a repeating sequence.'}

paradigm.fig.pth <- '../images/FigXXXX_alltasks.pdf'
knitr::include_graphics(paradigm.fig.pth)

```


### Attentional Blink (AB)
\label{sec:ABMeth}

The AB task taps limitations in the deployment of visual information processing over time. Participants are instructed to detect two targets from a rapidly presented series of visual items. Accuracy for the second target is poorer if it appears closer in time to the first target (at early lags, from lag 2 onwards), relative to further apart in time [@raymondTemporarySuppressionVisual1992]. 

#### Protocol

The AB protocol was the same as that reported in Bender et al [-@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. On each trial, letter targets and digit distractors were each presented centrally for 100 ms in rapid serial presentation. The eight distractors were drawn without replacement from the digits 2-9. The target letters were randomly selected from the English alphabet, excluding I, L, O, Q, U, V and X. The first target (T1) was the third item to be presented (serial position 3), and T2 was presented at either lag 2 (200 ms), 3 (300 ms), 5 (500 ms) or 7 (700 ms) relative to T1. All stimuli subtended 1.72 x 2.31 $^\circ$ (w x h) visual angle. Participants were instructed to make an unspeeded report of the identity of both targets at the end of each trial. Participants completed 24 practice trials and four test blocks of 24 trials. For the current analysis we calculated T2 accuracy, given that T1 was correctly reported (T2|T1), for each lag. 

### Multitasking (MT)
\label{sec:MTMeth}

MT paradigms tap the performance costs incurred when individuals attempt to perform more than one task concurrently. Participants are instructed to complete two simple sensorimotor tasks as accurately and quickly as possible under single or multitask conditions. RTs to the constituent tasks are typically slowed for multitask relative to single task conditions (see Pashler [-@pashlerDualtaskInterferenceSimple1994a], for a review).

#### Protocol

The MT protocol was previously reported in  Bender et al [-@benderRelationshipResponseSelection2016]. Each trial began with a black fixation cross presented in the center of a gray screen [RGB: 128, 128, 128] for a variable interval of 200-600 ms. Next either one of two coloured circles [red, RGB: 237, 32, 36 or blue, RGB: 44, 71, 151] or one of two sounds (complex tones taken from [@duxIsolationCentralBottleneck2006]), or both (circle and sound) were presented for 200 ms. The coloured circle subtended 1.3$^\circ$ visual angle. Participants were instructed to respond to all presented tasks by using the appropriate key press ['A' or 'S' for left hand responses, 'J' or 'K' for right hand responses, with the task-hand mapping counterbalanced across participants]. The MT protocol consisted of 4 blocks of 36 trials, with each trial type (single-task [ST] visual, ST auditory or MT) randomly mixed within blocks. Participants completed the MT protocols after completing two ST blocks as practice, one for the visual task and one for the auditory task. Mean response times (RTs) to each task modality x condition were taken as the dependent variable of interest.

### Contextual Cueing (CC)
\label{sec:CCMeth}

CC tasks tap how the visual system exploits statistical regularities to guide visual search (Sisk, Remington and Jiang, [-@siskMechanismsContextualCueing2019]; Jiang and Sisk [-@jiangContextualCueing2020]). Participants are typically asked to report the orientation of a rotated 'T' target presented among an array of distractor 'L's. Participants are not informed that a set of the displays are repeated throughout the course of the experiment, while the remaining displays are novel to each trial. Typically RTs to the repeat displays become faster than novel displays throughout the course of the experiment [e.g. @chunContextualCueingImplicit1998; @nydamCathodalElectricalStimulation2018]. Participants are typically poor at recognising repeat displays in a subsequent recognition test (Sisk, Remington and Jiang, [-@siskMechanismsContextualCueing2019]; Jiang and Sisk [-@jiangContextualCueing2020]), which has prompted the conclusion that CC reflects a process of implicit learning [but see @vadilloUnderpoweredSamplesFalse2016; @vadilloUnconsciousUnderpoweredProbabilistic2020; @vadilloRaisingAwarenessMeasurement2021].

#### Protocol

The CC protocol was the same as that reported by Nydam et al [-@nydamCathodalElectricalStimulation2018] which is modeled on Chun and Jiang [-@chunContextualCueingImplicit1998]. Each trial began with a white fixation cross presented on a grey screen [RGB: 80, 80, 80]. An array of 12 L's and a single T were then presented presented within an invisible 15 x 15 grid that subtended 10$^\circ$ x 10$^\circ$ of visual angle. Orientation of each L was determined randomly to be rotated 0$^\circ$, 90$^\circ$, 180$^\circ$ or 270$^\circ$ clockwise. The T was oriented to either 90$^\circ$ or 270$^\circ$. Participants reported whether the T was oriented to the left (using the 'z' key) or the right (using the 'm' key), as quickly and accurately as possible. The task consisted of 12 blocks of 24 trials. For half the trials in each block, the display was taken (without replacement) from 1 of 12 configurations that was uniquely generated for each participant, where the location of the distractors and target (but not the orientation of the target) was fixed. These trials were called 'repeats'. For the remaining trials, the display was randomly generated for each trial, making them 'novel'. Displays were generated with the constraint that equal items be placed in each quadrant and each eccentricity. Target positions were matched between the repeat and novel displays for both quadrant and eccentricity. The exact location of the item was jittered within each cell for each presentation, to prevent perceptual learning or adaptation to the specific position of the item. The order of display type (repeat vs novel), configuration (1:12) and target orientation (left or right) was randomised for each block. Mean RTs to each block (1:12) and display type (repeat vs novel) were taken as the dependent variable.

### Serial Response Task (SRT)
\label{sec:SRTMeth}

In the SRT task, participants are required to make key presses in response to one of four spatially compatible visual cues [@nissenAttentionalRequirementsLearning1987]. Participants are instructed to make the relevant key press as quickly and accurately as possible. Participants are not informed that for half of the experiment, visual cue presentation follows a repeating and reliable sequence, whereas for the remaining half of the experiment, cue presentation is random. RTs are typically faster for the final sequence block than for the final random block, even though the random block is presented subsequently to the sequence block. Comparable to the CC task, participant's lack of awareness of the sequence lends support to the notion that the SRT reflects an implicit process of procedural sequence learning.

#### Protocol
The SRT was adapted from Nissen & Bullemer [-@nissenAttentionalRequirementsLearning1987]. Four square placeholders were presented across the horizontal meridian. A red circle [RGB: 255, 0, 0] appeared in one of the 4 squares for 500 ms. This served as the target stimulus. Participants responded by pressing the finger of their dominant hand that spatially aligned to the target circle, using the relevant 'j', 'k', 'l' or ';' keys. The next target stimulus would appear 500 ms after the correct response had been made. Participants completed 4 blocks of 100 trials. For blocks 1 and 4, the location of the target stimulus for each trial was randomly selected from a uniform distribution. These blocks are referred to as 'random'. For blocks 2 and 3, a repeating sequence of 10 elements was used to determine the target location. The sequence was repeated 10 times. The repeating sequence was 4-2-3-1-3-2-4-2-3-1, with 1 being the leftmost placeholder, and 4 being the rightmost placeholder. These blocks are referred to as 'Repeat' blocks. Learning in the SRT is tested by comparing mean RTs between Random and Repeat blocks in the latter half of the experiment (block 4 vs 3).
<br>

## Statistical Approach

All the [data](https://doi.org/10.48610/1e6bf9a) and [code](https://github.com/kel-github/Super-Effects) used for the current analyses are available online. All data were analysed using R -@rcoreteamLanguageEnvironmentStatistical2015 and RStudio -@rstudiocitation. The analysis of the data from each task followed two steps; first, to ascertain that we observed the typical findings for each of the paradigms, we applied the relevant conventional statistical model to the full dataset ($N$=313). Next, we implemented a simulation procedure to determine the effect size and p-values that would be attained over many experiments conducted at multiple levels of sample size. 

### Sampling procedure
\label{sec:SamplingProc}

For each task, we simulated experiments across 20 different sample sizes ($N$s), defined on a logarithmic interval between N=13 and N=313 ($N$ = [13, 15, 18, 21, 25, 30, 36, 42, 50, 59, 69, 82, 97, 115, 136, 160, 189. 224, 265, 313]). We opted for a logarithmic interval given the decreasing information gained at higher $N$ values. To simulate $k$=1000 experiments at each of our chosen $N$, we sampled $N$ participants from $N_{max}$ over $k$ iterations. The relevant analysis was applied to each of the samples. Details regarding which analyses were applied to each $k$ sample are listed below for each paradigm. Sampling with replacement ensured that the samples carried the Markov property. One potential concern is that any reductions in observed effect size variability may be attributable to saturation as the simulated $N$ approaches the maximum ($N_{max}$=313), rather than a genuine reduction in variance of the estimate of the effect. Specifically, it could be that as $N$ approaches 313, the overlap of participants between samples is greater than when $N$ equals a lower number such as 13. It follows then that any decreasing variability in effect size estimates at higher $N$s could be due to the decrease in variability of the samples, rather than the improved estimate of the population variance that should come with a larger $N$. We have run simulations that argue against this explanation and these can be found in appendix i.

### Effect Sizes
For each task, we report the following information from the effect size densities that were produced by simulating 1000 experiments at each level of $N$; first we used our highest $N$ ($N_{313}$) to identify our best estimate of the effect size distribution. We therefore report, for each task, the median (M), standard deviation (SD) and the .025 (lower bound, LB) and .975 (upper bound, UB) quantiles. These values can be used to define, _a priori_, the range of anticipated effect sizes for future experiments, and consequently, can motivate power calculations for future studies. 

We next determined how effect size observations differ when experiments use an $N$ that is typical for the field. We report the same summary statistics as above, for the $N$ that is closest to the median sample size used in the field for that task ($N_{med}$). To identify the median $N$, we conducted a survey of the recent literature (see below). We computed _precision loss_ from using $N_{med}$ by taking the ratio between the UB or LB quantiles (e.g. $\frac{LB_{N_{med}} }{LB_{N_{313}}}$). We refer to these from now as the LB and UB ratios. We also present LB and UB ratios across all sampled $N$s, to provide an idea of how much may be gained in precision if the field were to increase the typical $N$. Note that although we expect these ratios to be some function of $\frac{1}{N}$ (given that variance is a function of this term), the exact relationship between $N$ and precision will be dependent on the population variance and measurement error. 

Next we sought to provide empirical estimates regarding how missing literature and imprecision from low $N$ may lead a researcher awry during study planning. We determine for each task and $N$, the probability that an observation from one experiment will fall within the UB and LB quantiles from $N_{313}$. (As above, although we expect this to change as a function of $\frac{1}{N}$, the exact relationship is dependent on measurement noise). This provides an idea of how likely a researcher is to have picked a reasonable effect size estimate when basing power calculations on a similar study, and the extent to which this confidence changes with $N$. We also provided an estimate of how effect size estimates might be biased by aggregating only across experiments with statistically significant results (p<.05), under the assumption that the published literature is more likely to contain significant results and is not likely to contain null results. Therefore, this estimate would reflect what is likely to be obtained from aggregating across the existing published literature. We present the difference between the mean observed effect size and the mean estimate of this _biased_ effect size, for each level of $N$. Effectively, this analysis is assessing the severity of the file-drawer effect for different sizes of $N$.

#### Computing Effect Sizes

To compute effect sizes for the tasks analysed using a repeated-measures ANOVA (AB, MT and CC tasks), we computed partial epsilon squared ($\epsilon_{p}^2$), as this measure is less subject to bias than $\eta_{p}^2$ [@okadaOmegaSquaredLess2013]. Indeed, an earlier version of our manuscript showed that $\eta_{p}$ estimates are biased on average, even for sample sizes of $N$=313, see [] for Supplemental Figures documenting the analysis ^[Note: we thank a helpful reviewer for drawing our attention to this]). We use the formula for $\epsilon_{p}^2$ as defined in [@carrollSamplingCharacteristicsKelley1975, eq 11]:

$$
\epsilon_{p}^{2} = \frac{F-1}{F + \frac{df_w}{df_b}}
$$
For the remaining task where a paired samples t-test is used to make the key comparison (SRT), we compute Cohen's $d_{z}$ (see @lakensCalculatingReportingEffect2013, eq 6):

$$
d_{z} = \frac{M_{diff}}{\sqrt{\frac{\sum(X_{diff} - M_{diff})^2}{N-1}}}
$$

#### Representative N

To attain an $N$ that reflects what is commonly used for each task, we surveyed the three most relevant _Journal of Experimental Psychology_ journals (_General_, _Human Perception & Performance_ and _Learning, Memory & Cognition_) for all articles mentioning the use of any of the current tasks. We searched back for a total of 60 experiments or back from today to 2005, whichever occurred first. We then computed the median sample size used across all experiments found from the survey. The results from the survey are presented in Table 1.

```{r, survey, message = FALSE, warning = FALSE}

survey_dat <- data.frame(task = c("AB", "MT", "CC", "SRT"),
                         e = c(60, 60, 49, 60),
                         med = c(24, 40, 24, 34))
knitr::kable(survey_dat, col.names = c("task", "n exp", "med N"), align = "lcc", digits = c(0, 0, 0, 0), "simple", caption = "Typical N found from literature survey. n exp = number or experiments, med N = median N")
```

### p-values

To determine the $N$ required to achieve 90% power to reject the null hypothesis, we report the $N$ for which over 90% of p-values pass the threshold for significance for the effects of interest ($\alpha$=.05). To assess how confidence in p-values, or certainty of the test outcome, changes with $N$, we present the same statistics for each $N$ (mean, standard deviation, UB, LB and UB/LB ratios). As p-values clustered close to 0 in many instances, this measure will be subject to some floor effects but should also determine where clear information gains are available by increasing $N$. Additionally, because p-values cluster close to 0, we applied the probit transform to rescale the values on the range [-$\infty$, $\infty$] for visualisation purposes only. 

### Analysis of Experimental Tasks

#### Attentional Blink

As is typical for the field, and to ascertain the effectiveness of the lag manipulation, T2|T1 accuracy was subject to a repeated measures ANOVA, with lag (2, 3, 5, & 7) as the independent variable. This analysis was also applied to each $k$ sample. For each $k$ sample, $\epsilon_{p}^2$ and the resulting $p$ value were taken for the main effect of lag. For this task, and all remaining ANOVA tests, models were fit using the anova_test() function from the [rstatix](https://rpkgs.datanovia.com/rstatix/index.html) package. Where possible, the models were fit using type 3 sum of squares, owing to the computational expediency and match to commercial statistical software packages. In some cases, models were unable to be fit using type 3 sum of squares, owing to rank deficiencies in the underlying design matrix (e.g. when one participant was drawn more than twice within a sample). In these cases, models were fit using type 1 sum of squares. However, as the experiment designs were fully balanced, each sum of squares type should yield the same results.

#### Multitasking
To ascertain the effectiveness of the multitasking manipulation, the data were modelled using a 2 (task-modality: visual-manual vs auditory-manual) x 2 (task: ST vs MT) repeated-measures ANOVA. This analysis was also applied to each $k$ sample; $\epsilon_{p}^2$ and $p$ are reported for both the main effect of task and the task-modality x task interaction. 

#### Contextual Cueing

To ascertain whether participants became faster for repeat relative to novel trials over the course of the experiment (i.e. whether participants learned the statistical regularities of the repeated displays), the data were subject to a block (1:12) x condition (repeat vs novel display) repeated measures ANOVA. Specifically, learning should be evidenced by a significant block x condition interaction. This analysis was applied to each $k$ sample, and we report $\epsilon_{p}^2$ and $p$ for the block x condition interaction.

As some studies from the contextual cueing literature suggest that the effect is better characterised by a main effect of condition thereby implying rapid learning of the statistical regularities [e.g. @petersonAttentionalGuidanceEyes2001; @travisRoleWorkingMemory], we also report the $\epsilon_{p}^2$ and $p$ for the main effect of condition.


#### Serial Response Task

To ascertain whether participants learned the repeating sequences, RTs in the final block of sequence trials (block 3) were compared to those in the final block of random trials (block 4) using a paired-samples t-test. This analysis was also applied to each $k$ sample, and we present the resulting Cohen's $d_{z}$, and $p$ value from each test. 

<br>

# Results
\label{sec:Results}

We first present the results from the standard analyses used for each task, to show that we replicate the classic results from each task. The key findings from each task are presented in \@ref(fig:ADD THE BEHAV RESULTS). Next we present the effect size data, followed by p-values.

## Behavioural Results

### Attentional Blink
\label{sec:ABRes}

```{r, AB_analysis, message=F}
abfname <- "../data/total_of_313_subs_AB_task_trial_level_data.csv"
ab_res <- do.AB.analysis(abfname)
```

An overview for the findings for the AB task are presented in Figure \@ref(fig:ABFX). T2|T1 performance suffered proportional to temporal proximity to T1; proportion accuracy for T2|T1 decreased (by around p = `r sprintf("%.2f", abs(ab_res[[2]][,3]$estimate))`) when T2 was presented at lag 2, relative to lag 7. A one-way ANOVA revealed that the effect of lag was statistically significant (F (`r sprintf("%.1f", ab_res[[1]]$DFn)`, `r sprintf("%.0f", ab_res[[1]]$DFd)`) = `r sprintf("%.0f", ab_res[[1]]$F)`, $\eta_{p}^2$ = `r sprintf("%.2f", ab_res[[1]]$pes)`, p = `r sprintf("%.2e", ab_res[[1]]$p)`). Post-hoc t-tests showed that accuracy at each lag differed statistically from accuracy at each of the other lags (all p's $\leq$ `r sprintf("%.2e", max(do.call(rbind, lapply(1:6, function(x) cbind(ab_res[[2]][,x]$p.value)))))`), with lower accuracy values at the shorter relative to the longer lags. Therefore, we see that our implementation of the AB paradigm yielded the typically observed effects.

### Multitasking

```{r, MT_F, message=F}
mtfname <- "../data/total_of_313_subs_SingDual_task_trial_level_data.csv"
mt_res <- do.MT.analysis(mtfname)
```

As was anticipated, RTs were slowed for multitask relative to single task conditions (see Figure \@ref(fig:MTFX), panel A). Mean RTs were on average `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$estimate[[1]]))` (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[2]))`, `r sprintf("%.2f", abs(mt_res[[2]]$me_MT$conf.int[1]))`]) seconds (s) slower on MT trials (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "trialtype"])`) = `r sprintf("%4.0f", mt_res[[1]]$F[mt_res[[1]]$Effect == "trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "trialtype"])`, p<.0001). There was also a significant task modality (sound or visual) x task (ST vs MT) interaction  (F(`r sprintf("%1.0f", mt_res[[1]]$DFn[mt_res[[1]]$Effect == "task:trialtype"])`, `r sprintf("%3.0f", mt_res[[1]]$DFd[mt_res[[1]]$Effect == "task:trialtype"])`) = `r sprintf("%2.1f", mt_res[[1]]$F[mt_res[[1]]$Effect == "task:trialtype"])`,  $\eta_{p}^2$ = `r sprintf("%.2f", mt_res[[1]]$pes[mt_res[[1]]$Effect == "task:trialtype"])`, p<.0001), with the MT cost (MT RT - ST RT) being larger for  the sound task relative to the visual task by on average `r sprintf("%.2f", abs(mt_res[[2]]$interaction$estimate[[1]]))` s (95% CI[`r sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[1]))`, `r sprintf(sprintf("%.2f", abs(mt_res[[2]]$interaction$conf.int[2])))`]). This latter finding has been previously reported in the multitasking literature [@hazeltineModalityPairingEffects2006], and we continue to interrogate it in the effect size results below, as it serves as an example of an interaction effect with a small effect size, to facilitate comparisons to interactions with small effect sizes in the implicit learning literature (see [REF] contextal cueing results below).

### Contextual Cueing
\label{sec:CCRes}

```{r, CC_interaction, message=F}
ccfname <- "../data/total_of_313_subs_CC_task_trial_level_data.csv"
cc_res <- do.CC.analysis(ccfname)
```

Participants learned the repeat displays over the course of the experiment; the RT data showed a significant albeit small block x condition interaction (F (`r sprintf("%2.2f", cc_res[[1]]$DFn[3])`, `r sprintf("%4.1f", cc_res[[1]]$DFd[3])`) = `r sprintf("%1.2f", cc_res[[1]]$F[3])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[3])`, p = `r sprintf("%.2e", cc_res[[1]]$p[3])`). There was no statistically significant difference between RTs for repeat and novel displays at the beginning of the experiment (block 1: t (`r sprintf("%3.0f", cc_res[[2]][[1]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[1]]$statistic)`, p = `r sprintf("%.2f", cc_res[[2]][[1]]$p.value)`, $\mu$ difference = `r sprintf("%.2f", cc_res[[2]][[1]]$estimate/1000)` s, sd: `r sprintf("%.2f", (cc_res[[2]][[1]]$stderr/1000)*sqrt(313))`). However, by block 12, RTs for repeat displays were on average `r sprintf("%.2f", cc_res[[2]][[2]]$estimate/1000)` s faster than novel displays (sd: `r sprintf("%.2f", (cc_res[[2]][[2]]$stderr/1000)*sqrt(313))`, t (`r sprintf("%3.0f", cc_res[[2]][[2]]$parameter)`) = `r sprintf("%.2f", cc_res[[2]][[2]]$statistic)`, p = `r sprintf("%.2e", cc_res[[2]][[2]]$p.value)`, see Figure \@ref(fig:CCbeh), panel A). There was also a significant and larger main effect of block (F(`r sprintf("%.2f", cc_res[[1]]$DFn[1])`, `r sprintf("%.2f", cc_res[[1]]$DFd[1])`) = `r sprintf("%1.2f", cc_res[[1]]$F[1])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[1])`, p = `r sprintf("%.2e", cc_res[[1]]$p[1])`). Of relevance for the subsequent discussion on the $\eta_{p}^2$ results, there was also a significant main effect of condition (F(`r sprintf("%.2f", cc_res[[1]]$DFn[2])`, `r sprintf("%.2f", cc_res[[1]]$DFd[2])`) = `r sprintf("%1.2f", cc_res[[1]]$F[2])`, $\eta_{p}^2$ = `r sprintf("%.2f", cc_res[[1]]$pes[2])`, p = `r sprintf("%.2e", cc_res[[1]]$p[2])`).

### SRT

```{r, SRT_F, message=F}
srtfname <- "../data/total_of_313_subs_SRT_task_trial_level_data.csv"
srt_res <- do.SRT.analysis(srtfname)
```

The results from the SRT paradigm are presented in [REF]. Participants learned the repeating sequence; RTs were on average `r sprintf("%.3f", srt_res[[1]]$estimate/1000)` s faster (95% CI [`r sprintf("%.3f", srt_res[[1]]$conf.int[[1]]/1000)`, `r sprintf("%.3f", srt_res[[1]]$conf.int[[2]]/1000)`]) for the sequence relative to the random condition (t(`r sprintf("%3.0f", srt_res[[1]]$parameter[[1]])`) = `r sprintf("%2.2f", srt_res[[1]]$statistic[[1]])`, $d$ = `r sprintf("%1.2f", srt_res[[2]])`, p = `r sprintf("%.2e", srt_res[[1]]$p.value)`). 

## Effect Sizes

## p-Values

# Discussion
\label{sec:Discussion}


CONVERGENCE AND COMMONALITIES


MEASURMENT ERROR HELD CONSTANT HERE

CC PARAMS 
## Conclusions




\clearpage

# References
\label{sec:Refref}

