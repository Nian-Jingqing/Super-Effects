@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1096-0465},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta^2$}, {$\omega^2$} and {$\epsilon^2$}) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta^2$} in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Sample Size,Statistical Analysis,Statistical Data,Statistical Estimation},
  file = {/home/kelly/Zotero/storage/NZMF2YG7/Albers and Lakens - 2018 - When power analyses based on pilot data are biased.pdf;/home/kelly/Zotero/storage/WPRV2Z4C/2017-53838-020.html}
}

@article{allefeldValidPopulationInference2016,
  title = {Valid Population Inference for Information-Based Imaging: {{From}} the Second-Level t-Test to Prevalence Inference},
  shorttitle = {Valid Population Inference for Information-Based Imaging},
  author = {Allefeld, Carsten and G{\"o}rgen, Kai and Haynes, John-Dylan},
  year = {2016},
  month = nov,
  journal = {NeuroImage},
  volume = {141},
  pages = {378--392},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2016.07.040},
  abstract = {In multivariate pattern analysis of neuroimaging data, 'second-level' inference is often performed by entering classification accuracies into a t-test vs chance level across subjects. We argue that while the random-effects analysis implemented by the t-test does provide population inference if applied to activation differences, it fails to do so in the case of classification accuracy or other 'information-like' measures, because the true value of such measures can never be below chance level. This constraint changes the meaning of the population-level null hypothesis being tested, which becomes equivalent to the global null hypothesis that there is no effect in any subject in the population. Consequently, rejecting it only allows to infer that there are some subjects in which there is an information effect, but not that it generalizes, rendering it effectively equivalent to fixed-effects analysis. This statement is supported by theoretical arguments as well as simulations. We review possible alternative approaches to population inference for information-based imaging, converging on the idea that it should not target the mean, but the prevalence of the effect in the population. One method to do so, 'permutation-based information prevalence inference using the minimum statistic', is described in detail and applied to empirical data.},
  langid = {english},
  pmid = {27450073},
  keywords = {Brain,Brain Mapping,Computer Simulation,Data Interpretation; Statistical,Effect prevalence,Humans,Image Interpretation; Computer-Assisted,Information-based imaging,Models; Statistical,Multivariate Analysis,Multivariate pattern analysis,Population inference,Reproducibility of Results,Sensitivity and Specificity,t-Test},
  file = {/home/kelly/Zotero/storage/YW7HG89A/Allefeld et al. - 2016 - Valid population inference for information-based i.pdf}
}

@article{andersonSampleSizePlanningMore2017,
  title = {Sample-{{Size Planning}} for {{More Accurate Statistical Power}}: {{A Method Adjusting Sample Effect Sizes}} for {{Publication Bias}} and {{Uncertainty}}},
  shorttitle = {Sample-{{Size Planning}} for {{More Accurate Statistical Power}}},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  year = {2017},
  month = nov,
  journal = {Psychological Science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797617723724},
  abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/9WMIPV5V/Anderson et al. - 2017 - Sample-Size Planning for More Accurate Statistical.pdf}
}

@article{bagiellaMixedeffectsModelsPsychophysiology2000,
  title = {Mixed-Effects Models in Psychophysiology},
  author = {Bagiella, Emilia and Sloan, Richard P. and Heitjan, Daniel F.},
  year = {2000},
  journal = {Psychophysiology},
  volume = {37},
  number = {1},
  pages = {13--20},
  issn = {1469-8986},
  doi = {10.1111/1469-8986.3710013},
  abstract = {The current methodological policy in Psychophysiology stipulates that repeated-measures designs be analyzed using either multivariate analysis of variance (ANOVA) or repeated-measures ANOVA with the Greenhouse\textendash Geisser or Huynh\textendash Feldt correction. Both techniques lead to appropriate type I error probabilities under general assumptions about the variance-covariance matrix of the data. This report introduces mixed-effects models as an alternative procedure for the analysis of repeated-measures data in Psychophysiology. Mixed-effects models have many advantages over the traditional methods: They handle missing data more effectively and are more efficient, parsimonious, and flexible. We described mixed-effects modeling and illustrated its applicability with a simple example.},
  copyright = {Copyright \textcopyright{} 2003 Society for Psychophysiological Research},
  langid = {english},
  keywords = {Mixed effects models,Repeated measures designs,Variance-covariance matrix},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1469-8986.3710013},
  file = {/home/kelly/Zotero/storage/PUIBZWRY/Bagiella et al. - 2000 - Mixed-effects models in psychophysiology.pdf;/home/kelly/Zotero/storage/VZYT5F3A/1469-8986.html}
}

@article{barrRandomEffectsStructure2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {68},
  number = {3},
  pages = {255--278},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.11.001},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  langid = {english},
  keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
  file = {/home/kelly/Zotero/storage/DFMN2XF6/Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf;/home/kelly/Zotero/storage/7JRBYVVU/S0749596X12001180.html}
}

@article{bartosAdjustingPublicationBias2022,
  title = {Adjusting for {{Publication Bias}} in {{JASP}} and {{R}}: {{Selection Models}}, {{PET-PEESE}}, and {{Robust Bayesian Meta-Analysis}}},
  shorttitle = {Adjusting for {{Publication Bias}} in {{JASP}} and {{R}}},
  author = {Barto{\v s}, Franti{\v s}ek and Maier, Maximilian and Quintana, Daniel S. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {3},
  pages = {25152459221109259},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459221109259},
  abstract = {Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video (https://bit.ly/pubbias) and an R-markdown script (https://osf.io/uhaew/), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/UABKFX4H/Bartoš et al. - 2022 - Adjusting for Publication Bias in JASP and R Sele.pdf}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  copyright = {Copyright (c) 2015 Douglas Bates, Martin M\"achler, Ben Bolker, Steve Walker},
  langid = {english},
  keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
  file = {/home/kelly/Zotero/storage/5N3NRINA/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf;/home/kelly/Zotero/storage/PK2MWJT5/v067i01.html}
}

@article{benderRelationshipResponseSelection2016,
  title = {On the Relationship between Response Selection and Response Inhibition: {{An}} Individual Differences Approach},
  shorttitle = {On the Relationship between Response Selection and Response Inhibition},
  author = {Bender, Angela D. and Filmer, Hannah L. and Garner, K. G. and Naughtin, Claire K. and Dux, Paul E.},
  year = {2016},
  month = nov,
  journal = {Attention, Perception \& Psychophysics},
  volume = {78},
  number = {8},
  pages = {2420--2432},
  issn = {1943-393X},
  doi = {10.3758/s13414-016-1158-8},
  abstract = {The abilities to select appropriate responses and suppress unwanted actions are key executive functions that enable flexible and goal-directed behavior. However, to date it has been unclear whether these two cognitive operations tap a common action control resource or reflect two distinct processes. In the present study, we used an individual differences approach to examine the underlying relationships across seven paradigms that varied in their response selection and response inhibition requirements: stop-signal, go-no-go, Stroop, flanker, single-response selection, psychological refractory period, and attentional blink tasks. A confirmatory factor analysis suggested that response inhibition and response selection are separable, with stop-signal and go-no-go task performance being related to response inhibition, and performance in the psychological refractory period, Stroop, single-response selection, and attentional blink tasks being related to response selection. These findings provide evidence in support of the hypothesis that response selection and response inhibition reflect two distinct cognitive operations.},
  langid = {english},
  pmid = {27381631},
  keywords = {Adolescent,Adult,Attentional Blink,Executive control,Executive Function,Female,Humans,Individual differences,Individuality,Inhibition; Psychological,Male,Response inhibition,Response selection,Young Adult},
  file = {/home/kelly/Zotero/storage/93XIE7PA/Bender et al. - 2016 - On the relationship between response selection and.pdf}
}

@article{brainardPsychophysicsToolbox1997,
  ids = {brainardPsychophysicsToolbox1997a},
  title = {The {{Psychophysics Toolbox}}},
  author = {Brainard, D. H.},
  year = {1997},
  journal = {Spatial Vision},
  volume = {10},
  number = {4},
  pages = {433--436},
  publisher = {{Brill}},
  issn = {0169-1015},
  abstract = {The Psychophysics Toolbox is a software package that supports visual psychophysics. Its routines provide an interface between a high-level interpreted language (MATLAB on the Macintosh) and the video display hardware. A set of example programs is included with the Toolbox distribution.},
  chapter = {Spatial Vision},
  langid = {english},
  pmid = {9176952},
  keywords = {Computer Terminals,Data Display,Humans,Microcomputers,Psychophysics,Research,Software,User-Computer Interface},
  file = {/home/kelly/Zotero/storage/N4GQCAU7/Brainard - 1997 - The Psychophysics Toolbox.pdf}
}

@article{brandAccuracyEffectSize2008,
  title = {Accuracy of {{Effect Size Estimates}} from {{Published Psychological Research}}},
  author = {Brand, Andrew and Bradley, Michael T. and Best, Lisa A. and Stoica, George},
  year = {2008},
  month = apr,
  journal = {Perceptual and Motor Skills},
  volume = {106},
  number = {2},
  pages = {645--649},
  publisher = {{SAGE Publications Inc}},
  issn = {0031-5125},
  doi = {10.2466/pms.106.2.645-649},
  abstract = {A Monte-Carlo simulation was used to model the biasing of effect sizes in published studies. The findings from the simulation indicate that, when a predominant bias to publish studies with statistically significant results is coupled with inadequate statistical power, there will be an overestimation of effect sizes. The consequences such an effect size overestimation will then have on meta-analyses and power analyses are highlighted and discussed along with measures which can be taken to reduce the problem.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/DF9QS823/Brand et al. - 2008 - Accuracy of Effect Size Estimates from Published P.pdf}
}

@article{broersWhenNumbersNot2021,
  title = {When the {{Numbers Do Not Add Up}}: {{The Practical Limits}} of {{Stochastologicals}} for {{Soft Psychology}}},
  shorttitle = {When the {{Numbers Do Not Add Up}}},
  author = {Broers, Nick J.},
  year = {2021},
  month = jan,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  pages = {1745691620970557},
  issn = {1745-6924},
  doi = {10.1177/1745691620970557},
  abstract = {One particular weakness of psychology that was left implicit by Meehl is the fact that psychological theories tend to be verbal theories, permitting at best ordinal predictions. Such predictions do not enable the high-risk tests that would strengthen our belief in the verisimilitude of theories but instead lead to the practice of null-hypothesis significance testing, a practice Meehl believed to be a major reason for the slow theoretical progress of soft psychology. The rising popularity of meta-analysis has led some to argue that we should move away from significance testing and focus on the size and stability of effects instead. Proponents of this reform assume that a greater emphasis on quantity can help psychology to develop a cumulative body of knowledge. The crucial question in this endeavor is whether the resulting numbers really have theoretical meaning. Psychological science lacks an undisputed, preexisting domain of observations analogous to the observations in the space-time continuum in physics. It is argued that, for this reason, effect sizes do not really exist independently of the adopted research design that led to their manifestation. Consequently, they can have no bearing on the verisimilitude of a theory.},
  langid = {english},
  pmid = {33482071},
  keywords = {effect size,meta-analysis,ordinal versus quantitative predictions,verbal versus formal theories},
  file = {/home/kelly/Zotero/storage/6U2J2V2G/Broers - 2021 - When the Numbers Do Not Add Up The Practical Limi.pdf}
}

@article{brysbaertPowerAnalysisEffect2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  abstract = {Article: Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/57U3WQJH/Brysbaert and Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf;/home/kelly/Zotero/storage/225PUW45/joc.10.html}
}

@article{cafriMetaMetaAnalysisEmpiricalReview2010,
  title = {A {{Meta-Meta-Analysis}}: {{Empirical Review}} of {{Statistical Power}}, {{Type I Error Rates}}, {{Effect Sizes}}, and {{Model Selection}} of {{Meta-Analyses Published}} in {{Psychology}}},
  shorttitle = {A {{Meta-Meta-Analysis}}},
  author = {Cafri, Guy and Kromrey, Jeffrey D. and Brannick, Michael T.},
  year = {2010},
  month = jun,
  journal = {Multivariate Behavioral Research},
  publisher = {{Taylor \& Francis Group}},
  doi = {10.1080/00273171003680187},
  abstract = {This article uses meta-analyses published in Psychological Bulletin from 1995 to 2005 to describe meta-analyses in psychology, including examination of statistical power, Type I errors resulting fr...},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english},
  file = {/home/kelly/Zotero/storage/5VWGMJ4B/00273171003680187.html}
}

@article{carrollSamplingCharacteristicsKelley1975,
  title = {Sampling {{Characteristics}} of {{Kelley}}'s {$\epsilon$} and {{Hays}}' {$\omega$}},
  author = {Carroll, Robert M. and Nordholm, Lena A.},
  year = {1975},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {35},
  number = {3},
  pages = {541--554},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/001316447503500304},
  abstract = {Statistics used to estimate the population correlation ratio were reviewed and evaluated. The sampling distributions of Kelley's {$\epsilon$}2 and Hays' ?2 were studied empirically by computer simulation within the context of a three level one-way fixed effects analysis of variance design. These statistics were found to have rather large standard errors when small samples were used. As with other correlation indices, large samples are recommended for accuracy of estimation. Both {$\epsilon$}2 and ?2 were found to be negligibly biased. Heterogeneity of variances had negligible effects on the estimates under conditions of proportional representativeness of sample sizes with respect to their population counterparts, but combinations of heterogeneity of variance and unrepresentative sample sizes yielded especially poor estimates.},
  langid = {english}
}

@article{chenHandlingMultiplicityNeuroimaging2019,
  title = {Handling {{Multiplicity}} in {{Neuroimaging}} through {{Bayesian Lenses}} with {{Multilevel Modeling}}},
  author = {Chen, Gang and Xiao, Yaqiong and Taylor, Paul A. and Rajendra, Justin K. and Riggins, Tracy and Geng, Fengji and Redcay, Elizabeth and Cox, Robert W.},
  year = {2019},
  month = oct,
  journal = {Neuroinformatics},
  volume = {17},
  number = {4},
  pages = {515--545},
  issn = {1539-2791},
  doi = {10.1007/s12021-018-9409-6},
  abstract = {Here we address the current issues of inefficiency and over-penalization in the massively univariate approach followed by the correction for multiple testing, and propose a more efficient model that pools and shares information among brain regions. Using Bayesian multilevel (BML) modeling, we control two types of error that are more relevant than the conventional false positive rate (FPR): incorrect sign (type S) and incorrect magnitude (type M). BML also aims to achieve two goals: 1) improving modeling efficiency by having one integrative model and thereby dissolving the multiple testing issue, and 2) turning the focus of conventional null hypothesis significant testing (NHST) on FPR into quality control by calibrating type S errors while maintaining a reasonable level of inference efficiency. The performance and validity of this approach are demonstrated through an application at the region of interest (ROI) level, with all the regions on an equal footing: unlike the current approaches under NHST, small regions are not disadvantaged simply because of their physical size. In addition, compared to the massively univariate approach, BML may simultaneously achieve increased spatial specificity and inference efficiency, and promote results reporting in totality and transparency. The benefits of BML are illustrated in performance and quality checking using an experimental dataset. The methodology also avoids the current practice of sharp and arbitrary thresholding in the p-value funnel to which the multidimensional data are reduced. The BML approach with its auxiliary tools is available as part of the AFNI suite for general use.},
  pmcid = {PMC6635105},
  pmid = {30649677},
  file = {/home/kelly/Zotero/storage/KTU3QM73/Chen et al. - 2019 - Handling Multiplicity in Neuroimaging through Baye.pdf}
}

@article{chunContextualCueingImplicit1998,
  title = {Contextual Cueing: {{Implicit}} Learning and Memory of Visual Context Guides Spatial Attention},
  shorttitle = {Contextual Cueing},
  author = {Chun, Marvin M. and Jiang, Yuhong},
  year = {1998},
  journal = {Cognitive Psychology},
  volume = {36},
  number = {1},
  pages = {28--71},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1095-5623},
  doi = {10.1006/cogp.1998.0681},
  abstract = {Examined how visual context is learned and how it influences visual processing in standard visual tasks which required observers to localize and identify targets presented among multiple distractors. Six experiments were conducted using a total of 108 Ss. Global context was operationalized as the spatial layout of objects in visual search displays. Half of the configurations were repeated across blocks throughout the entire session, and targets appeared within consistent locations in these arrays. Targets appearing in learned configurations were detected more quickly. This newly discovered form of search facilitation is termed contextual cueing. Contextual cueing is driven by incidentally learned associations between spatial configurations (context) and target locations. This benefit was obtained despite chance performance for recognizing the configurations, suggesting that the memory for context was implicit. Results show how implicit learning and memory of visual context can guide spatial attention towards task-relevant aspects of a scene. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Attention,Contextual Cues,Cues,Memory,Spatial Perception,Visual Search},
  file = {/home/kelly/Zotero/storage/TA4DLS7W/1998-10248-002.html}
}

@article{cohenPowerPrimer1992,
  title = {A Power Primer},
  author = {Cohen, Jacob},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  pages = {155--159},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.112.1.155},
  abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product\textendash moment correlation, (3) the difference between independent rs, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Statistical Analysis,Statistical Power},
  file = {/home/kelly/Zotero/storage/XVWSS55R/doiLanding.html}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical {{Power Analysis}} for the {{Behavioural Sciences}}},
  author = {Cohen, J},
  year = {1988},
  edition = {Second Edition},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Hillsdale, NJ}}
}

@article{cummingNewStatisticsWhy2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797613504966},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/2HRVU4KY/Cumming - 2014 - The New Statistics Why and How.pdf}
}

@article{duBayesianFillinMethod2017,
  title = {A {{Bayesian}} ``Fill-in'' Method for Correcting for Publication Bias in Meta-Analysis},
  author = {Du, Han and Liu, Fang and Wang, Lijuan},
  year = {2017},
  journal = {Psychological Methods},
  volume = {22},
  pages = {799--817},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000164},
  abstract = {Publication bias occurs when the statistical significance or direction of the results between published and unpublished studies differ after controlling for study quality, which threatens the validity of the systematic review and summary of the results on a research topic. Conclusions based on a meta-analysis of published studies without correcting for publication bias are often optimistic and biased toward significance or positivity. We propose a Bayesian fill-in meta-analysis (BALM) method for adjusting publication bias and estimating population effect size that accommodates different assumptions for publication bias. Simulation studies were conducted to examine the performance of BALM and compare it with several commonly used/discussed and recently proposed publication bias correction methods. The simulation results suggested BALM yielded small biases, small RMSE values, and close-to-nominal-level coverage rates in inferring the population effect size and the between-study variance, and outperformed the other examined publication bias correction methods across a wide range of simulation scenarios when the publication bias mechanism is correctly specified. The performance of BALM was relatively sensitive to the assumed publication bias mechanism. Even with a misspecified publication bias mechanism, BALM still outperformed the naive methods without correcting for publication in inferring the overall population effect size. BALM was applied to 2 meta-analysis case studies to illustrate the use of BALM in real life situations. R functions are provided to facilitate the implementation of BALM. Guidelines on how to specify the publication bias mechanisms in BALM and how to report overall effect size estimates are provided. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Mathematical Modeling,Meta Analysis,Scientific Communication,Statistical Data,Statistical Estimation,Statistical Probability},
  file = {/home/kelly/Zotero/storage/IY7TGT5B/2017-55458-007.html}
}

@article{duxAttentionalBlinkReview2009,
  title = {The Attentional Blink: A Review of Data and Theory},
  shorttitle = {The Attentional Blink},
  author = {Dux, Paul E. and Marois, Ren{\'e}},
  year = {2009},
  month = nov,
  journal = {Attention, Perception \& Psychophysics},
  volume = {71},
  number = {8},
  pages = {1683--1700},
  issn = {1943-393X},
  doi = {10.3758/APP.71.8.1683},
  abstract = {Under conditions of rapid serial visual presentation, subjects display a reduced ability to report the second of two targets (Target 2; T2) in a stream of distractors if it appears within 200-500 msec of Target 1 (T1). This effect, known as the attentional blink (AB), has been central in characterizing the limits of humans' ability to consciously perceive stimuli distributed across time. Here, we review theoretical accounts of the AB and examine how they explain key findings in the literature. We conclude that the AB arises from attentional demands of T1 for selection, working memory encoding, episodic registration, and response selection, which prevents this high-level central resource from being applied to T2 at short T1-T2 lags. T1 processing also transiently impairs the redeployment of these attentional resources to subsequent targets and the inhibition of distractors that appear in close temporal proximity to T2. Although these findings are consistent with a multifactorial account of the AB, they can also be largely explained by assuming that the activation of these multiple processes depends on a common capacity-limited attentional process for selecting behaviorally relevant events presented among temporally distributed distractors. Thus, at its core, the attentional blink may ultimately reveal the temporal limits of the deployment of selective attention.},
  langid = {english},
  pmcid = {PMC2915904},
  pmid = {19933555},
  keywords = {Attentional Blink,Humans,Pattern Recognition; Visual,Psychological Theory,Reaction Time,Refractory Period; Psychological},
  file = {/home/kelly/Zotero/storage/CNWUFU4D/Dux and Marois - 2009 - The attentional blink a review of data and theory.pdf}
}

@article{duxIsolationCentralBottleneck2006,
  title = {Isolation of a Central Bottleneck of Information Processing with Time-Resolved {{FMRI}}},
  author = {Dux, Paul E. and Ivanoff, Jason and Asplund, Christopher L. and Marois, Ren{\'e}},
  year = {2006},
  month = dec,
  journal = {Neuron},
  volume = {52},
  number = {6},
  pages = {1109--1120},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2006.11.009},
  abstract = {When humans attempt to perform two tasks at once, execution of the first task usually leads to postponement of the second one. This task delay is thought to result from a bottleneck occurring at a central, amodal stage of information processing that precludes two response selection or decision-making operations from being concurrently executed. Using time-resolved functional magnetic resonance imaging (fMRI), here we present a neural basis for such dual-task limitations, e.g. the inability of the posterior lateral prefrontal cortex, and possibly the superior medial frontal cortex, to process two decision-making operations at once. These results suggest that a neural network of frontal lobe areas acts as a central bottleneck of information processing that severely limits our ability to multitask.},
  langid = {english},
  pmcid = {PMC2527865},
  pmid = {17178412},
  keywords = {Acoustic Stimulation,Adult,Brain Mapping,Decision Making,Female,Frontal Lobe,Functional Laterality,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Mental Processes,Photic Stimulation,Problem Solving,Random Allocation,Reaction Time,Time Factors}
}

@article{eggerBiasMetaanalysisDetected1997,
  title = {Bias in Meta-Analysis Detected by a Simple, Graphical Test},
  author = {Egger, Matthias and Smith, George Davey and Schneider, Martin and Minder, Christoph},
  year = {1997},
  month = sep,
  journal = {BMJ},
  volume = {315},
  number = {7109},
  pages = {629--634},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.315.7109.629},
  abstract = {Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trialsFunnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biasesFunnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trialsFunnel plot asymmetry was found in 38\% of meta-analyses published in leading general medicine journals and in 13\% of reviews from the Cochrane Database of Systematic ReviewsCritical examination of systematic reviews for publication and related biases should be considered a routine procedure},
  chapter = {Paper},
  copyright = {\textcopyright{} 1997 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {9310563},
  file = {/home/kelly/Zotero/storage/ZQV5HL3U/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf;/home/kelly/Zotero/storage/TELX39L4/629.html}
}

@article{faulPowerFlexibleStatistical2007,
  title = {G*{{Power}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  shorttitle = {G*{{Power}} 3},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  year = {2007},
  month = may,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {2},
  pages = {175--191},
  issn = {1554-3528},
  doi = {10.3758/BF03193146},
  abstract = {G*Power (Erdfelder, Faul, \& Buchner, 1996) was designed as a general stand-alone power analysis program for statistical tests commonly used in social and behavioral research. G*Power 3 is a major extension of, and improvement over, the previous versions. It runs on widely used computer platforms (i.e., Windows XP, Windows Vista, and Mac OS X 10.4) and covers many different statistical tests of thet, F, and {$\chi$}2 test families. In addition, it includes power analyses forz tests and some exact tests. G*Power 3 provides improved effect size calculators and graphic options, supports both distribution-based and design-based input modes, and offers all types of power analyses in which users might be interested. Like its predecessors, G*Power 3 is free.},
  langid = {english},
  keywords = {Implicit Association Test,Main Window,Negative Priming,Noncentrality Parameter,Power Analysis},
  file = {/home/kelly/Zotero/storage/4T755LZ7/Faul et al. - 2007 - GPower 3 A flexible statistical power analysis p.pdf}
}

@article{fiserUnsupervisedStatisticalLearning2001,
  title = {Unsupervised {{Statistical Learning}} of {{Higher-Order Spatial Structures}} from {{Visual Scenes}}},
  author = {Fiser, J{\'o}zsef and Aslin, Richard N.},
  year = {2001},
  month = nov,
  journal = {Psychological Science},
  volume = {12},
  number = {6},
  pages = {499--504},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00392},
  abstract = {Three experiments investigated the ability of human observers to extract the joint and conditional probabilities of shape cooccurrences during passive viewing of complex visual scenes. Results indicated that statistical learning of shape conjunctions was both rapid and automatic, as subjects were not instructed to attend to any particular features of the displays. Moreover, in addition to single-shape frequency, subjects acquired in parallel several different higher-order aspects of the statistical structure of the displays, including absolute shape-position relations in an array, shape-pair arrangements independent of position, and conditional probabilities of shape co-occurrences. Unsupervised learning of these higher-order statistics provides support for Barlow's theory of visual recognition, which posits that detecting ``suspicious coincidences'' of elements during recognition is a necessary prerequisite for efficient learning of new visual features.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/7NSJD2SE/Fiser and Aslin - 2001 - Unsupervised Statistical Learning of Higher-Order .pdf}
}

@book{foxCompanionAppliedRegression2018,
  title = {An {{R Companion}} to {{Applied Regression}}},
  author = {Fox, J. and Weisberg, S.},
  year = {2018},
  month = sep,
  publisher = {{SAGE Publications}},
  abstract = {An R Companion to Applied Regression is a broad introduction to the R statistical computing environment in the context of applied regression analysis. John Fox and Sanford Weisberg provide a step-by-step guide to using the free statistical software R, an emphasis on integrating statistical computing in R with the practice of data analysis, coverage of generalized linear models, and substantial web-based support materials. The Third Edition includes a new chapter on mixed-effects models, new and updated data sets, and a de-emphasis on statistical programming, while retaining a general introduction to basic R programming. The authors have substantially updated both the car and effects packages for R for this new edition, and include coverage of RStudio and R Markdown.},
  googlebooks = {uPNrDwAAQBAJ},
  isbn = {978-1-5443-3648-0},
  langid = {english},
  keywords = {Reference / Research,Social Science / Statistics}
}

@article{fristonTenIronicRules2012,
  title = {Ten Ironic Rules for Non-Statistical Reviewers},
  author = {Friston, Karl},
  year = {2012},
  month = jul,
  journal = {NeuroImage},
  volume = {61},
  number = {4},
  pages = {1300--1310},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.04.018},
  abstract = {As an expert reviewer, it is sometimes necessary to ensure a paper is rejected. This can sometimes be achieved by highlighting improper statistical practice. This technical note provides guidance on how to critique the statistical analysis of neuroimaging studies to maximise the chance that the paper will be declined. We will review a series of critiques that can be applied universally to any neuroimaging paper and consider responses to potential rebuttals that reviewers might encounter from authors or editors.},
  langid = {english},
  keywords = {Classical inference,Effect size,Power,Sample-size,Statistical testing},
  file = {/home/kelly/Zotero/storage/DKI6JJYR/S1053811912003990.html}
}

@article{garnerIncentiveValueSpatial2021a,
  title = {Incentive Value and Spatial Certainty Combine Additively to Determine Visual Priorities},
  author = {Garner, K.G. and Bowman, H. and Raymond, J.E.},
  year = {2021},
  month = jan,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {83},
  number = {1},
  pages = {173--186},
  issn = {1943-393X},
  doi = {10.3758/s13414-020-02124-w},
  abstract = {How does the brain combine information predictive of the value of a visually guided task (incentive value) with information predictive of where task-relevant stimuli may occur (spatial certainty)? Human behavioural evidence indicates that these two predictions may be combined additively to bias visual selection (Additive Hypothesis), whereas neuroeconomic studies posit that they may be multiplicatively combined (Expected Value Hypothesis). We sought to adjudicate between these two alternatives. Participants viewed two coloured placeholders that specified the potential value of correctly identifying an imminent letter target if it appeared in that placeholder. Then, prior to the target's presentation, an endogenous spatial cue was presented indicating the target's more likely location. Spatial cues were parametrically manipulated with regard to the information gained (in bits). Across two experiments, performance was better for targets appearing in high versus low value placeholders and better when targets appeared in validly cued locations. Interestingly, as shown with a Bayesian model selection approach, these effects did not interact, clearly supporting the Additive Hypothesis. Even when conditions were adjusted to increase the optimality of a multiplicative operation, support for it remained. These findings refute recent theories that expected value computations are the singular mechanism driving the deployment of endogenous spatial attention. Instead, incentive value and spatial certainty seem to act independently to influence visual selection.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/FCW234QL/Garner et al. - 2021 - Incentive value and spatial certainty combine addi.pdf}
}

@misc{garnerQuantifyingErrorEffect2022,
  title = {Quantifying Error in Effect Size Estimates in Executive Function and Implicit Learning: {{Code}} Repository},
  author = {Garner, Kelly G. and Nolan, Christopher R and Knott, Zoie},
  year = {2022}
}

@misc{garnerQuantifyingErrorEffect2022a,
  title = {Quantifying Error in Effect Size Estimates in Executive Function and Implicit Learning: {{Data Collection}}},
  author = {Garner, Kelly G. and Nolan, Christopher R.},
  year = {2022}
}

@article{garnerTransferabilityTrainingBenefits2015,
  title = {Transferability of {{Training Benefits Differs}} across {{Neural Events}}: {{Evidence}} from {{ERPs}}},
  shorttitle = {Transferability of {{Training Benefits Differs}} across {{Neural Events}}},
  author = {Garner, Kelly G. and Matthews, Natasha and Remington, Roger W. and Dux, Paul E.},
  year = {2015},
  month = oct,
  journal = {Journal of Cognitive Neuroscience},
  volume = {27},
  number = {10},
  pages = {2079--2094},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00833},
  abstract = {Humans can show striking capacity limitations in sensorimotor processing. Fortunately, these limitations can be attenuated with training. However, less fortunately, training benefits often remain limited to trained tasks. Recent behavioral observations suggest that the extent to which training transfers may depend on the specific stage of information processing that is being executed. Training benefits for a task that taps the consolidation of sensory information (sensory encoding) transfer to new stimulus\textendash response mappings, whereas benefits for selecting an appropriate action (decision-making/response selection) remain specific to the trained mappings. Therefore, training may have dissociable influences on the neural events underlying subsequent sensorimotor processing stages. Here, we used EEG to investigate this possibility. In a pretraining baseline session, participants completed two four-alternative-choice response time tasks, presented both as a single task and as part of a dual task (with another task). The training group completed a further 3,000 training trials on one of the four-alternative-choice tasks. Hence, one task became trained, whereas the other remained untrained. At test, a negative-going component that is sensitive to sensory-encoding demands (N2) showed increased amplitudes and reduced latencies for trained and untrained mappings relative to a no-train control group. In contrast, the onset of the stimulus-locked lateralized readiness potential, a component that reflects the activation of motor plans, was reduced only for tasks that employed trained stimulus\textendash response mappings, relative to untrained stimulus\textendash response mappings and controls. Collectively, these results show that training benefits are dissociable for the brain events that reflect distinct sensorimotor processing stages.},
  file = {/home/kelly/Zotero/storage/9MMD38TS/Garner et al. - 2015 - Transferability of Training Benefits Differs acros.pdf;/home/kelly/Zotero/storage/BCU8N682/Transferability-of-Training-Benefits-Differs.html}
}

@article{gelmanPowerCalculationsAssessing2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  issn = {1745-6924},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  pmid = {26186114},
  keywords = {Beauty,Data Interpretation; Statistical,design calculation,exaggeration ratio,Female,Humans,Male,Menstrual Cycle,Politics,power analysis,Psychology,replication crisis,Research Design,Sex Ratio,statistical significance,Type M error,Type S error},
  file = {/home/kelly/Zotero/storage/MQ9GTI37/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf}
}

@article{gelmanWeaklyInformativeDefault2008,
  title = {A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models},
  author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
  year = {2008},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {4},
  pages = {1360--1383},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS191},
  abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
  keywords = {Bayesian inference,generalized linear model,hierarchical model,least squares,Linear regression,logistic regression,multilevel model,noninformative prior distribution,weakly informative prior distribution},
  file = {/home/kelly/Zotero/storage/RAQ8LXVK/Gelman et al. - 2008 - A weakly informative default prior distribution fo.pdf;/home/kelly/Zotero/storage/V9BXBYTB/08-AOAS191.html}
}

@article{gignacEffectSizeGuidelines2016,
  title = {Effect Size Guidelines for Individual Differences Researchers},
  author = {Gignac, Gilles E. and Szodorai, Eva T.},
  year = {2016},
  month = nov,
  journal = {Personality and Individual Differences},
  volume = {102},
  pages = {74--78},
  issn = {0191-8869},
  doi = {10.1016/j.paid.2016.06.069},
  abstract = {Individual differences researchers very commonly report Pearson correlations between their variables of interest. Cohen (1988) provided guidelines for the purposes of interpreting the magnitude of a correlation, as well as estimating power. Specifically, r=0.10, r=0.30, and r=0.50 were recommended to be considered small, medium, and large in magnitude, respectively. However, Cohen's effect size guidelines were based principally upon an essentially qualitative impression, rather than a systematic, quantitative analysis of data. Consequently, the purpose of this investigation was to develop a large sample of previously published meta-analytically derived correlations which would allow for an evaluation of Cohen's guidelines from an empirical perspective. Based on 708 meta-analytically derived correlations, the 25th, 50th, and 75th percentiles corresponded to correlations of 0.11, 0.19, and 0.29, respectively. Based on the results, it is suggested that Cohen's correlation guidelines are too exigent, as {$<$}3\% of correlations in the literature were found to be as large as r=0.50. Consequently, in the absence of any other information, individual differences researchers are recommended to consider correlations of 0.10, 0.20, and 0.30 as relatively small, typical, and relatively large, in the context of a power analysis, as well as the interpretation of statistical results from a normative perspective.},
  langid = {english},
  keywords = {Correlations,Effect size,Guidelines},
  file = {/home/kelly/Zotero/storage/QJDKUFJE/Gignac and Szodorai - 2016 - Effect size guidelines for individual differences .pdf;/home/kelly/Zotero/storage/8CJIAVZD/S0191886916308194.html}
}

@article{guoSelectingSampleSize2013,
  title = {Selecting a Sample Size for Studies with Repeated Measures},
  author = {Guo, Yi and Logan, Henrietta L. and Glueck, Deborah H. and Muller, Keith E.},
  year = {2013},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {13},
  number = {1},
  pages = {100},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-100},
  abstract = {Many researchers favor repeated measures designs because they allow the detection of within-person change over time and typically have higher statistical power than cross-sectional designs. However, the plethora of inputs needed for repeated measures designs can make sample size selection, a critical step in designing a successful study, difficult. Using a dental pain study as a driving example, we provide guidance for selecting an appropriate sample size for testing a time by treatment interaction for studies with repeated measures. We describe how to (1) gather the required inputs for the sample size calculation, (2) choose appropriate software to perform the calculation, and (3) address practical considerations such as missing data, multiple aims, and continuous covariates.},
  keywords = {Interaction,Repeated measures,Sample size selection},
  file = {/home/kelly/Zotero/storage/DSQPYV6M/Guo et al. - 2013 - Selecting a sample size for studies with repeated .pdf;/home/kelly/Zotero/storage/V6DF584S/1471-2288-13-100.html}
}

@article{hazeltineModalityPairingEffects2006,
  title = {Modality Pairing Effects and the Response Selection Bottleneck},
  author = {Hazeltine, Eliot and Ruthruff, Eric},
  year = {2006},
  month = nov,
  journal = {Psychological Research},
  volume = {70},
  number = {6},
  pages = {504--513},
  issn = {1430-2772},
  doi = {10.1007/s00426-005-0017-3},
  abstract = {The present experiment examined the effects of input/output modality pairings on dual-task performance using the psychological refractory period (PRP) procedure. Four groups of participants performed two tasks composed of the same sets of inputs (visual and auditory) and the same sets of outputs (manual and vocal), but with different input/output modality pairings. Whereas modality pairings had only small effects on single-task reaction times, they had large effects on dual-task reaction times. The modality pairing effect cannot stem from differences in the difficulty of stimulus classification or response execution, because these task demands were the same across groups. The effect also does not appear to result from changes in stimulus\textendash response compatibility. The present findings suggest dual-task interference arises not only from postponement of central operations (due to a central bottleneck), but also from a slowing of central operations whose magnitude is sensitive to the input/output modality pairings.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/QWMB9BG5/Hazeltine and Ruthruff - 2006 - Modality pairing effects and the response selectio.pdf}
}

@article{hedgesEstimationEffectSize1982,
  title = {Estimation of Effect Size from a Series of Independent Experiments},
  author = {Hedges, Larry V.},
  year = {1982},
  journal = {Psychological Bulletin},
  volume = {92},
  pages = {490--499},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.92.2.490},
  abstract = {Extends statistical theory for procedures based on the Glass estimator of effect size for methods used in the quantitative synthesis of research. An unbiased estimator of effect size is given. A weighted estimator of effect size based on data from several experiments is defined and shown to be optimal (asymptotically efficient). An approximate (large-sample) test for homogeneity of effect size across experiments is also given. The results of an empirical sampling study show that the large-sample distributions of the weighted estimator and the homogeneity statistic are quite accurate when the experimental and control group sample sizes exceed 10 and the effect sizes are smaller than about 1.5. (12 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Estimation,Experimentation,Statistical Significance},
  file = {/home/kelly/Zotero/storage/FC6WX482/Hedges - 1982 - Estimation of effect size from a series of indepen.pdf;/home/kelly/Zotero/storage/68VIUCBJ/1983-00212-001.html}
}

@article{hentschkeComputationMeasuresEffect2011,
  title = {Computation of Measures of Effect Size for Neuroscience Data Sets},
  author = {Hentschke, Harald and St{\"u}ttgen, Maik C.},
  year = {2011},
  journal = {European Journal of Neuroscience},
  volume = {34},
  number = {12},
  pages = {1887--1894},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2011.07902.x},
  abstract = {The overwhelming majority of research in the neurosciences employs P-values stemming from tests of statistical significance to decide on the presence or absence of an effect of some treatment variable. Although a continuous variable, the P-value is commonly used to reach a dichotomous decision about the presence of an effect around an arbitrary criterion of 0.05. This analysis strategy is widely used, but has been heavily criticized in the past decades. To counter frequent misinterpretations of P-values, it has been advocated to complement or replace P-values with measures of effect size (MES). Many psychological, biological and medical journals now recommend reporting appropriate MES. One hindrance to the more frequent use of MES may be their scarcity in standard statistical software packages. Also, the arguably most widespread data analysis software in neuroscience, matlab, does not provide MES beyond correlation and receiver-operating characteristic analysis. Here we review the most common criticisms of significance testing and provide several examples from neuroscience where use of MES conveys insights not amenable through the use of P-values alone. We introduce an open-access matlab toolbox providing a wide range of MES to complement the frequently used types of hypothesis tests, such as t-tests and analysis of variance. The accompanying documentation provides calculation formulae, intuitive explanations and example calculations for each measure. The toolbox described is usable without sophisticated statistical knowledge and should be useful to neuroscientists wishing to enhance their repertoire of statistical reporting.},
  langid = {english},
  keywords = {confidence interval,matlab,null-hypothesis significance testing,P-value,statistics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2011.07902.x},
  file = {/home/kelly/Zotero/storage/ZK4L6JSE/Hentschke and Stüttgen - 2011 - Computation of measures of effect size for neurosc.pdf;/home/kelly/Zotero/storage/BD5DZGVP/j.1460-9568.2011.07902.html}
}

@incollection{jiangContextualCueing2020,
  title = {Contextual Cueing},
  booktitle = {Neuromethods},
  author = {Jiang, YV and Sisk, CA},
  year = {2020},
  volume = {151},
  publisher = {{Humana Press Inc}}
}

@article{jiangImplicitGuidanceAttention2019,
  title = {Implicit Guidance of Attention in Contextual Cueing: {{Neuropsychological}} and Developmental Evidence},
  shorttitle = {Implicit Guidance of Attention in Contextual Cueing},
  author = {Jiang, Yuhong V. and Sisk, Caitlin A. and Toh, Yi Ni},
  year = {2019},
  month = oct,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {105},
  pages = {115--125},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2019.07.002},
  abstract = {When searching for an object in a familiar environment, we may automatically orient to locations where this object was often placed previously. Contextual cueing refers to the guidance of attention by repeated search context. As an implicit mechanism with high capacity, contextual cueing may be important for people whose cognitive function is compromised, immature, or in decline. Here we review and synthesize the last two decades of research on contextual cueing, focusing on neuropsychological and developmental evidence. Contextual cueing is largely preserved in young children, older adults, and individuals with autism spectrum disorders or mild intellectual impairment. Some, though not all, studies find a deficit in contextual cueing in amnesic patients, patients with basal ganglia damage, children with ADHD, and individuals with psychiatric disorders. Although the medial temporal lobe, the basal ganglia, and the posterior parietal cortex are implicated in contextual cueing, definitive evidence for their necessity is lacking. These findings suggest that contextual cueing is an evolutionarily conserved mechanism that is exceptionally robust to damages to single brain sites.},
  langid = {english},
  keywords = {Attention,Basal ganglia,Cognitive development,Contextual cueing,Implicit learning,Medial temporal lobe,Memory}
}

@article{lakensCalculatingReportingEffect2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  month = nov,
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  pmcid = {PMC3840331},
  pmid = {24324449},
  file = {/home/kelly/Zotero/storage/JSZ3F85Z/Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf}
}

@article{lakensSimulationBasedPowerAnalysis2021,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920951503},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920951503},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need ?p2 or Cohen?s f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/3R668DX3/Lakens and Caldwell - 2021 - Simulation-Based Power Analysis for Factorial Anal.pdf}
}

@article{laneEstimatingEffectSize1978,
  title = {Estimating Effect Size: {{Bias}} Resulting from the Significance Criterion in Editorial Decisions},
  shorttitle = {Estimating Effect Size},
  author = {Lane, David M. and Dunlap, William P.},
  year = {1978},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {31},
  number = {2},
  pages = {107--112},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.1978.tb00578.x},
  abstract = {Experiments that find larger differences between groups than actually exist in the population are more likely to pass stringent tests of significance and be published than experiments that find smaller differences. Published measures of the magnitude of experimental effects will therefore tend to overestimate these effects. This bias was investigated as a function of sample size, actual population difference, and alpha level. The overestimation of experimental effects was found to be quite large with the commonly employed significance levels of 5 per cent and 1 per cent. Further, the recently recommended measure, {$\omega$}2, was found to depend much more heavily on the alpha level employed than the true population {$\omega$}2 value. Hence, it was concluded that effect size estimation is impractical unless scientific journals drop the consideration of statistical significance as one of the criteria of publication.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1978.tb00578.x},
  file = {/home/kelly/Zotero/storage/X9JG6JNF/Lane and Dunlap - 1978 - Estimating effect size Bias resulting from the si.pdf;/home/kelly/Zotero/storage/RZEPN4XC/j.2044-8317.1978.tb00578.html}
}

@article{liangMixturesPriorsBayesian2008,
  title = {Mixtures of g {{Priors}} for {{Bayesian Variable Selection}}},
  author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {410--423},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214507000001337},
  abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. Please see Arnold Zellner's letter and the author's response.},
  keywords = {AIC,Bayesian model averaging,BIC,Cauchy,Empirical Bayes,Gaussian hypergeometric functions,Model selection,Zellner–Siow priors},
  annotation = {\_eprint: https://doi.org/10.1198/016214507000001337},
  file = {/home/kelly/Zotero/storage/TT2CE2VM/Liang et al. - 2008 - Mixtures of g Priors for Bayesian Variable Selecti.pdf;/home/kelly/Zotero/storage/STCQ3FNJ/016214507000001337.html}
}

@article{lorca-pulsImpactSampleSize2018,
  title = {The Impact of Sample Size on the Reproducibility of Voxel-Based Lesion-Deficit Mappings},
  author = {{Lorca-Puls}, Diego L. and {Gajardo-Vidal}, Andrea and White, Jitrachote and Seghier, Mohamed L. and Leff, Alexander P. and Green, David W. and Crinion, Jenny T. and Ludersdorfer, Philipp and Hope, Thomas M. H. and Bowman, Howard and Price, Cathy J.},
  year = {2018},
  month = jul,
  journal = {Neuropsychologia},
  series = {Special {{Issue}}: {{Lesions}} and {{Brain Mapping}}},
  volume = {115},
  pages = {101--111},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2018.03.014},
  abstract = {This study investigated how sample size affects the reproducibility of findings from univariate voxel-based lesion-deficit analyses (e.g., voxel-based lesion-symptom mapping and voxel-based morphometry). Our effect of interest was the strength of the mapping between brain damage and speech articulation difficulties, as measured in terms of the proportion of variance explained. First, we identified a region of interest by searching on a voxel-by-voxel basis for brain areas where greater lesion load was associated with poorer speech articulation using a large sample of 360 right-handed English-speaking stroke survivors. We then randomly drew thousands of bootstrap samples from this data set that included either 30, 60, 90, 120, 180, or 360 patients. For each resample, we recorded effect size estimates and p values after conducting exactly the same lesion-deficit analysis within the previously identified region of interest and holding all procedures constant. The results show (1) how often small effect sizes in a heterogeneous population fail to be detected; (2) how effect size and its statistical significance varies with sample size; (3) how low-powered studies (due to small sample sizes) can greatly over-estimate as well as under-estimate effect sizes; and (4) how large sample sizes (N\,{$\geq$}\,90) can yield highly significant p values even when effect sizes are so small that they become trivial in practical terms. The implications of these findings for interpreting the results from univariate voxel-based lesion-deficit analyses are discussed.},
  langid = {english},
  keywords = {Deficit,Lesion,Lesion-symptom,Reproducibility,Speech production,Stroke,Voxel-based},
  file = {/home/kelly/Zotero/storage/TC5PFBYH/Lorca-Puls et al. - 2018 - The impact of sample size on the reproducibility o.pdf;/home/kelly/Zotero/storage/FATYCTGC/S0028393218301076.html}
}

@article{maassenReproducibilityIndividualEffect2020,
  title = {Reproducibility of Individual Effect Sizes in Meta-Analyses in Psychology},
  author = {Maassen, Esther and van Assen, Marcel A. L. M. and Nuijten, Mich{\`e}le B. and {Olsson-Collentine}, Anton and Wicherts, Jelte M.},
  year = {2020},
  month = may,
  journal = {PLOS ONE},
  volume = {15},
  number = {5},
  pages = {e0233107},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233107},
  abstract = {To determine the reproducibility of psychological meta-analyses, we investigated whether we could reproduce 500 primary study effect sizes drawn from 33 published meta-analyses based on the information given in the meta-analyses, and whether recomputations of primary study effect sizes altered the overall results of the meta-analysis. Results showed that almost half (k = 224) of all sampled primary effect sizes could not be reproduced based on the reported information in the meta-analysis, mostly because of incomplete or missing information on how effect sizes from primary studies were selected and computed. Overall, this led to small discrepancies in the computation of mean effect sizes, confidence intervals and heterogeneity estimates in 13 out of 33 meta-analyses. We provide recommendations to improve transparency in the reporting of the entire meta-analytic process, including the use of preregistration, data and workflow sharing, and explicit coding practices.},
  langid = {english},
  keywords = {Clinical psychology,Metaanalysis,Peer review,Psychology,Publication ethics,Reproducibility,Research reporting guidelines,Systematic reviews},
  file = {/home/kelly/Zotero/storage/7UT2I3ZN/article.html}
}

@article{mccullochRepeatedMeasuresANOVA2005,
  title = {Repeated {{Measures ANOVA}}, {{R}}.{{I}}.{{P}}.?},
  author = {McCulloch, Charles E.},
  year = {2005},
  month = jun,
  journal = {CHANCE},
  volume = {18},
  number = {3},
  pages = {29--33},
  publisher = {{Taylor \& Francis}},
  issn = {0933-2480},
  doi = {10.1080/09332480.2005.10722732},
  annotation = {\_eprint: https://doi.org/10.1080/09332480.2005.10722732},
  file = {/home/kelly/Zotero/storage/QR3MVSSC/McCulloch - 2005 - Repeated Measures ANOVA, R.I.P..pdf;/home/kelly/Zotero/storage/B66E8R3I/09332480.2005.html}
}

@article{mcshaneAdjustingPublicationBias2016,
  title = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}: {{An Evaluation}} of {{Selection Methods}} and {{Some Cautionary Notes}}},
  shorttitle = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}},
  author = {McShane, Blakeley B. and B{\"o}ckenholt, Ulf and Hansen, Karsten T.},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {730--749},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691616662243},
  abstract = {We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis?that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/N2FR6V29/McShane et al. - 2016 - Adjusting for Publication Bias in Meta-Analysis A.pdf}
}

@article{moreyBayesFactorApproaches2011,
  title = {Bayes Factor Approaches for Testing Interval Null Hypotheses},
  author = {Morey, Richard D. and Rouder, Jeffrey N.},
  year = {2011},
  journal = {Psychological Methods},
  volume = {16},
  number = {4},
  pages = {406--419},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/a0024377},
  abstract = {Psychological theories are statements of constraint. The role of hypothesis testing in psychology is to test whether specific theoretical constraints hold in data. Bayesian statistics is well suited to the task of finding supporting evidence for constraint, because it allows for comparing evidence for 2 hypotheses against each another. One issue in hypothesis testing is that constraints may hold only approximately rather than exactly, and the reason for small deviations may be trivial or uninteresting. In the large-sample limit, these uninteresting, small deviations lead to the rejection of a useful constraint. In this article, we develop several Bayes factor 1-sample tests for the assessment of approximate equality and ordinal constraints. In these tests, the null hypothesis covers a small interval of non-0 but negligible effect sizes around 0. These Bayes factors are alternatives to previously developed Bayes factors, which do not allow for interval null hypotheses, and may especially prove useful to researchers who use statistical equivalence testing. To facilitate adoption of these Bayes factor tests, we provide easy-to-use software. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Bayesian Analysis,Null Hypothesis Testing,Psychological Theories,Statistical Analysis,Statistical Probability},
  file = {/home/kelly/Zotero/storage/AVQ587GD/2011-15467-001.html}
}

@article{muthAlternativeModelsSmall2016,
  title = {Alternative {{Models}} for {{Small Samples}} in {{Psychological Research}}: {{Applying Linear Mixed Effects Models}} and {{Generalized Estimating Equations}} to {{Repeated Measures Data}}},
  shorttitle = {Alternative {{Models}} for {{Small Samples}} in {{Psychological Research}}},
  author = {Muth, Chelsea and Bales, Karen L. and Hinde, Katie and Maninger, Nicole and Mendoza, Sally P. and Ferrer, Emilio},
  year = {2016},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {76},
  number = {1},
  pages = {64--87},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/0013164415580432},
  abstract = {Unavoidable sample size issues beset psychological research that involves scarce populations or costly laboratory procedures. When incorporating longitudinal designs these samples are further reduced by traditional modeling techniques, which perform listwise deletion for any instance of missing data. Moreover, these techniques are limited in their capacity to accommodate alternative correlation structures that are common in repeated measures studies. Researchers require sound quantitative methods to work with limited but valuable measures without degrading their data sets. This article provides a brief tutorial and exploration of two alternative longitudinal modeling techniques, linear mixed effects models and generalized estimating equations, as applied to a repeated measures study (n = 12) of pairmate attachment and social stress in primates. Both techniques provide comparable results, but each model offers unique information that can be helpful when deciding the right analytic tool.},
  langid = {english},
  keywords = {generalized estimating equations,linear mixed effects models,longitudinal data,repeated measures ANOVA,small sample},
  file = {/home/kelly/Zotero/storage/GKM24HSH/Muth et al. - 2016 - Alternative Models for Small Samples in Psychologi.pdf}
}

@article{nissenAttentionalRequirementsLearning1987,
  ids = {AttentionalRequirementsLearning1987a,AttentionalRequirementsLearning1987b,AttentionalRequirementsLearning1987c},
  title = {Attentional Requirements of Learning: {{Evidence}} from Performance Measures},
  shorttitle = {Attentional Requirements of Learning},
  author = {Nissen, Mary Jo and Bullemer, Peter},
  year = {1987},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {19},
  number = {1},
  pages = {1--32},
  publisher = {{Academic Press}},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(87)90002-8},
  abstract = {Important differences have emerged between introspective measures of learning, such as recall and recognition, and performance measures, in which the \ldots},
  langid = {english},
  file = {/home/kelly/Zotero/storage/EZ8BHLJ2/0010028587900028.html;/home/kelly/Zotero/storage/NFIPB9TX/0010028587900028.html;/home/kelly/Zotero/storage/PMTZN59Y/0010028587900028.html;/home/kelly/Zotero/storage/TFSSD39S/0010028587900028.html}
}

@article{nolanEvidenceDetectabilityHippocampal2018,
  title = {Evidence against the {{Detectability}} of a {{Hippocampal Place Code Using Functional Magnetic Resonance Imaging}}},
  author = {Nolan, Christopher R. and Vromen, Joyce M. G. and Cheung, Allen and Baumann, Oliver},
  year = {2018},
  month = jul,
  journal = {eNeuro},
  volume = {5},
  number = {4},
  publisher = {{Society for Neuroscience}},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0177-18.2018},
  abstract = {Individual hippocampal neurons selectively increase their firing rates in specific spatial locations. As a population, these neurons provide a decodable representation of space that is robust against changes to sensory- and path-related cues. This neural code is sparse and distributed, theoretically rendering it undetectable with population recording methods such as functional magnetic resonance imaging (fMRI). Existing studies nonetheless report decoding spatial codes in the human hippocampus using such techniques. Here we present results from a virtual navigation experiment in humans in which we eliminated visual- and path-related confounds and statistical limitations present in existing studies, ensuring that any positive decoding results would represent a voxel-place code. Consistent with theoretical arguments derived from electrophysiological data and contrary to existing fMRI studies, our results show that although participants were fully oriented during the navigation task, there was no statistical evidence for a place code.},
  chapter = {Negative Results},
  copyright = {Copyright \textcopyright{} 2018 Nolan et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {30225362},
  keywords = {fMRI,hippocampus,MVPA,navigation,place cells},
  file = {/home/kelly/Zotero/storage/TADLJBES/Nolan et al. - 2018 - Evidence against the Detectability of a Hippocampa.pdf;/home/kelly/Zotero/storage/JS69RL5M/ENEURO.0177-18.html}
}

@article{nydamCathodalElectricalStimulation2018,
  title = {Cathodal Electrical Stimulation of Frontoparietal Cortex Disrupts Statistical Learning of Visual Configural Information},
  author = {Nydam, Abbey S. and Sewell, David K. and Dux, Paul E.},
  year = {2018},
  month = feb,
  journal = {Cortex},
  volume = {99},
  pages = {187--199},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2017.11.008},
  abstract = {Attentional performance is facilitated by exploiting regularities and redundancies in the environment by way of incidental statistical learning. For example, during visual search, response times to a target are reduced by repeating distractor configurations\textendash a phenomenon known as contextual cueing (Chun \& Jiang, 1998). A range of neuroscientific methods have provided evidence that incidental statistical learning relies on subcortical neural structures associated with long-term memory, such as the hippocampus. Functional neuroimaging studies have also implicated the prefrontal cortex (PFC) and posterior parietal cortex (PPC) in contextual cueing. However, the extent to which these cortical regions are causally involved in statistical learning remains unclear. Here, we delivered anodal, cathodal, or sham transcranial direct current stimulation (tDCS) to the left PFC and left PPC online while participants performed a contextual cueing task. Cathodal stimulation of both PFC and PPC disrupted the early cuing effect, relative to sham and anodal stimulation. These findings causally implicate frontoparietal regions in incidental statistical learning that acts on visual configural information. We speculate that contextual cueing may rely on the availability of cognitive control resources in frontal and parietal regions.},
  langid = {english},
  keywords = {Contextual cueing,Incidental learning,Statistical learning,tDCS},
  file = {/home/kelly/Zotero/storage/J4R9WA9D/Nydam et al. - 2018 - Cathodal electrical stimulation of frontoparietal .pdf;/home/kelly/Zotero/storage/LZLC8E57/S0010945217303891.html}
}

@article{okadaOmegaSquaredLess2013,
  title = {Is {{Omega Squared Less Biased}}? A {{Comparison}} of {{Three Major Effect Size Indices}} in {{One-Way Anova}}},
  shorttitle = {Is {{Omega Squared Less Biased}}?},
  author = {Okada, Kensuke},
  year = {2013},
  month = jul,
  journal = {Behaviormetrika},
  volume = {40},
  number = {2},
  pages = {129--147},
  issn = {1349-6964},
  doi = {10.2333/bhmk.40.129},
  abstract = {The purpose of this study is to find less biased effect size index in one-way analysis of variance (ANOVA) by performing a thorough Monte Carlo study with 1,000,000 replications per condition. Our results show that contrary to common belief, epsilon squared is the least biased among the threemajorindices, while omega squared produces the least root mean squared errors, for all conditions. Although eta squared results in the least standard deviation, this does not necessarily make it a good estimator because a considerable amount of bias still occurs when the sample size is small.},
  langid = {english},
  keywords = {ANOVA,bias,effect size,epsilon squared,eta squared,omega squared},
  file = {/home/kelly/Zotero/storage/N8VUMN8F/Okada - 2013 - Is Omega Squared Less Biased a Comparison of Thre.pdf}
}

@article{pashlerDualtaskInterferenceSimple1994a,
  title = {Dual-Task Interference in Simple Tasks: Data and Theory},
  shorttitle = {Dual-Task Interference in Simple Tasks},
  author = {Pashler, H.},
  year = {1994},
  month = sep,
  journal = {Psychological Bulletin},
  volume = {116},
  number = {2},
  pages = {220--244},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.116.2.220},
  abstract = {People often have trouble performing 2 relatively simple tasks concurrently. The causes of this interference and its implications for the nature of attentional limitations have been controversial for 40 years, but recent experimental findings are beginning to provide some answers. Studies of the psychological refractory period effect indicate a stubborn bottleneck encompassing the process of choosing actions and probably memory retrieval generally, together with certain other cognitive operations. Other limitations associated with task preparation, sensory-perceptual processes, and timing can generate additional and distinct forms of interference. These conclusions challenge widely accepted ideas about attentional resources and probe reaction time methodologies. They also suggest new ways of thinking about continuous dual-task performance, effects of extraneous stimulation (e.g., stop signals), and automaticity. Implications for higher mental processes are discussed.},
  langid = {english},
  pmid = {7972591},
  keywords = {Attention,Automatism,Humans,Reaction Time,Task Performance and Analysis}
}

@article{pelliVideoToolboxSoftwareVisual1997,
  ids = {pelliVideoToolboxSoftwareVisual1997a},
  title = {The {{VideoToolbox}} Software for Visual Psychophysics: Transforming Numbers into Movies},
  shorttitle = {The {{VideoToolbox}} Software for Visual Psychophysics},
  author = {Pelli, D.G.},
  year = {1997},
  month = jan,
  journal = {Spatial Vision},
  volume = {10},
  number = {4},
  pages = {437--442},
  publisher = {{Brill}},
  issn = {1568-5683},
  doi = {10.1163/156856897X00366},
  abstract = {The VideoToolbox is a free collection of two hundred C subroutines for Macintosh computers that calibrates and controls the computer-display interface to create accurately specified visual stimuli. High-level platform-independent languages like MATLAB are best for creating the numbers that describe the desired images. Low-level, computer-specific VideoToolbox routines control the hardware that transforms those numbers into a movie. Transcending the particular computer and language, we discuss the nature of the computer-display interface, and how to calibrate and control it.},
  chapter = {Spatial Vision},
  file = {/home/kelly/Zotero/storage/B2NHKQGS/Pelli - 1997 - The VideoToolbox software for visual psychophysics.pdf;/home/kelly/Zotero/storage/G6MFTHKC/Pelli - 1997 - The VideoToolbox software for visual psychophysics.pdf;/home/kelly/Zotero/storage/D7SMSH5H/article-p437_16.html;/home/kelly/Zotero/storage/UCIFZ8Q4/156856897x00366.html}
}

@article{petersonAttentionalGuidanceEyes2001,
  title = {Attentional Guidance of the Eyes by Contextual Information and Abrupt Onsets.},
  author = {Peterson, Matthew S. and Kramer, Arthur F.},
  year = {2001},
  month = oct,
  journal = {Perception \& Psychophysics},
  volume = {63},
  number = {7},
  pages = {1239--1249},
  issn = {1532-5962},
  doi = {10.3758/BF03194537},
  abstract = {Contextual cuing is a memory-based phenomenon in which previously encountered global pattern information in a display can automatically guide attention to the location of a target (Chun\& Jiang, 1998), leading to rapid and accurate responses. What is not clear is how contextual cuing works. By monitoring eye movements, we investigated the roles that recognition and guidance play in contextual cuing. Recognition does not appear to occur on every trial and sometimes does not have its effects until later in the search process. When recognition does occur, attention is guided straight to the target rather than in the general direction. In Experiment 2, we investigated the interaction between memorydriven search (contextual cuing) and stimulus-driven attentional capture by abrupt onsets. Contextual cuing was able to override capture by abrupt onsets. In contrast, onsets had almost no effect on the degree of contextual cuing. These data are discussed in terms of the role of top-down and bottom-up factors in the guidance of attention in visual search.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/AW2RL29A/Peterson and Kramer - 2001 - Attentional guidance of the eyes by contextual inf.pdf}
}

@book{popperLogicScientificDiscovery1959,
  title = {The {{Logic}} of {{Scientific Discovery}}},
  author = {Popper, Karl},
  year = {1959},
  publisher = {{Routledge}},
  abstract = {Described by the philosopher A.J. Ayer as a work of 'great originality and power', this book revolutionized contemporary thinking on science and knowledge. Ideas such as the now legendary doctrine of 'falsificationism' electrified the scientific community, influencing even working scientists, as well as post-war philosophy. This astonishing work ranks alongside The Open Society and Its Enemies as one of Popper's most enduring books and contains insights and arguments that demand to be read to this day.},
  googlebooks = {LWSBAgAAQBAJ},
  isbn = {978-1-134-47002-0},
  langid = {english},
  keywords = {Philosophy / General,Philosophy / Logic,Science / General,Science / Philosophy \& Social Aspects}
}

@article{raymondTemporarySuppressionVisual1992,
  title = {Temporary {{Suppression}} of {{Visual Processing}} in an {{RSVP Task}}: {{An Attentional Blink}}?},
  author = {Raymond, J and Shapiro, K and Arnell, K},
  year = {1992},
  journal = {Journal of Experimental Psychology. Human Perception and Performance},
  volume = {18},
  number = {3},
  pages = {849--860}
}

@misc{rcoreteamLanguageEnvironmentStatistical2015,
  title = {R: {{A}} Language and Environment for Statistical  Computing.},
  author = {R Core Team},
  year = {2015},
  address = {{Vienna, Austria.}},
  howpublished = {R Foundation for Statistical Computing,}
}

@article{rouderAreThereReliable2021,
  title = {Are {{There Reliable Qualitative Individual Differences}} in {{Cognition}}?},
  author = {Rouder, Jeffrey and Haaf, Julia M.},
  year = {2021},
  month = aug,
  journal = {Journal of Cognition},
  volume = {4},
  number = {1},
  pages = {46},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.131},
  abstract = {In this paper we propose a new set of questions that focus on the direction of effects. In almost all studies the direction is important. For example, in a Stroop task we expect responses to incongruent items to be slower than those to congruent ones, and this direction implies one theoretical explanation. Yet, if congruent words are slowed down relative to incongruent words we would have a completely different theoretical explanation. We ask a `does everybody' question, such as, `does every individual show a Stroop effect in the same direction?' Or, `does every individual respond faster to loud tones than soft tones?' If all individuals truly have effects in the same direction that implicate a common theory, we term the differences among them as quantitative individual differences. Conversely, if all individuals truly have effects in different directions that implicate different theories, we term the differences among them as qualitative individual differences. Here, we provide a users guide to the question of whether individual differences are qualitative or quantitative. We discuss theoretical issues, methodological advances, new software for assessment, and, most importantly, how the question impacts theory development in cognitive science. Our hope is that this mode of analysis is a productive tool in researchers' toolkits.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {Bayesian Inference,Cognitive Tasks,Hierarchical Models,Individual Differences},
  file = {/home/kelly/Zotero/storage/EN6RMH5H/Rouder and Haaf - 2021 - Are There Reliable Qualitative Individual Differen.pdf;/home/kelly/Zotero/storage/YRYBUN2X/joc.131.html}
}

@article{rouderBayesianTestsAccepting2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/SJS5G7NV/Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf}
}

@article{rouderDefaultBayesFactors2012,
  title = {Default {{Bayes}} Factors for {{ANOVA}} Designs},
  author = {Rouder, Jeffrey N. and Morey, Richard D. and Speckman, Paul L. and Province, Jordan M.},
  year = {2012},
  month = oct,
  journal = {Journal of Mathematical Psychology},
  volume = {56},
  number = {5},
  pages = {356--374},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2012.08.001},
  abstract = {Bayes factors have been advocated as superior to p-values for assessing statistical evidence in data. Despite the advantages of Bayes factors and the drawbacks of p-values, inference by p-values is still nearly ubiquitous. One impediment to the adoption of Bayes factors is a lack of practical development, particularly a lack of ready-to-use formulas and algorithms. In this paper, we discuss and expand a set of default Bayes factor tests for ANOVA designs. These tests are based on multivariate generalizations of Cauchy priors on standardized effects, and have the desirable properties of being invariant with respect to linear transformations of measurement units. Moreover, these Bayes factors are computationally convenient, and straightforward sampling algorithms are provided. We cover models with fixed, random, and mixed effects, including random interactions, and do so for within-subject, between-subject, and mixed designs. We extend the discussion to regression models with continuous covariates. We also discuss how these Bayes factors may be applied in nonlinear settings, and show how they are useful in differentiating between the power law and the exponential law of skill acquisition. In sum, the current development makes the computation of Bayes factors straightforward for the vast majority of designs in experimental psychology.},
  langid = {english},
  keywords = {Bayes factor,Bayesian statistics,Linear models,Model selection},
  file = {/home/kelly/Zotero/storage/Z99CCS2R/S0022249612000806.html}
}

@article{rouderDefaultBayesFactors2012a,
  title = {Default {{Bayes Factors}} for {{Model Selection}} in {{Regression}}},
  author = {Rouder, Jeffrey N. and Morey, Richard D.},
  year = {2012},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {47},
  number = {6},
  pages = {877--903},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1080/00273171.2012.734737},
  abstract = {In this article, we present a Bayes factor solution for inference in multiple regression. Bayes factors are principled measures of the relative evidence from data for various models or positions, including models that embed null hypotheses. In this regard, they may be used to state positive evidence for a lack of an effect, which is not possible in conventional significance testing. One obstacle to the adoption of Bayes factor in psychological science is a lack of guidance and software. Recently, Liang, Paulo, Molina, Clyde, and Berger (2008) developed computationally attractive default Bayes factors for multiple regression designs. We provide a web applet for convenient computation and guidance and context for use of these priors. We discuss the interpretation and advantages of the advocated Bayes factor evidence measures.},
  pmid = {26735007},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2012.734737},
  file = {/home/kelly/Zotero/storage/MWI3R2L2/Rouder and Morey - 2012 - Default Bayes Factors for Model Selection in Regre.pdf;/home/kelly/Zotero/storage/SXGH6HD3/00273171.2012.html}
}

@manual{rstudiocitation,
  type = {Manual},
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  year = {2020},
  address = {{Boston, MA}},
  organization = {{RStudio, PBC.}}
}

@article{schaferMeaningfulnessEffectSizes2019,
  title = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}: {{Differences Between Sub-Disciplines}} and the {{Impact}} of {{Potential Biases}}},
  shorttitle = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}},
  author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A.},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {813},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.00813},
  abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes\textemdash when is an effect small, medium, or large?\textemdash has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
  file = {/home/kelly/Zotero/storage/Q4EZLDDY/Schäfer and Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychologica.pdf}
}

@article{schumacherVirtuallyPerfectTime2001,
  title = {Virtually Perfect Time Sharing in Dual-Task Performance: Uncorking the Central Cognitive Bottleneck},
  shorttitle = {Virtually Perfect Time Sharing in Dual-Task Performance},
  author = {Schumacher, E. H. and Seymour, T. L. and Glass, J. M. and Fencsik, D. E. and Lauber, E. J. and Kieras, D. E. and Meyer, D. E.},
  year = {2001},
  month = mar,
  journal = {Psychological Science},
  volume = {12},
  number = {2},
  pages = {101--108},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00318},
  abstract = {A fundamental issue for psychological science concerns the extent to which people can simultaneously perform two perceptual-motor tasks. Some theorists have hypothesized that such dual-task performance is severely and persistently constrained by a central cognitive "bottle-neck," whereas others have hypothesized that skilled procedural decision making and response selection for two or more tasks can proceed at the same time under adaptive executive control. The three experiments reported here support this latter hypothesis. Their results show that after relatively modest amounts of practice, at least some participants achieve virtually perfect time sharing in the dual-task performance of basic choice reaction tasks. The results also show that observed interference between tasks can be modulated by instructions about differential task priorities and personal preferences for daring (concurrent) or cautious (successive) scheduling of tasks. Given this outcome, future research should investigate exactly when and how such sophisticated skills in dual-task performance are acquired.},
  langid = {english},
  pmid = {11340917},
  keywords = {Adult,Choice Behavior,Cognition,Female,Humans,Individuality,Male,Models; Psychological,Practice; Psychological,Refractory Period; Psychological,Task Performance and Analysis,Time Factors},
  file = {/home/kelly/Zotero/storage/G5I5SGZ9/Schumacher et al. - 2001 - Virtually perfect time sharing in dual-task perfor.pdf}
}

@article{seddonIndividualDifferencesMedia2021,
  title = {Individual Differences in Media Multitasking Ability: {{The}} Importance of Cognitive Flexibility},
  shorttitle = {Individual Differences in Media Multitasking Ability},
  author = {Seddon, Alexandra L. and Law, Anna S. and Adams, Anne-Marie and Simmons, Fiona R.},
  year = {2021},
  month = jan,
  journal = {Computers in Human Behavior Reports},
  volume = {3},
  pages = {100068},
  issn = {2451-9588},
  doi = {10.1016/j.chbr.2021.100068},
  abstract = {Previous research on media multitasking has often focussed on the frequency with which people perform this type of behaviour. Heavy media multitaskers have been found to differ from light media multitaskers in their performance of tasks involving executive functioning (although these differences have not always been found consistently). The aim of the present study was to explore individuals' executive functioning in relation to their ability to media multitask (i.e., their ability to retain information presented during the session), rather than their propensity to media multitask. Participants (N~\hspace{0pt}=~\hspace{0pt}116, aged 18\textendash 25, male N~\hspace{0pt}=~\hspace{0pt}32) completed an executive function task battery, inclusive of working memory, inhibition and cognitive flexibility tasks, followed by a studious media multitasking situation. Individual executive function task performance scores were correlated with media multitasking ability scores. Greater cognitive flexibility was significantly associated with greater ability to media multitask, in terms of retention of information from a media multitasking situation. Furthermore, media multitasking influenced mood, reducing levels of self-reported arousal. Thus, the present study provides some elucidation as to what cognitive characteristics are involved in being able to media multitask, whilst also indicating a possible cognitive mechanism for negative associations found between media multitasking and academic performance.},
  langid = {english},
  keywords = {Cognitive flexibility,Inhibition,Media multitasking,Working memory},
  file = {/home/kelly/Zotero/storage/TQ5SUWHJ/Seddon et al. - 2021 - Individual differences in media multitasking abili.pdf}
}

@article{siskMechanismsContextualCueing2019,
  title = {Mechanisms of Contextual Cueing: {{A}} Tutorial Review},
  shorttitle = {Mechanisms of Contextual Cueing},
  author = {Sisk, Caitlin A. and Remington, Roger W. and Jiang, Yuhong V.},
  year = {2019},
  month = nov,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {8},
  pages = {2571--2589},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01832-2},
  abstract = {Repeated contexts yield faster response time in visual search, compared with novel contexts. This effect is known as contextual cueing. Despite extensive study over the past two decades, there remains a spirited debate over whether repeated displays expedite search before the target is found (early locus) or facilitate response after the target is found (late locus). Here, we provide a tutorial review of contextual cueing, with a focus on assessing the locus of the effect. We evaluate the evidence from psychophysics, EEG, and eye tracking. Existing studies support an early locus of contextual cueing, consistent with attentional guidance accounts. Evidence for a late locus exists, though it is less conclusive. Existing literature also highlights a distinction between habit-guided attention learned through experience and changes in spatial priority driven by task goals and stimulus salience.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/ZR5EAJT6/Sisk et al. - 2019 - Mechanisms of contextual cueing A tutorial review.pdf}
}

@article{stark-inbarIndividualDifferencesImplicit2017,
  title = {Individual Differences in Implicit Motor Learning: Task Specificity in Sensorimotor Adaptation and Sequence Learning},
  shorttitle = {Individual Differences in Implicit Motor Learning},
  author = {{Stark-Inbar}, Alit and Raza, Meher and Taylor, Jordan A. and Ivry, Richard B.},
  year = {2017},
  month = jan,
  journal = {Journal of Neurophysiology},
  volume = {117},
  number = {1},
  pages = {412--428},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.01141.2015},
  abstract = {In standard taxonomies, motor skills are typically treated as representative of implicit or procedural memory. We examined two emblematic tasks of implicit motor learning, sensorimotor adaptation and sequence learning, asking whether individual differences in learning are correlated between these tasks, as well as how individual differences within each task are related to different performance variables. As a prerequisite, it was essential to establish the reliability of learning measures for each task. Participants were tested twice on a visuomotor adaptation task and on a sequence learning task, either the serial reaction time task or the alternating reaction time task. Learning was evident in all tasks at the group level and reliable at the individual level in visuomotor adaptation and the alternating reaction time task but not in the serial reaction time task. Performance variability was predictive of learning in both domains, yet the relationship was in the opposite direction for adaptation and sequence learning. For the former, faster learning was associated with lower variability, consistent with models of sensorimotor adaptation in which learning rates are sensitive to noise. For the latter, greater learning was associated with higher variability and slower reaction times, factors that may facilitate the spread of activation required to form predictive, sequential associations. Interestingly, learning measures of the different tasks were not correlated. Together, these results oppose a shared process for implicit learning in sensorimotor adaptation and sequence learning and provide insight into the factors that account for individual differences in learning within each task domain. NEW \& NOTEWORTHY We investigated individual differences in the ability to implicitly learn motor skills. As a prerequisite, we assessed whether individual differences were reliable across test sessions. We found that two commonly used tasks of implicit learning, visuomotor adaptation and the alternating serial reaction time task, exhibited good test-retest reliability in measures of learning and performance. However, the learning measures did not correlate between the two tasks, arguing against a shared process for implicit motor learning.},
  keywords = {adaptation,implicit learning,individual differences,reliability,sequence learning},
  file = {/home/kelly/Zotero/storage/N73JZJ2P/Stark-Inbar et al. - 2017 - Individual differences in implicit motor learning.pdf}
}

@article{szollosiPreregistrationWorthwhile2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and van Rooij, Iris and Zandt, Trisha Van and Donkin, Chris},
  year = {2020},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {2},
  pages = {94--95},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.11.009},
  langid = {english},
  pmid = {31892461},
  keywords = {inference,preregistration,theory development},
  file = {/home/kelly/Zotero/storage/GAT7DPWW/Szollosi et al. - 2020 - Is Preregistration Worthwhile.pdf;/home/kelly/Zotero/storage/EP8WNH98/S1364-6613(19)30285-2.html}
}

@article{szucsWhenNullHypothesis2017,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  pages = {390},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.},
  file = {/home/kelly/Zotero/storage/MF793TJT/Szucs and Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{thomsonMoreYourMind2015,
  title = {The More Your Mind Wanders, the Smaller Your Attentional Blink: {{An}} Individual Differences Study},
  shorttitle = {The More Your Mind Wanders, the Smaller Your Attentional Blink},
  author = {Thomson, David R. and Ralph, Brandon C. W. and Besner, Derek and Smilek, Daniel},
  year = {2015},
  month = jan,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {68},
  number = {1},
  pages = {181--191},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470218.2014.940985},
  abstract = {The present studies investigate the hypothesis that individuals who frequently report experiencing episodes of mind wandering do so because they under-invest attentional/executive resources in the external environment. Here we examined whether self-reported instances of mind wandering predict the magnitude of the ``attentional blink'' (AB) in a rapid serial visual presentation (RSVP) task, since a prominent view is that the AB derives from an over-investment of attention in the information stream. Study 1 demonstrates that subjective reports of mind wandering in a sustained attention task have a negative predictive relation with respect to the magnitude of the AB measured in a subsequent RSVP task. In addition, using the Spontaneous and Deliberate Mind Wandering Questionnaire in Study 2, we were again able to show that trait-level mind wandering in everyday life negatively predicts AB magnitude. We suggest that mind wandering may be the behavioural outcome of an adaptive cognitive style intended to maximize the efficient processing of dynamic and temporally unpredictable events.},
  langid = {english},
  keywords = {Attention,Attentional blink,Mind wandering},
  file = {/home/kelly/Zotero/storage/BACH23VD/Thomson et al. - 2015 - The more your mind wanders, the smaller your atten.pdf}
}

@article{travisRoleWorkingMemory2013,
  title = {On the Role of Working Memory in Spatial Contextual Cueing},
  author = {Travis, S.L. and Mattingley, J.B. and Dux, P.E.},
  year = {2013},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {39},
  number = {1},
  pages = {208--219},
  doi = {http://dx.doi.org/10.1037/a0028644}
}

@article{troncososkidmoreBiasPrecisionClassical2013,
  title = {Bias and Precision of Some Classical {{ANOVA}} Effect Sizes When Assumptions Are Violated},
  author = {Troncoso Skidmore, Susan and Thompson, Bruce},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {536--546},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0257-2},
  abstract = {Previous simulation research has focused on evaluating the impact of analytic assumption violations on statistics related to the F test and associated pCALCULATED values. The present article evaluated the bias of classical estimates of practical significance (i.e., effect size sample estimators \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$, \$\$ \{\textbackslash widehat\{\textbackslash varepsilon\}\^2\} \$\$, and \$\$ \{\textbackslash widehat\{\textbackslash omega\}\^2\} \$\$) in a one-way between-subjects univariate ANOVA when assumptions are violated. The simulation conditions modeled were selected on the basis of prior empirical research. Estimated (1) sampling error bias and (2) precision computed for each of the three effect size estimates for the 5,000 samples drawn for each of the 270 (5 parameter Cohen's d values \texttimes{} 3 group size ratios \texttimes{} 3 population distribution shapes \texttimes{} 3 variance ratios \texttimes{} 2 total ns) conditions were modeled for each of the k = 2, 3, and 4 group analyses. Our results corroborate the limited previous related research and suggest that \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$should not be used as an ANOVA effect size estimator, even though \$\$ \{\textbackslash widehat\{\textbackslash eta\}\^2\} \$\$is the only available choice in the menus in most commonly available software.},
  langid = {english},
  file = {/home/kelly/Zotero/storage/RAV57693/Troncoso Skidmore and Thompson - 2013 - Bias and precision of some classical ANOVA effect .pdf}
}

@article{vadilloRaisingAwarenessMeasurement2021,
  title = {Raising Awareness about Measurement Error in Research on Unconscious Mental Processes},
  author = {Vadillo, Miguel A. and Malejka, Simone and Lee, Daryl Y. H. and Dienes, Zoltan and Shanks, David R.},
  year = {2021},
  month = jun,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01923-y},
  abstract = {Experimental psychologists often neglect the poor psychometric properties of the dependent measures collected in their studies. In particular, a low reliability of measures can have dramatic consequences for the interpretation of key findings in some of the most popular experimental paradigms, especially when strong inferences are drawn from the absence of statistically significant correlations. In research on unconscious cognition, for instance, it is commonly argued that the lack of a correlation between task performance and measures of awareness or explicit recollection of the target stimuli provides strong support for the conclusion that the cognitive processes underlying performance must be unconscious. Using contextual cuing of visual search as a case study, we show that given the low reliability of the dependent measures collected in these studies, it is usually impossible to draw any firm conclusion about the unconscious character of this effect from correlational analyses. Furthermore, both a psychometric meta-analysis of the available evidence and a cognitive-modeling approach suggest that, in fact, we should expect to see very low correlations between performance and awareness at the empirical level, even if both constructs are perfectly related at the latent level. Convincing evidence for the unconscious character of contextual cuing and other effects will most likely demand richer and larger data sets, coupled with more powerful analytic approaches.},
  langid = {english},
  pmid = {34131891},
  keywords = {Contextual cuing,Meta-analysis,Reliability,Unconscious learning}
}

@article{vadilloUnconsciousUnderpoweredProbabilistic2020,
  title = {Unconscious or Underpowered? {{Probabilistic}} Cuing of Visual Attention},
  shorttitle = {Unconscious or Underpowered?},
  author = {Vadillo, Miguel A. and Linssen, Douglas and Orgaz, Cristina and Parsons, Stephanie and Shanks, David R.},
  year = {2020},
  month = jan,
  journal = {Journal of Experimental Psychology. General},
  volume = {149},
  number = {1},
  pages = {160--181},
  issn = {1939-2222},
  doi = {10.1037/xge0000632},
  abstract = {Recent debate about the reliability of psychological research has raised concerns about the prevalence of false positives in our discipline. However, false negatives can be just as concerning in areas of research that depend on finding support for the absence of an effect. This risk is particularly high in unconscious learning experiments, where researchers commonly seek to demonstrate that people can learn to perform a task in the absence of any explicit knowledge of the information that drives performance. The fact that some unconscious learning effects are typically studied with small samples and unreliable awareness measures makes false negatives especially likely. In the present article we focus on a popular unconscious learning paradigm, probabilistic cuing of visual attention, as a case study. First, we show that, at the meta-analytic level, previous experiments reveal positive signs of participant awareness, although individual studies are severely underpowered to detect this. Second, we report the results of 2 empirical studies in which participants' awareness was tested with alternative and more sensitive dependent measures, both of which manifest positive evidence of awareness. We also show that, based on the predictions of a formal model of probabilistic cuing and given the reliabilities of the dependent measures collected in these experiments, any statistical test aimed at detecting a significant correlation between learning and awareness is doomed to return a nonsignificant result, even if at the latent level both constructs are actually related and participants' knowledge is completely explicit. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {31246061},
  keywords = {Attention,Awareness,Cues,Humans,Learning,Male,Photic Stimulation,Probability,Reaction Time,Reproducibility of Results,Unconscious; Psychology},
  file = {/home/kelly/Zotero/storage/CVDUHMR4/Vadillo et al. - 2020 - Unconscious or underpowered Probabilistic cuing o.pdf}
}

@article{vadilloUnderpoweredSamplesFalse2016,
  title = {Underpowered Samples, False Negatives, and Unconscious Learning},
  author = {Vadillo, Miguel A. and Konstantinidis, Emmanouil and Shanks, David R.},
  year = {2016},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {1},
  pages = {87--102},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0892-6},
  abstract = {The scientific community has witnessed growing concern about the high rate of false positives and unreliable results within the psychological literature, but the harmful impact of false negatives has been largely ignored. False negatives are particularly concerning in research areas where demonstrating the absence of an effect is crucial, such as studies of unconscious or implicit processing. Research on implicit processes seeks evidence of above-chance performance on some implicit behavioral measure at the same time as chance-level performance (that is, a null result) on an explicit measure of awareness. A systematic review of 73 studies of contextual cuing, a popular implicit learning paradigm, involving 181 statistical analyses of awareness tests, reveals how underpowered studies can lead to failure to reject a false null hypothesis. Among the studies that reported sufficient information, the meta-analytic effect size across awareness tests was d z = 0.31 (95~\% CI 0.24-0.37), showing that participants' learning in these experiments was conscious. The unusually large number of positive results in this literature cannot be explained by selective publication. Instead, our analyses demonstrate that these tests are typically insensitive and underpowered to detect medium to small, but true, effects in awareness tests. These findings challenge a widespread and theoretically important claim about the extent of unconscious human cognition.},
  langid = {english},
  pmcid = {PMC4742512},
  pmid = {26122896},
  keywords = {Adult,Awareness,Contextual cuing,Cues,Data Interpretation; Statistical,False negatives,Female,Humans,Implicit learning,Learning,Male,Meta-Analysis as Topic,Null hypothesis Significance testing· Statistical power,Unconscious; Psychology}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  month = oct,
  journal = {Journal of Experimental Psychology. General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli.},
  langid = {english},
  pmid = {25111580},
  keywords = {Humans,Models; Statistical,Research Design,Sample Size,Statistics as Topic},
  file = {/home/kelly/Downloads/2014-32656-001.pdf}
}

@misc{yarkoniGeneralizabilityCrisis2019,
  title = {The {{Generalizability Crisis}}},
  author = {Yarkoni, Tal},
  year = {2019},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jqw35},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned\textemdash that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology\textemdash the linear mixed model\textemdash I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that whereas the "random effect" formalism is used pervasively in psychology to model inter-subject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that the failure to  problems many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  keywords = {generalization,inference,philosophy of science,prediction,psychology,Quantitative Methods,random effects,Social and Behavioral Sciences,statistics,Theory and Philosophy of Science},
  file = {/home/kelly/Zotero/storage/CJSPGB8H/Yarkoni - 2019 - The Generalizability Crisis.pdf}
}

